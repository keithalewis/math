<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Keith A. Lewis" />
  <meta name="dcterms.date" content="2022-09-28" />
  <title>Bayes Theorem</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="math.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans&family=STIX+Two+Text&display=swap" rel="stylesheet">
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Bayes Theorem</h1>
<p class="author">Keith A. Lewis</p>
<p class="date">September 28, 2022</p>
<div class="abstract">
<div class="abstract-title">Abstract</div>
Conditional probability
</div>
</header>

<p><em>Frequentists</em> interpret probability as the number of times an
outcome belongs to an event divided by the number of times it is
sampled. This assumes it is possible to repeatedly sample identical
situations. The <em>law of large numbers</em> says the frequency
converges to the probablity of the event. This is fine when applied to
flipping coins, rolling dice, or running other physical experiments
where conditions don’t change, but it limits the use of probability.</p>
<p><em>Bayesians</em> interpret probability as a subjective <em>degree
of belief</em> based on available information. As more information
becomes available it can be used to update individual degrees of belief.
They are delayed satisfaction frequentists who believe subjective
degrees of belief will converge for all individuals given sufficient
common information over repeated trials.</p>
<section id="probability-space" class="level2">
<h2>Probability Space</h2>
<p>A <em>probability space</em> is a set <span
class="math inline">Ω</span> of possible outcomes and a <em>probability
measure</em> <span class="math inline">P</span> that takes subsets of
<span class="math inline">Ω</span> to a number between 0 and 1. Measures
satisfy <span class="math inline">P(E\cup F) = P(E) + P(F) - P(E\cap
F)</span> and <span class="math inline">P(\emptyset) = 0</span>, <span
class="math inline">E,F\subseteq Ω</span>. Measures don’t count things
twice and the measure of nothing is zero. Probability measures are
non-negative and <span class="math inline">P(Ω) = 1</span>. Subsets of a
sample space are <em>events</em>. If the sample space is finite we only
need to specify the probability of each outcome. The probability of each
outcome must be between 0 and 1 and sum to 1. The probability of an
event is <span class="math inline">P(E) = \sum_{ω \in E}
P(\{ω\})</span>.</p>
</section>
<section id="conditional-probability" class="level2">
<h2>Conditional Probability</h2>
<p>The <em>conditional probability</em> of <span
class="math inline">A</span> <em>given</em> <span
class="math inline">B</span> is defined by <span
class="math inline">P(A\mid B) = P(A\cap B)/P(B)</span>. It satifies
<span class="math inline">P(B\mid B) = 1</span> and <span
class="math inline">A\mapsto P(A\mid B)</span> is a <em>probability
measure</em>. Since <span class="math inline">P(A\mid B) = P(A\cap
B)/P(B)</span> and <span class="math inline">P(B\mid A) = P(B\cap
A)/P(A)</span> it follows <span class="math inline">P(A\mid B) = P(A\cap
B)/P(B) = P(B\cap A)/P(B) = P(B\mid A)P(A)/P(B)</span>. <span
class="math display">
    P(A\mid B) = P(B\mid A)P(A)/P(B)
</span> This formula is the basis of Baysian probabilty. It shows how to
update probabilities given new information. <em>Bayesians</em> interpret
probability as a subjective <em>degree of belief</em> instead of a
physical <em>frequency</em> of (potential?) occurance.
<em>Frequentists</em> can flip a coin to their heart’s content but have
nothing to say about the probability you will get hit by a car while
crossing a particular steet at some specific time.</p>
<p>Bayesians like to belive that as more infomation becomes available
subjective probabilites will converge to the same value. They haven’t
come up with a general theory ensuring this, but that seems to
frequently occur.</p>
<section id="example" class="level3">
<h3>Example</h3>
<p>A coin is <em>fair</em> if heads and tails occur with equal
probability when flipped<a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a>. Suppose a coin might be fair or
might have two heads, but we cannot examine the coin directly. The only
information we will be given is the outcomes of a series of flips. If we
ever see tails we know the coin is not double-headed but if every flip
we see is heads then that provides evidence the coin is
double-headed.</p>
<section class="footnotes footnotes-end-of-block" role="doc-endnotes">
<ol>
<li id="fn1" role="doc-endnote"><p>Persi Diaconis<a href="#fnref1"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<p>Suppose the first flip of the coin is heads, <span
class="math inline">H_1</span>. A naive application of Bayes theorem is
<span class="math inline">P(C|H_1) = P(H_1|C)P(C)/P(H_1)</span>, where
<span class="math inline">C</span> indicates the coin is fair. For a
fair coin <span class="math inline">P(H_1|C) = 1/2</span> and for a
double-headed coin <span class="math inline">P(H_1|D) = 1</span>, where
<span class="math inline">D</span> indicates the coin is double-headed.
The other probabilities require some assumptions.</p>
<p>Step one in probability theory is to specify the <em>sample
space</em> of possible outcomes and the probability of <em>events</em>
(subsets of the sample space). If the sample space is finite it is
sufficient to specify the probability of each outcome.</p>
<p>In our case the sample space has two elements <span
class="math inline">Ω = \{C,D\}</span> indicating the coin is either
fair or double-headed. Assume the probability that the coin is fair
equals <span class="math inline">p</span> so the probability of the coin
being double-headed is <span class="math inline">1 - p</span> and <span
class="math inline">P(H_1) = p/2 + (1 - p) = (1 - p)/2</span>. Using
Bayes formula, <span class="math inline">p_1 = P(F|H_1) = (1/2)p/((1 -
p)/2) = p/(1 - p) &lt; p</span> if <span class="math inline">0 &lt; p
&lt; 1</span>. Seeing a head flipped on the first try provides evidence
against the coin being fair.</p>
<p>If you had trouble believing <span class="math inline">P(H_1) = p/2 +
(1 - p)</span> then you have good intuition. We were sloppy about
specifying the sample space. (This happens quite often in the
literature, for example, the <a
href="https://en.wikipedia.org/wiki/Monty_Hall_problem">Monte Hall
problem</a>.) The sample space should also include the outcome of the
first toss <span class="math inline">Ω = \{(C,H), (C,T), (D,H), (D,T)\}
= \{C,D\}\times \{H,T\}</span>; the coin can be fair or double-headed
and the first toss can be either heads or tails. The probability of each
outcome is <span class="math inline">P(\{(C,H)\} = p/2</span>, <span
class="math inline">P(\{(C,T)\} = p/2</span>, <span
class="math inline">P(\{(D,H)\} = 1 - p</span>, <span
class="math inline">P(\{(D,T)\} = 0</span>, where <span
class="math inline">0 &lt; p &lt; 1</span>. Flipping heads on the first
toss is the subset <span class="math inline">H_1 =
\{(C,H),(D,H)\}</span> so <span class="math inline">P(H_1) =
P(\{(C,H),(D,H)\}) = P(\{(C,H)\}) + P(\{(D,H)\}) = p/2 + (1 -
p)</span>.</p>
<p>If we want to model flipping the coin <span
class="math inline">n</span> times the sample space becomes <span
class="math inline">Ω = \{C,D\}\times\{H,T\}^n</span> and the
probabilites are <span class="math inline">P(\{C,ω)\} = p/2^n</span> for
<span class="math inline">ω\in\{H,T\}^n</span>, and <span
class="math inline">P(\{D\}, ω) = 0</span> unless <span
class="math inline">ω</span> is all heads, denoted <span
class="math inline">H_n</span>. Since <span class="math inline">P(H_n) =
p/2^n + (1 - p)</span> Bayes Theorem gives <span class="math inline">p_n
= P(C\mid H_n) = p/2^n/(p/2^n + (1 - p)) = p/(p + 2^n(1 - p))</span>. As
the number of consecutive heads goes to infinity the probability of the
coin being fair approaches zero no matter the initial guess <span
class="math inline">p &gt; 0</span> of the coin being fair.</p>
<p>This is an example of Bayesian reasoning showing subjective
probabilities converge given sufficient information. It is important to
note that this depends on the model. A different model could allow for
the possibility the coin might also be two-tailed. Every model makes
assumptions about what information is available so all probabilities are
conditional.</p>
</section>
</section>
<section id="says-who" class="level2">
<h2>Says Who?</h2>
<p>If you had trouble believing <span class="math inline">P(A\mid B) =
P(A\cap B)/P(B)</span> then you have good intuition. Mathematics hijacks
standard vocabulary to define mathematical statements. Descartes and
Spinoza seem to be the first scholars to attempt rigourous definitions
of words beyond the limited vocabulary of <em>and</em>, <em>or</em>,
<em>not</em>, and <em>implies</em> used in Aristotelian logic. Prior to
his invention of a logic involving only <em>true</em> or <em>false</em>
statements (<em>propositions</em>) the main effort was directed at
determining <em>valid</em> methods of reasoning to arrive at
<em>plausible</em> statements. This most likely originated early in our
prehistory when shamans could fool people into giving them bear skins
and arrow heads in exchange for empty promises.</p>
<p>Probability theory is an extension of logic. Instead of statements
that are either false or true it assigns a probability between 0 and 1
to events. From its sordid beginnings in of games of chance it achieved
mathematical respectability when Kolmogorov axiomatized it in 1933.
(cite?) Reputable mathematicians could now slot Probability Theory into
the existing Measure Theory they were comfortable with as positive
measures having mass 1.</p>
<p>Mathematics is following your nose and thinking rigourously, which
immediately leads to difficulties most people, rightfully so, think are
Much Ado About Nothing. For example, probability 1 does not correspond
to true and must be replaced with the more subtle notion of <em>almost
surely</em>.</p>
<p>Richard Threlkeld Cox put conditional probabilty on firmer
philosophical foundations by axiomatizing the notion of
<em>likelihood</em>. Staying true to the earliest foundations, he
considered <em>statements</em> instead of propositions. He denoted the
likelihood of statement <span class="math inline">A</span> given
statement <span class="math inline">B</span> by <span
class="math inline">A\mid B</span> and required consistency with
standard symbolic logic.</p>
<p>If the shaman tells you he can make it rain tomorrow if you give him
a basket of grain and you come back from a long day in the field to find
him scurrying out of your hut and your wife with her hair mussed and
won’t look you in the eye, how likely is it that he will deliver on his
promise given this information? If history is any guide, he will likely
tell you he now needs two baskets of grain.</p>
<p>Cox assumed likelihood is a real number. Real numbers are totally
ordered so this is a big assumption. He was also vague on exactly what
statements constitute information. His notation for the likelihood of
statement <span class="math inline">A</span> given statement/information
<span class="math inline">B</span> is <span class="math inline">A\mid
B</span>. He assumes the likelihood of statements <span
class="math inline">A</span> and <span class="math inline">B</span>,
denoted <span class="math inline">AB</span>, given information <span
class="math inline">C</span> is <span class="math inline">AB\mid C =
p(A\mid B, B\mid AC)</span> for some function <span
class="math inline">p</span>. It can be shown that consistency with
symbolic logic requires <span class="math inline">p(p(x,y),z) =
p(x,p(y,z))</span> for any real numbers <span
class="math inline">x</span>, <span class="math inline">y</span>, and
<span class="math inline">z</span>.</p>
<p>Cox wanted to show <span class="math inline">P(ST) = P(S)P(T\mid
S)</span> is a consequence of of more general assumptions. His original
derivation was not mathematically correct, but it inspired others to
improve his assumptions that utimately led to a precise formulation.</p>
<p>This was really an aside to his life’s work. He was an experimental
physicist who, among other results, demontrated a parity violation for
double scattering of β rays from radium that could not be explained by
existing theory. Eventually theory caught up and proved him correct.</p>
<!-- how do you combine statements and information? -->
</section>
<section id="unfiled" class="level2">
<h2>Unfiled</h2>
<p>Keynes A Treatise on Probability - probability is not a total
order.</p>
<blockquote>
<p>Is our expectation of rain, when we start out for a walk, always more
likely than not, or less likely than not, or as likely as not? I am
prepared to argue that on some occasions none of these alternatives
hold, and that it will be an arbitrary matter to decide for or against
the umbrella. If the barometer is high, but the clouds are black, it is
not always rational that one should prevail over the other in our minds,
or even that we should balance them, though it will be rational to allow
caprice to determine us and to waste no time on the debate.</p>
</blockquote>
<p>maxplus algebras</p>
</section>
<section id="bibiliography" class="level2">
<h2>Bibiliography</h2>
<p>[ { “id”: “http://zotero.org/users/6482136/items/H8KQMTYV”, “type”:
“article-journal”, “abstract”: “By basing Bayesian probability theory on
ﬁve axioms, we can give a trivial proof of Cox’s Theorem on the product
rule and sum rule for conditional plausibility without assuming
continuity or diﬀerentiablity of plausibility. Instead, we extend the
notion of plausibility to apply to unknowns, giving them plausible
values. Thus, we combine the best aspects of two approaches to Bayesian
probability theory, namely the Cox-Jaynes theory and the de Finetti
theory.”, “container-title”: “Bayesian Analysis”, “DOI”:
“10.1214/09-BA422”, “ISSN”: “1936-0975”, “issue”: “3”,
“journalAbbreviation”: “Bayesian Anal.”, “language”: “en”, “source”:
“DOI.org (Crossref)”, “title”: “New axioms for rigorous Bayesian
probability”, “URL”:
“https://projecteuclid.org/journals/bayesian-analysis/volume-4/issue-3/New-axioms-for-rigorous-Bayesian-probability/10.1214/09-BA422.full”,
“volume”: “4”, “author”: [ { “family”: “Dupré”, “given”: “Maurice J.” },
{ “family”: “Tipler”, “given”: “Frank J.” } ], “issued”: { “date-parts”:
[ [ “2009”, 9, 1 ] ] } }]</p>
</section>
<footer>
Return to <a href="index.html">index</a>.
</footer>
</body>
</html>
