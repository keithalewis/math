<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Keith A. Lewis" />
  <title>Cumulative Distribution Functions</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="math.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
   var mathElements = document.getElementsByClassName("math");
   for (var i = 0; i < mathElements.length; i++) {
    var texText = mathElements[i].firstChild;
    if (mathElements[i].tagName == "SPAN") {
     katex.render(texText.data, mathElements[i], {
      displayMode: mathElements[i].classList.contains('display'),
      throwOnError: false,
      fleqn: true
     });
  }}});
  </script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Literata:wght@300&display=swap" rel="stylesheet"> 
</head>
<body>
<header id="title-block-header">
<h1 class="title">Cumulative Distribution Functions</h1>
<p class="author">Keith A. Lewis</p>
</header>
<p>The <em>cumulative distribution function</em> of a random variable <span class="math inline">X</span> is <span class="math inline">F_X(x) = F(x) = P(X\le x)</span>. It tells you everything there is to know about the distribution of <span class="math inline">X</span>. For example <span class="math inline">P(X\in E) = E[1_E(X)] = \int_E dF(x)</span> where <span class="math inline">1_E(x) = 1</span> if <span class="math inline">x\in E</span>, <span class="math inline">1_E(x) = 0</span> if <span class="math inline">x\not\in E</span> and we use Riemann-Stieltjes integration.</p>
<p>Every cdf is non-decreasing, continuous from the right, has left limits, and <span class="math inline">\lim_{x\to-\infty}F(x) = 0</span>, <span class="math inline">\lim_{x\to+\infty}F(x) = 1</span>. Any function with these properties is the cdf of a random variable.</p>
<p><strong>Exercise</strong>. <em>Prove right continuity using <span class="math inline">\cap_n (-\infty, x + 1/n] = (-\infty, x]</span></em>.</p>
<p>Note <span class="math inline">\cup_n (-\infty,x - 1/n] = (-\infty,x) \not= (-\infty,x]</span>. The sequence <span class="math inline">F(x - 1/n)</span> is non-decreasing so it has a limit, but not necessarily <span class="math inline">F(x)</span>.</p>
<p>The distribution of a <em>uniformly distributed</em> random variable on <span class="math inline">[0,1]</span>, <span class="math inline">U</span>, is <span class="math inline">F(x) = x</span> if <span class="math inline">0\le x\le 1</span>, <span class="math inline">F(x) = 0</span> if <span class="math inline">x &lt; 0</span>, and <span class="math inline">F(x) = 1</span> if <span class="math inline">x &gt; 1</span>. In this case <span class="math inline">P(X\in(a, b]) = b - a</span> for <span class="math inline">0\le a &lt; b\le 1</span>.</p>
<p><strong>Exercise</strong> <em>If <span class="math inline">X</span> has cdf <span class="math inline">F</span>, show <span class="math inline">F^{-1}(U)</span> has the same distribution as <span class="math inline">X</span> and <span class="math inline">F(X)</span> has the same distribution as <span class="math inline">U</span></em>.</p>
<p>If <span class="math inline">F(x)</span> jumps from <span class="math inline">a</span> to <span class="math inline">b</span> at <span class="math inline">x = c</span> we define <span class="math inline">F^{-1}(u) = c</span> for <span class="math inline">a \le u &lt; b</span>.</p>
<h2 id="continuous-and-discrete">Continuous and Discrete</h2>
<p>A random variable is <em>continuously distributed</em> if <span class="math inline">F = \int F&#39;</span> and we call <span class="math inline">f = F&#39;</span> the <em>probability density function</em> of <span class="math inline">X</span>. If <span class="math inline">X</span> is discrete with <span class="math inline">P(X = x_j) = p_j</span> then <span class="math inline">F(x) = \sum_j 1(x\le x_j) p_j</span> and <span class="math inline">f(x) = \sum_j δ_{x_j} p_j</span> where <span class="math inline">δ_c</span> is the <em>delta function</em>, or <em>point mass</em>, at <span class="math inline">c</span>. A delta function is not a function but we can define <span class="math inline">\int_{\bm{R}} g(x) \delta_a(x) \,dx = \int_{\bm{R}} g(x) d1(x \ge a) = g(a)</span> using Riemann-Stieltjes integration.</p>
<h2 id="cumulant">Cumulant</h2>
<p>The <em>cumulant</em> of the random variable <span class="math inline">X</span> is <span class="math inline">\kappa_X(s) = \kappa(s) = \log E[\exp(s X)]</span>.</p>
<p><strong>Exercise</strong>. <em>Show if <span class="math inline">c</span> is a constant then <span class="math inline">\kappa_{c + X}(s) = cs + \kappa_X(s)</span></em>.</p>
<p><strong>Exercise</strong>. <em>Show if <span class="math inline">c</span> is a constant then <span class="math inline">\kappa_{cX}(s) = \kappa_X(cs)</span> if <span class="math inline">c</span></em>.</p>
<p><strong>Exercise</strong>. <em>Show if <span class="math inline">X</span> and <span class="math inline">y</span> are independent then <span class="math inline">\kappa_{X + Y}(s) = \kappa_X(s) + \kappa_Y(s)</span></em>.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">\kappa_X(0) = 0</span>, <span class="math inline">\kappa_X&#39;(0) = E[X]</span>, and <span class="math inline">\kappa_X&#39;&#39;(0) = \operatorname{Var}(X)</span></em>.</p>
<p>The <em>cumulants</em>, <span class="math inline">(\kappa_n(X))</span>, are the coefficients of the Taylor series expansion of the cumulant <span class="math inline">\kappa(s) = \sum_{n &gt; 0} \kappa_n s^n/n!</span>. Note <span class="math inline">\kappa_n = \kappa^{(n)}(0)</span> if <span class="math inline">\kappa</span> is sufficiently differentiable.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">\kappa_1(c + X) = c + \kappa_1(X)</span> and <span class="math inline">\kappa_n(c + X) = \kappa_n(X)</span> if <span class="math inline">n &gt; 1</span></em>.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">\kappa_n(cX) = c^n\kappa_n(X)</span> for all <span class="math inline">n</span></em>.</p>
<p><strong>Exercise</strong>. <em>Show if <span class="math inline">X</span> and <span class="math inline">Y</span> are independent <span class="math inline">\kappa_n(X + Y) = \kappa_n(X) + \kappa_n(Y)</span> for all <span class="math inline">n</span></em>.</p>
<p>The <em>moments</em> of <span class="math inline">X</span>, <span class="math inline">\mu_n = E[X^n]</span> are related to the cumulants via complete Bell polynomials <span class="math inline">(B_n)</span>. <span class="math display">
    E[e^{sX}] = \sum_{n\ge0} \mu_n s^n/n! = e^{\kappa(s)} = e^{\sum_{n&gt;0} \kappa_n s^n/n!}
    = \sum_{n\ge0} B_n(\kappa_1,\ldots,\kappa_n) s^n/n!
</span> Taking a derivative with respect to <span class="math inline">s</span> of the last equality gives the recurrence formula <span class="math display">
    B_0 = 1, B_{n+1}(\kappa_1,\ldots,\kappa_{n+1})
        = \sum_{k = 0}^n \binom{n}{k} B_{n - k}(\kappa_1,\ldots,\kappa_{n-k})\kappa_{k + 1}, n &gt; 0.
</span></p>
<p>The cumulants can be expressed in terms of moments by <span class="math display">
    \kappa_n = \sum_{k=0}^{n-1} (-1)^k k! B_{n,k+1}(\mu_1,\ldots,\mu_{n - k})
</span> where <span class="math inline">(B_{n,k})</span> are partial Bell polynomials satisfying the recurrence <span class="math inline">B_{0,0} = 1</span>, <span class="math inline">B_{n,0} = 0</span> for <span class="math inline">n &gt; 0</span>, <span class="math inline">B_{0,k} = 0</span> for <span class="math inline">k &gt; 0</span> and <span class="math display">
    B_{n,k}(x_1,\ldots,x_{n - k + 1})
        = \sum_{j=0}^{n-k}\binom{n-1}{j} B_{n-j+1,k-1}(x_1,\ldots,x_{j+1})x_{j+1}
</span></p>
<p>If <span class="math inline">e_X(s) = E[\exp(sX)]</span> then <span class="math inline">e_X^{(n)}(s) = E[X^n\exp(sX)]</span> and <span class="math inline">\kappa_X^{(n)}(s) = ...</span>.</p>
<h2 id="distributions">Distributions</h2>
<p>We gather some facts about the distribution and cumulant of random variables.</p>
<h2 id="discrete">Discrete</h2>
<p>Let <span class="math inline">P(X = x_j) = p_j</span>, <span class="math inline">p_j\ge 0</span>, <span class="math inline">\sum_j p_j = 1</span>. The cdf is <span class="math inline">F(X\le x) = \sum_j 1(x\le x_j) p_j</span> and pdf is <span class="math inline">f(x) = \sum_j \delta_{x_j} \_j</span>. The cumulant is <span class="math inline">\kappa(s) = \log E[\sum_j \exp(sx_j) p_j</span>].</p>
<h3 id="normal">Normal</h3>
<p>The standard normal random variable <span class="math inline">Z</span> has density <span class="math inline">\phi(z) = \exp(-z^2/2)/\sqrt{2\pi}</span>, <span class="math inline">-\infty &lt; z &lt; \infty</span>, and cumulative distribution <span class="math inline">\Phi(z) = \int_{-\infty}^z \exp(-t^2/2)\, dt/\sqrt{2\pi}</span>.</p>
<p>The derivatives of <span class="math inline">\phi</span> are <span class="math inline">\phi^{(n)}(x) = (-1)^n\phi(x)H_n(x)</span> where <span class="math inline">H_n</span> are the Hermite polynomials. They satisfy the recurrence <span class="math inline">H_0(x) = 1</span>, <span class="math inline">H_1(x) = x</span> and <span class="math inline">H_{n+1}(x) = x H_n(x) - n H_{n-1}(x)</span>, <span class="math inline">n\ge 1</span>.</p>
<p>Note <span class="math display">
\begin{aligned}
E[e^{\mu + \sigma Z}] &amp;= \int_{-\infty}^\infty e^{\mu + \sigma z} e^{-z^2/2}\, dz/\sqrt{2\pi} \\
    &amp;= \int_{-\infty}^\infty e^{\mu + \sigma^2/2} e^{-(z - \sigma)^2/2}\, dz/\sqrt{2\pi} \\
    &amp;= e^{\mu + \sigma^2/2} \\
\end{aligned}
</span> so <span class="math inline">E[\exp(N)] = \exp(E[N] + \operatorname{Var}(N)/2)</span> for any normally distributed random variable <span class="math inline">N</span>,</p>
<p>Also <span class="math display">
\begin{aligned}
E[g(Z) e^{sZ}] &amp;= \int_{-\infty}^\infty g(z) e^{sz} e^{-z^2/2}\, dz/\sqrt{2\pi} \\
    &amp;= \int_{-\infty}^\infty g(z) e^{s^2/2} e^{-(z - s)^2/2}\, dz/\sqrt{2\pi} \\
    &amp;= E[e^{sZ}] E(g(Z - s)]. \\
\end{aligned}
</span> More generally, if <span class="math inline">N</span>, <span class="math inline">N_1, \ldots</span> are jointly normal then <span class="math inline">E[\exp(N)g(N_1,\ldots)] = E[\exp(N)] E[g(N_1 + \operatorname{Cov}(N, N_1), \ldots)]</span>. In particular <span class="math display">
    E[\exp(s X - s^2/2)g(X)] = E[g(X + s)].
</span> Taking derivatives with respect to <span class="math inline">s</span> shows <span class="math display">
    E[\exp(s X - s^2/2)(X - s)g(X)] = E[g&#39;(X + s)].
</span> so <span class="math inline">E[X g(X)] = E[g&#39;(X)]</span> when <span class="math inline">s = 0</span>.</p>
<p>The moments of <span class="math inline">X</span> can be calculated using <span class="math inline">E[X^n] = E[X X^{n-1}] = (n - 1)E[X^{n - 2}]</span>, <span class="math inline">E[X^0] = 1</span>, and <span class="math inline">E[X^1] = 0</span>. The odd moments are zero and the even moments are <span class="math inline">E[X^{2n}] = (2n - 1)(2n - 3)\cdots 1 = (2n - 1)!!</span>.</p>
<p>The cumulant is <span class="math inline">\kappa(s) = \log E[\exp(sZ)] = s^2/2</span> so <span class="math inline">\kappa_2 = 1</span> and all other cumulants are zero.</p>
<h3 id="logistic">Logistic</h3>
<p>A logistic random variate has cumulative distribution <span class="math inline">F(x) = 1/(1 + e^{-x})</span>, <span class="math inline">x\in\mathbf{R}</span>. It’s density function is <span class="math inline">f(x) = e^{-x}/(1 + e^{-x})^2</span>. Note <span class="math inline">f(x) = \int_0^\infty e^{-x} e^{-\alpha e^{-x}} e^{-\alpha}\,d\alpha</span>.</p>
<p>The quantile function is <span class="math inline">Q(u) = F^{-1}(u) = \log u/(1-u)</span>.</p>
<p>The moment generating function is <span class="math inline">M_X(t) = E e^{tX} = \Gamma(1 - t)\Gamma(1 + t)</span>, <span class="math inline">-1 &lt; t &lt; 1</span>.</p>
<p>Using <span class="math inline">u = F(x) = 1/(1 + e^{-x})</span>, so <span class="math inline">e^x = u/(1 - u)</span> <span class="math display">
\begin{aligned}
E e^{tX} &amp;= \int_{-\infty}^\infty e^{tx} e^{-x}/(1 + e^{-x})^2\,dx \\
    &amp;= \int_0^1 u^t(1 - u)^{-t}\,du \\
    &amp;= B(1 + t, 1 - t) \\
    &amp;= \Gamma(1 + t)\Gamma(1 - t)/\Gamma(2) \\
    &amp;= \Gamma(1 + t)\Gamma(1 - t)
\end{aligned}
</span></p>
<p>Using <span class="math inline">u = F(x) = 1/(1 + e^{-x})</span>, so <span class="math inline">e^x = u/(1 - u)</span> <span class="math display">
\begin{aligned}
\int_{-\infty}^a e^{tx} dF(x)
    &amp;= \int_0^{F(a)} u^t(1 - u)^{-t}\,du \\
    &amp;= B(F(a); 1 + t, 1 - t) \\
\end{aligned}
</span> where <span class="math inline">B(u;\alpha,\beta) = \int_0^u t^{\alpha-1}(1 - t)^{\beta - 1}\,dt</span> is the partial beta function.</p>
<h2 id="scratch">Scratch</h2>
<p><span class="math inline">E[g(X)e^{s X - \kappa(s)}] = E[g(h(X,s))]</span> for some <span class="math inline">h</span>? <span class="math inline">h(X,s) = X + s</span> if <span class="math inline">X</span> std normal.</p>
<p><span class="math inline">E[g(X)e^{s X - \kappa(s)}(X - \kappa&#39;(s)] = E[g&#39;(h(X,s))dh(X,s)/ds]</span>.</p>
<p>Note <span class="math inline">E[X^ne^{sX}] = (d/ds)^s E[e^{sX}] = e^{\kappa(s)}\sum_{k=0}^n B_{n,k}(\kappa&#39;(s), \ldots, \kappa^{(n-k+1)}(s))</span>.</p>
<p>Note <span class="math inline">E[g(X)e^{sX - \kappa(s)}] = E[\sum_{n\ge 0} g^{(n)}(0) X^n/n! e^{sX - \kappa(s)}] = \sum_{n\ge 0} g^{(n)}(0)/n! \sum_{k=0}^n B_{n,k}(\kappa&#39;(s), \ldots, \kappa^{(n-k+1)}(s)) = \sum_{k\ge 0} \sum_{n\ge k} D^ng(0)/n! B_{n,k}(\kappa&#39;(s), \ldots, \kappa^{(n-k+1)}(s))</span></p>
<p><span class="math inline">1 = E[e^{sX - \kappa(s)}]</span>.</p>
<p><span class="math inline">0 = E[e^{s X - \kappa(s)}(X - \kappa&#39;(s))]</span>.</p>
<p><span class="math inline">e^{\kappa(s)} = E[e^{sX}]</span>.</p>
<p><span class="math inline">e^{\kappa(s)}\kappa&#39;(s) = E[Xe^{sX}]</span>.</p>
<p><span class="math inline">e^{\kappa(s)}(\kappa&#39;&#39;(s) + \kappa&#39;(s)^2) = E[X^2e^{sX}]</span></p>
<p><span class="math inline">e^{\kappa(s)}(\kappa&#39;&#39;&#39;(s) + 2\kappa&#39;(s)\kappa&#39;&#39;(s) + \kappa&#39;(s)((\kappa&#39;&#39;(x) + \kappa&#39;(s)^2)) = e^{\kappa(s)}(\kappa&#39;&#39;&#39;(s) + 3\kappa&#39;&#39;(s)\kappa&#39;(s) + \kappa&#39;(s)^3) = E[X^3e^{sX}]</span></p>
<p><span class="math inline">\sum_{k=1}^n B_{n,k}(\kappa&#39;(s), \ldots, \kappa^{(n-k+1)}(s)) = E[X^ne^{sX - \kappa(s)}]</span>.</p>
<p><span class="math inline">B_{n,k}(x_1,\ldots,x_{n-k+1}) = \sum_{j=0}^{n-k}\binom{n-1}{j}B_{n-j+1,k-1}(x_1,\ldots,x_{n-j+1})x_{j+1}</span>, <span class="math inline">B_{0,0} = 1</span>, <span class="math inline">B_{n,0} = 0</span>, <span class="math inline">B_{0,k} = 0</span>.</p>
<p>$ E[g(X) e^{sX - (s)}] E[_n g^{(n)}(0) X^n/n! e^{sX - (s)}] = <em>n g^{(n)}(0)/n! </em>{k=1}^n B_{n,k}(‘(s), , ^{(n-k+1)}(s)) = <em>{k=1}^</em>{n=k}^a_n t^n/n! B_{n,k}(’(s), , ^{(n-k+1)}(s)) =~ _{k=1}<sup>(<em>{m=1}<sup></sup>{(m)}(s)t<sup>m/m!)</sup>k =~ </em>{k=1}</sup>(kappa(s + t))^k $</p>
<p><span class="math inline">E[Xe^{s X - \kappa(s)}(X - \kappa&#39;(s))] =E[X^2e^{s X - \kappa(s)} - \kappa&#39;(s)Xe^{s X - \kappa(s)}] =\kappa&#39;&#39;(s) + \kappa&#39;(s)^2 - \kappa&#39;(s)\kappa&#39;(s) = \kappa&#39;&#39;(s) = (d/ds)\kappa&#39;(s) = (d/ds)(X + \kappa&#39;(s))</span></p>
<p><span class="math inline">E[X^2e^{s X - \kappa(s)}(X - \kappa&#39;(s))] =E[X^3e^{s X - \kappa(s)} - \kappa&#39;(s)X^2e^{s X - \kappa(s)}] =\kappa&#39;&#39;&#39;(s) + 3\kappa&#39;&#39;(s)\kappa&#39;(s) + \kappa&#39;(s)^3 - \kappa&#39;(s)(\kappa&#39;&#39;(s) + \kappa&#39;(s)^2) =\kappa&#39;&#39;&#39;(s) + 2\kappa&#39;&#39;(s)\kappa&#39;(s) - \kappa&#39;(s)^3</span>.</p>
<footer>
Return to <a href="index.html">index</a>.
</footer>
</body>
</html>
