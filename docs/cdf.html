<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Keith A. Lewis" />
  <title>Cumulative Distribution Functions</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="math.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
   var mathElements = document.getElementsByClassName("math");
   for (var i = 0; i < mathElements.length; i++) {
    var texText = mathElements[i].firstChild;
    if (mathElements[i].tagName == "SPAN") {
     katex.render(texText.data, mathElements[i], {
      displayMode: mathElements[i].classList.contains('display'),
      throwOnError: false,
      fleqn: true
     });
  }}});
  </script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <!--
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Literata:wght@300&display=swap" rel="stylesheet"> 
  <link href="https://fonts.googleapis.com/css2?family=Markazi+Text:wght@500&display=swap" rel="stylesheet"> 
  -->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Cumulative Distribution Functions</h1>
<p class="author">Keith A. Lewis</p>
</header>
<p>The <em>cumulative distribution function</em> of a random variable <span class="math inline">X</span> is the probability that the random variable is less than or equal to <span class="math inline">x</span> – <span class="math inline">F^X(x) = F(x) = P(X\le x)</span>. It tells you everything there is to know about the random variable. For example, <span class="math inline">P(a &lt; X \le b) = F(b) - F(a)</span>. Although <span class="math inline">F</span> is a function it is more accurate to think of it as a function on sets – <span class="math inline">P(X\in (a, b]) = \int_a^b dF = F(b) - F(a)</span> using Riemann-Stieltjes integration.</p>
<p>More generally for <span class="math inline">E\subseteq\bm{R}</span>, <span class="math inline">P(X\in E) = E[1_E(X)] = \int 1_E(x) dF(x)</span> where <span class="math inline">1_E(x) = 1</span> if <span class="math inline">x\in E</span>, <span class="math inline">1_E(x) = 0</span> if <span class="math inline">x\not\in E</span>.</p>
<p>Every cdf is non-decreasing, continuous from the right, has left limits, and <span class="math inline">\lim_{x\to-\infty}F(x) = 0</span>, <span class="math inline">\lim_{x\to+\infty}F(x) = 1</span>. Any function with these properties is the cdf of a random variable.</p>
<p><strong>Exercise</strong>. <em>Prove right continuity using <span class="math inline">\cap_n (-\infty, x + 1/n] = (-\infty, x]</span></em>.</p>
<p>Note <span class="math inline">\cup_n (-\infty,x - 1/n] = (-\infty,x) \not= (-\infty,x]</span>. The sequence <span class="math inline">F(x - 1/n)</span> is non-decreasing and bounded by <span class="math inline">F(x)</span> so it has a limit, but not necessarily <span class="math inline">F(x)</span>.</p>
<p>The distribution of a <em>uniformly distributed</em> random variable on <span class="math inline">[0,1]</span>, <span class="math inline">U</span>, is <span class="math inline">F(x) = x</span> if <span class="math inline">0\le x\le 1</span>, <span class="math inline">F(x) = 0</span> if <span class="math inline">x &lt; 0</span>, and <span class="math inline">F(x) = 1</span> if <span class="math inline">x &gt; 1</span>. In this case <span class="math inline">P(X\in(a, b]) = b - a</span> for <span class="math inline">0\le a &lt; b\le 1</span>.</p>
<p><strong>Exercise</strong> <em>If <span class="math inline">X</span> has cdf <span class="math inline">F</span>, show <span class="math inline">F^{-1}(U)</span> has the same distribution as <span class="math inline">X</span> and <span class="math inline">F(X)</span> has the same distribution as <span class="math inline">U</span></em>.</p>
<p>If <span class="math inline">F(x)</span> jumps from <span class="math inline">a</span> to <span class="math inline">b</span> at <span class="math inline">x = c</span> we define <span class="math inline">F^{-1}(u) = c</span> for <span class="math inline">a \le u &lt; b</span>.</p>
<section id="cumulant" class="level2">
<h2>Cumulant</h2>
<p>The <em>cumulant</em> of the random variable <span class="math inline">X</span> is <span class="math inline">κ_X(s) = κ(s) = \log E[\exp(s X)]</span>. The Esscher transform <span class="math inline">X_s</span>, <span class="math inline">s\in\bm{R}</span>, of a random variable <span class="math inline">X</span> with density <span class="math inline">f</span> has density <span class="math inline">f_s(x) = f(x)e^{s x - κ(s)}</span> and we write <span class="math inline">E_s</span> for expectation using density <span class="math inline">f_s</span>. Since <span class="math inline">E[\exp(sX)] = \exp(κ(s))</span> this is a probability density.</p>
<p>Note <span class="math inline">(d/ds)E[g(X)e^{s X - κ(s)}] = E[g(X)e^{s X - κ(s)}(X - κ&#39;(s))]</span> so <span class="math display">
    \frac{d}{ds}E_s[g(X)] = E_s[g(X)(X - κ&#39;(s))].
</span> The cdf of <span class="math inline">X_s</span> is <span class="math inline">F_s(x) = P_s(X\le x) = E[1(X\le x) e^{s X - κ(s)}]</span> so <span class="math inline">dF_s(x)/ds = E[1(X\le x) e^{s X - κ(s)}(X - κ&#39;(s))]</span>.</p>
</section>
<section id="distributions" class="level2">
<h2>Distributions</h2>
<p>We gather some facts about the distributions and cumulants of particular random variables.</p>
<section id="discrete" class="level3">
<h3>Discrete</h3>
<p>If <span class="math inline">X</span> is discrete with <span class="math inline">P(X = x_j) = p_j</span> then <span class="math inline">F(x) = \sum_j 1(x\le x_j) p_j</span> and <span class="math inline">f(x) = \sum_j δ_{x_j}(x) p_j</span> where <span class="math inline">δ_a</span> is the <em>delta function</em>, or <em>point mass</em>, at <span class="math inline">a</span>. In general, <span class="math inline">F^{(n)}(x) = \sum_j δ_{x_j}^{(n-1)}(x) p_j</span> where <span class="math inline">\int_{\bm{R}} g(x) δ_{a}^{(k)}\,dx = g^{(k)}(a)</span>. A delta function is not a function but we can define <span class="math inline">\int_{\bm{R}} g(x) \delta_a(x) \,dx = \int_{\bm{R}} g(x) d1(x \ge a) = g(a)</span> using Riemann-Stieltjes integration.</p>
<p>The cumulant is <span class="math inline">κ(s) = \log E[\sum_j \exp(sx_j) p_j]</span>.</p>
</section>
<section id="normal" class="level3">
<h3>Normal</h3>
<p>The standard normal random variable <span class="math inline">X</span> has density <span class="math inline">φ(x) = \exp(-x^2/2)/\sqrt{2\pi}</span>, <span class="math inline">-\infty &lt; x &lt; \infty</span>, and cumulative distribution <span class="math inline">Φ(x) = \int_{-\infty}^x \exp(-ξ^2/2)\, dξ/\sqrt{2\pi}</span>.</p>
<p>The derivatives of the density are <span class="math inline">φ^{(n)}(x) = (-1)^nφ(x)H_n(x)</span> where <span class="math inline">H_n</span> are the Hermite polynomials. They satisfy the recurrence <span class="math inline">H_0(x) = 1</span>, <span class="math inline">H_1(x) = x</span> and <span class="math inline">H_{n+1}(x) = x H_n(x) - n H_{n-1}(x)</span>, <span class="math inline">n\ge 1</span>.</p>
<p>Some useful properties are <span class="math display">
\begin{aligned}
E[e^{\mu + \sigma X}] &amp;= \int_{-\infty}^\infty e^{\mu + \sigma z} e^{-z^2/2}\, dz/\sqrt{2\pi} \\
    &amp;= \int_{-\infty}^\infty e^{\mu + \sigma^2/2} e^{-(z - \sigma)^2/2}\, dz/\sqrt{2\pi} \\
    &amp;= e^{\mu + \sigma^2/2} \\
\end{aligned}
</span> so <span class="math inline">E[\exp(N)] = \exp(E[N] + \operatorname{Var}(N)/2)</span> for any normally distributed random variable <span class="math inline">N</span>,</p>
<p>Also <span class="math display">
\begin{aligned}
E[g(X) e^{sX}] &amp;= \int_{-\infty}^\infty g(z) e^{sz} e^{-z^2/2}\, dz/\sqrt{2\pi} \\
    &amp;= \int_{-\infty}^\infty g(z) e^{s^2/2} e^{-(z - s)^2/2}\, dz/\sqrt{2\pi} \\
    &amp;= E[e^{sX}] E(g(X - s)]. \\
\end{aligned}
</span> More generally, if <span class="math inline">N</span>, <span class="math inline">N_1, \ldots</span> are jointly normal then <span class="math display">
E[\exp(N)g(N_1,\ldots)] = E[\exp(N)] E[g(N_1 + \operatorname{Cov}(N, N_1), \ldots)].
</span> This is an elementary form of Girsanov’s Theorem.</p>
<p><strong>Exercise</strong>. <em>Prove this</em>.</p>
<p>Hint: <span class="math inline">N_1, \ldots</span> are jointly normal if and only if they are linear combinations of independent standard normals.</p>
<p>The cumulant is <span class="math inline">κ(s) = \log E[\exp(sX)] = s^2/2</span> so <span class="math inline">κ_2 = 1</span> and all other cumulants are zero. The transformed cdf is <span class="math inline">Φ_s(x) = P_s(X\le x) = E[1(X\le x) e^{sX - s^2/2}] = P(X + s\le x) = Φ(x - s)</span> so <span class="math inline">dΦ_s(x)/ds = E[1(X\le x) e^{sX - s^2/2}(X - s)] = -φ(x - s)</span>.</p>
<p>In particular <span class="math display">
    E[\exp(s X - s^2/2)g(X)] = E[g(X + s)].
</span> Taking derivatives with respect to <span class="math inline">s</span> shows <span class="math display">
    E[\exp(s X - s^2/2)(X - s)g(X)] = E[g&#39;(X + s)].
</span> so <span class="math inline">E[X g(X)] = E[g&#39;(X)]</span> when <span class="math inline">s = 0</span>.</p>
</section>
<section id="poisson" class="level3">
<h3>Poisson</h3>
<p>The Poisson distribution with parameter <span class="math inline">λ</span> has density <span class="math inline">P(X_λ = n) = e^{-λ}λ^n/n!</span>, <span class="math inline">n\ge 0</span>. Since <span class="math inline">E[e^{s X}] = \sum_{n\ge 0} e^{sn} e^{-λ}λ^n/n! = e^{-λ}\exp(e^sλ) = \exp(λ(e^s - 1))</span> we have <span class="math display">
κ(s) = λ(e^s - 1), κ_n = λ, n\ge 1.
</span></p>
<p>The Esscher transform a Poisson distribution with parameter <span class="math inline">λ</span> is Poisson with parameter <span class="math inline">λe^s</span>.</p>
<p><span class="math display">
\begin{aligned}
    E[g(X_λ)e^{s X - κ(s)}]
    &amp;= \sum_{n\ge0} g(n)e^{sn - λ(e^s - 1)}e^{-λ}λ^n/n! \\
    &amp;= \sum_{n\ge0} g(n)e^{-λ e^s} (λ e^s)^n/n! \\
    &amp;= E[g(X_{λ e^s})] \\
\end{aligned}
</span> so <span class="math inline">E_s[g(X_λ)] = E[g(X_{λ e^s})]</span>. Taking a derivative with respect to <span class="math inline">s</span> <span class="math display">
\begin{aligned}
    \frac{d}{ds} E[g(X_λ)e^{s X - κ(s)}]
    &amp;= \sum_{n\ge0} g(n)\frac{d}{ds}\bigl(e^{-λ e^s} (λ e^s)^n\bigr)/n! \\
    &amp;= \sum_{n\ge0} g(n)\bigl(e^{-λ e^s} n(λ e^s)^{n-1}λ e^s - e^{-λ e^s}λ e^s (λ e^s)^n\bigr)/n! \\
    &amp;= \sum_{n\ge0} g(n)λ e^se^{-λ e^s} \bigl((λ e^s)^{n-1}/(n-1)! - (λ e^s)^n/n!\bigr) \\
    &amp;= λ e^s E_s[g(X_{λ e^s} + 1) - g(X_{λ e^s})] \\
\end{aligned}
</span> so <span class="math inline">(d/ds)E_s[g(X_λ)] = λ e^s E_s[g(X_{λ e^s} + 1) - g(X_{λ e^s})]</span>.</p>
</section>
<section id="exponential" class="level3">
<h3>Exponential</h3>
<p>The density of an exponential with parameter <span class="math inline">λ</span> is <span class="math inline">f(x) = λ\exp(-λ x)</span>, <span class="math inline">x\ge 0</span>. Since <span class="math inline">E[\exp sX] = \int_0^\infty \exp(sx) λ\exp(-λ x)\,dx = λ/(λ - s)</span> The cumulant is <span class="math inline">κ(s) = -\log(1 - s/λ)</span> so <span class="math inline">κ&#39;(s) = (1 - s/λ)^{-1}</span> and <span class="math inline">κ^{(n)}(s) = (1 - s/λ)^{-n}(n - 1)!/λ^{n-1}</span></p>
</section>
<section id="logistic" class="level3">
<h3>Logistic</h3>
<p>A logistic random variate has cumulative distribution <span class="math inline">F(x) = 1/(1 + e^{-x})</span>, <span class="math inline">-\infty &lt; x &lt; \infty</span> and density function <span class="math inline">f(x) = e^{-x}/(1 + e^{-x})^2</span>. The quantile function is <span class="math inline">Q(u) = F^{-1}(u) = \log u/(1-u)</span>.</p>
<p>Aside: <span class="math inline">f(x) = \int_0^\infty e^{-x} e^{-\alpha e^{-x}} e^{-\alpha}\,d\alpha</span>.</p>
<p>Using <span class="math inline">u = F(x) = 1/(1 + e^{-x})</span>, so <span class="math inline">e^x = u/(1 - u)</span> and <span class="math display">
\begin{aligned}
E e^{sX} &amp;= \int_{-\infty}^\infty e^{sx} e^{-x}/(1 + e^{-x})^2\,dx \\
    &amp;= \int_0^1 u^s(1 - u)^{-s}\,du \\
    &amp;= B(1 + s, 1 - s) \\
    &amp;= Γ(1 + s)Γ(1 - s)/Γ(2) \\
    &amp;= Γ(1 + s)Γ(1 - s)
\end{aligned}
</span> we have the cumulant of the logistic is <span class="math inline">κ(s) = \logΓ(1 + s) + \logΓ(1 - s)</span>.</p>
<p>The <em>digamma</em> function is the derivative of the log of the Gamma function <span class="math inline">\psi(s) = Γ&#39;(s)/Γ(s)</span>. Its Taylor series at <span class="math inline">1</span> is <span class="math inline">\psi(1 + s) = -\gamma - \sum_{k\ge 1} \zeta(k+1)(-s)^k</span> where <span class="math inline">\gamma</span> is the Euler-Mascheroni constant and <span class="math inline">\zeta(s) = \sum_{k\ge 1} n^{-s}</span> is the zeta function.</p>
<p>The first derivative of the cumulant is <span class="math inline">κ&#39;(s) = \psi(1 + s) - \psi(1 - s) = 2\sum_{k\ge 1} \zeta(2k)s^{2k - 1}</span> so <span class="math inline">κ_{2k-1} = 0</span> and <span class="math inline">κ_{2k} = 2\zeta(2k)(2k-1)!</span>, <span class="math inline">k\ge1</span>. In particular, the variance of the logistic is <span class="math inline">κ_2 = 2\zeta(2) = \pi^2/3</span>.</p>
<p>The Esscher transformed cumulative distribution is <span class="math display">
    F_s(u) = \int_{-\infty}^u e^{sx - κ(s)} dF(x)
    = \frac{e^{u(1 + s)}\bigl(1 + s - s(1 + e^u)\,_2F_1(1, 1 + s; 2 + s; -e^u)\bigr)}
    {Γ(1 + s)Γ(1 - s)(1 + e^u)(1 + s)}, s &gt; -1.
</span> where <span class="math inline">\,_2F_1(a,b;c;x) = \sum_{n=0}^\infty\frac{(a)_n (b)_n}{(c)_n} x^n/n!</span> is the Gaussian hypergeometric function.</p>
<!--
Using
$$
\begin{aligned}
    \,_2F_1(a,b;c;x) &= (-x)^{-a}\frac{Γ(c)Γ(b - a)}{Γ(b)Γ(c - a)}\,_2F_1(a, a - c + 1; a - b + 1;1/x) \\
        &\quad + (-x)^{-b}\frac{Γ(c)Γ(a - b)}{Γ(a)Γ(c - b)}\,_2F_1(b - c + 1, b; b - a + 1;1/x)
\end{aligned}
$$
so
$$
\begin{aligned}
    \,_2F_1(1,1+s;2+s;-e^u) 
        &= e^{-au}\frac{Γ(2 + s)Γ(s)}{Γ(1 + s)Γ(1 + s)}\,_2F_1(1, -s; 1 - s;-e^{-u}) \\
        &\quad + e^{-bu}\frac{Γ(2 + s)Γ(-s)}{Γ(1 + s)Γ(1)}\,_2F_1(0, 1 + s; 1 + s;-e^{-u}) \\
        &= e^{-au}\frac{Γ(2 + s)Γ(s)}{Γ(1 + s)^2}\,_2F_1(1, -s; 1 - s;-e^{-u}) \\
        &\quad + e^{-bu}\frac{Γ(2 + s)Γ(-s)}{Γ(1 + s)}\,_2F_1(0, 1 + s; 1 + s;-e^{-u}) \\
\end{aligned}
$$

## Scratch

$E[g(X)e^{s X - κ(s)}] = E[g(h(X,s))]$ for some $h$? $h(X,s) = X + s$ if $X$ std normal.

$E[g(X)e^{s X - κ(s)}(X - κ'(s)] = E[g'(h(X,s))dh(X,s)/ds]$.

Note $E[X^ne^{sX}] = (d/ds)^s E[e^{sX}]
= e^{κ(s)}\sum_{k=0}^n B_{n,k}(κ'(s), \ldots, κ^{(n-k+1)}(s))$.

Note $E[g(X)e^{sX - κ(s)}] = E[\sum_{n\ge 0} g^{(n)}(0) X^n/n! e^{sX - κ(s)}]
= \sum_{n\ge 0} g^{(n)}(0)/n! \sum_{k=0}^n B_{n,k}(κ'(s), \ldots, κ^{(n-k+1)}(s))
= \sum_{k\ge 0} \sum_{n\ge k} D^ng(0)/n! B_{n,k}(κ'(s), \ldots, κ^{(n-k+1)}(s))$

$1 = E[e^{sX - κ(s)}]$.

$0 = E[e^{s X - κ(s)}(X - κ'(s))]$.

$e^{κ(s)} = E[e^{sX}]$.

$e^{κ(s)}κ'(s) = E[Xe^{sX}]$.

$e^{κ(s)}(κ''(s) + κ'(s)^2) = E[X^2e^{sX}]$

$e^{κ(s)}(κ'''(s) + 2κ'(s)κ''(s) + κ'(s)((κ''(x) + κ'(s)^2))
= e^{κ(s)}(κ'''(s) + 3κ''(s)κ'(s) + κ'(s)^3) = E[X^3e^{sX}]$

$\sum_{k=1}^n B_{n,k}(κ'(s), \ldots, κ^{(n-k+1)}(s)) = E[X^ne^{sX - κ(s)}]$.

$B_{n,k}(x_1,\ldots,x_{n-k+1}) = \sum_{j=0}^{n-k}\binom{n-1}{j}B_{n-j+1,k-1}(x_1,\ldots,x_{n-j+1})x_{j+1}$,
$B_{0,0} = 1$, $B_{n,0} = 0$, $B_{0,k} = 0$.

$
E[g(X) e^{sX - κ(s)}]
E[\sum_n g^{(n)}(0) X^n/n! e^{sX - κ(s)}]
= \sum_n  g^{(n)}(0)/n! \sum_{k=1}^n B_{n,k}(κ'(s), \ldots, κ^{(n-k+1)}(s))
= \sum_{k=1}^\infty \sum_{n=k}^\infty a_n t^n/n!  B_{n,k}(κ'(s), \ldots, κ^{(n-k+1)}(s))
=~ \sum_{k=1}^\infty (\sum_{m=1}^\infty κ^{(m)}(s)t^m/m!)^k
=~ \sum_{k=1}^\infty (kappa(s + t))^k
$


$E[Xe^{s X - κ(s)}(X - κ'(s))]
=E[X^2e^{s X - κ(s)} - κ'(s)Xe^{s X - κ(s)}]
=κ''(s) + κ'(s)^2 - κ'(s)κ'(s) = κ''(s) = (d/ds)κ'(s) = (d/ds)(X + κ'(s))$

$E[X^2e^{s X - κ(s)}(X - κ'(s))]
=E[X^3e^{s X - κ(s)} - κ'(s)X^2e^{s X - κ(s)}]
=κ'''(s) + 3κ''(s)κ'(s) + κ'(s)^3 - κ'(s)(κ''(s) + κ'(s)^2)
=κ'''(s) + 2κ''(s)κ'(s) - κ'(s)^3$.

-->
</section>
</section>
<footer>
Return to <a href="index.html">index</a>.
</footer>
</body>
</html>
