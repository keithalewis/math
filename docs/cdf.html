<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Keith A. Lewis" />
  <meta name="dcterms.date" content="2023-04-10" />
  <title>Cumulative Distribution Functions</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="math.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans&family=STIX+Two+Text&display=swap" rel="stylesheet">
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Cumulative Distribution Functions</h1>
<p class="author">Keith A. Lewis</p>
<p class="date">April 10, 2023</p>
<div class="abstract">
<div class="abstract-title">Abstract</div>
Facts about cumulative distribution functions
</div>
</header>
<p>We collect some facts about <a
href="prob.html#random-variable"><em>random variables</em></a>, their <a
href="prob.html#cumulant"><em>cumulants</em></a>, and deriatives that
that are useful for <a href="op.html">option pricing</a>.</p>
<section id="esscher" class="level2">
<h2>Esscher</h2>
<p>Recall the moment generating function of a random variable <span
class="math inline">X</span> is <span class="math inline">M(s) =
E[e^{sX}]</span> and its cumulant is <span class="math inline">κ(s) =
\log M(s)</span>. Let <span class="math display">
    M(s, x) = E[1(X\le x) e^{sX}]
</span> be the <em>incomplete moment generating function</em>.</p>
<p>The Esscher transform of the random variable <span
class="math inline">X</span> with parameter <span
class="math inline">s</span> has cumulative distribution function <span
class="math display">
    F_s(x) = M(s, x)/M(s) = E[1(X\le x) e^{s X - κ(s)}].
</span> Let <span class="math inline">ε_s(x) = e^{s x - κ(s)}</span> and
note <span class="math inline">E[ε_s(X)] = 1</span> so <span
class="math inline">F_s</span> is a cdf.</p>
<p>Write <span class="math inline">E_s</span> for the Esscher
transformed measure so for any reasonable function <span
class="math inline">g</span> we have <span class="math inline">E_s[g(X)]
= E[g(X)ε_s(X)]</span> so <span class="math display">
    ∂_s E_s[g(X)] = ∂_s E[g(X) ε_s(X)] = E[g(X) ε_s(X) (X - κ&#39;(s))]
= E_s[g(X) (X - κ&#39;(s))]
</span> since <span class="math inline">∂_s ε_s(x) = ε_s(x) (x -
κ&#39;(s))</span>. Define the <em>partial cumulant</em> by <span
class="math inline">κ(s, x) = \log M(s,x)</span> so <span
class="math display">
    ∂_s F_s(x) = ∂_s e^{κ(s, x) - κ(s)} = F_s(x) (κ&#39;(s, x) -
κ&#39;(s)).
</span></p>
</section>
<section id="distributions" class="level2">
<h2>Distributions</h2>
<p>We gather some facts about the distributions and cumulants of
particular random variables.</p>
<section id="discrete" class="level3">
<h3>Discrete</h3>
<p>A discrete random variable is defined by the values it can have,
<span class="math inline">\{x_j\}</span>, and the probability it takes
on those values, <span class="math inline">P(X = x_j) = p_j</span>,
where <span class="math inline">p_j &gt; 0</span> and <span
class="math inline">\sum_j p_j = 1</span>. It has cdf <span
class="math inline">F(x) = \sum_j 1(x\le x_j) p_j</span> and density
<span class="math inline">f(x) = \sum_j δ_{x_j}(x) p_j</span> where
<span class="math inline">δ_a</span> is the <em>delta function</em>, or
<em>point mass</em>, at <span class="math inline">a</span>. The Esscher
transform of a discrete random variable is discrete and takes the same
values with <span class="math inline">P(X_s = x_j) ε_s(x_j)
p_j</span>.</p>
</section>
<section id="normal" class="level3">
<h3>Normal</h3>
<p>The standard normal random variable <span
class="math inline">X</span> has density <span class="math inline">φ(x)
= \exp(-x^2/2)/\sqrt{2\pi}</span>, <span class="math inline">-\infty
&lt; x &lt; \infty</span>. The cdf can be expressed in terms of
<em>error functions</em> as <span class="math inline">Φ(x) = (1 +
\operatorname{erf}(x/\sqrt{2}))/2 = 1 -
\operatorname{erfc}(x/\sqrt{2})/2</span>. Since <span
class="math inline">\exp(sx - s^2/2) \exp(-x^2/2) = \exp(-(x -
s)^2/2)</span> the Esscher transformed density is <span
class="math inline">φ_s(x) = φ(x - s)</span>.</p>
<p>The derivatives of the density are <span
class="math inline">φ^{(n)}(x) = (-1)^nφ(x)H_n(x)</span> where <span
class="math inline">H_n</span> are the Hermite polynomials. They satisfy
the recurrence <span class="math inline">H_0(x) = 1</span>, <span
class="math inline">H_1(x) = x</span> and <span
class="math inline">H_{n+1}(x) = x H_n(x) - n H_{n-1}(x)</span>, <span
class="math inline">n\ge 1</span>.</p>
<!--
Some useful properties are 
$$
\begin{aligned}
E[e^{\mu + \sigma X}] &= \int_{-\infty}^\infty e^{\mu + \sigma x} e^{-x^2/2}\, dx/\sqrt{2\pi} \\
    &= \int_{-\infty}^\infty e^{\mu + \sigma^2/2} e^{-(x - \sigma)^2/2}\, dx/\sqrt{2\pi} \\
    &= e^{\mu + \sigma^2/2} \\
\end{aligned}
$$
so $E[\exp(N)] = \exp(E[N] + \Var(N)/2)$ for any normally distributed random variable $N$.

Also
$$
\begin{aligned}
E[g(X) e^{sX}] &= \int_{-\infty}^\infty g(x) e^{sx} e^{-x^2/2}\, dx/\sqrt{2\pi} \\
    &= \int_{-\infty}^\infty g(x) e^{s^2/2} e^{-(x - s)^2/2}\, dx/\sqrt{2\pi} \\
    &= E[e^{sX}] E(g(X + s)]. \\
\end{aligned}
$$
More generally, if $N$, $N_1, \ldots$ are jointly normal then
$$
E[\exp(N)g(N_1,\ldots)] = E[\exp(N)] E[g(N_1 + \Cov(N, N_1), \ldots)].
$$
This is an elementary form of Girsanov's Theorem.

__Exercise__. _Prove this_.

Hint: $N, N_1, \ldots$ are jointly normal if and only if 
they are linear combinations of independent standard normals.
The cumulant is $κ(s) = \log E[\exp(sX)] = s^2/2$ so $κ_2 = 1$ and all other
cumulants are zero. We also have
$$
    E[\exp(s X - s^2/2)g(X)] = E[g(X + s)].
$$
Taking derivatives with respect to $s$ shows
$$
    E[\exp(s X - s^2/2)(X - s)g(X)] = E[g'(X + s)].
$$
so $E[X g(X)] = E[g'(X)]$ when $s = 0$ \cite{Stein}.

The incomplete moment generating function is $M(s,x) = E[e^{sX}1(X\le x)]
= e^{s^2/2} P(X + s\le x) = e^{s^2/2} Φ(x - s)$.

The transformed cdf is
$Φ_s(x) = P_s(X\le x) = E[1(X\le x) e^{sX - s^2/2}] = P(X + s\le x) = Φ(x - s)$
so 
$$
∂_s Φ_s(x) = E[1(X\le x) e^{sX - s^2/2}(X - s)] = -φ(x - s).
$$
-->
</section>
<section id="poisson" class="level3">
<h3>Poisson</h3>
<p>The Poisson distribution with parameter <span
class="math inline">λ</span> has density <span class="math inline">P(X =
n) = e^{-λ}λ^n/n!</span>, <span class="math inline">n\ge 0</span> and
moment generating function <span class="math inline">M(s) = E[e^{s X}] =
\exp(λ(e^s - 1))</span>, <span class="math inline">s &lt; λ</span>.
Since <span class="math inline">\exp(sn - λ(e^s - 1)) e^{-λ}λ^n =
\exp(-λe^s) (λ e^s)^n</span> the Esscher transformed distribution is
also Poisson with parameter <span class="math inline">λe^s</span>.</p>
<p>Taking a derivative with respect to <span
class="math inline">s</span> we have <span class="math inline">∂_s
E_s[g(X_λ)]] = λ e^s E[g(X_{λ e^s} + 1) - g(X_{λ e^s})]</span> so <span
class="math display">
∂_s E_s[1(X_λ \le x)] = λ e^s e^{-λ e^s} (λe^s)^n/n!
</span> where <span class="math inline">λ e^s &lt; n \le λ e^s +
1</span>. <!--
$$
\begin{aligned}
    \frac{d}{ds} E_s[g(X_λ)] &= \frac{d}{ds} E[g(X_λ)e^{s X - κ(s)}] \\
    &= \sum_{n\ge0} g(n)\frac{d}{ds}\bigl(e^{s n - λ(e^s - 1)}\bigr)e^{-λ}λ^n/n! \\
    &= \sum_{n\ge0} g(n)\bigl(e^{s n - λ(e^s - 1)}(n -  λe^s)\bigr)e^{-λ}λ^n/n! \\
    &= \sum_{n\ge0} g(n)\bigl(e^{s n - λe^s}(n -  λe^s)\bigr) λ^n/n! \\
    &= \sum_{n\ge0} g(n) \bigl(e^{s n}(n -  λe^s)\bigr) e^{-λ e^s} λ^n/n! \\
    &= \sum_{n\ge0} g(n) e^{s n} (n - λe^s) e^{-λ e^s} λ^n/n!) \\
    &= \sum_{n\ge0} g(n) (n - λe^s) e^{-λ e^s} (λe^s)^n/n!) \\
    &= \sum_{n\ge0} g(n) n e^{-λ e^s} (λe^s)^n/n! - λe^s  e^{-λ e^s} (λe^s)^n/n! \\
    &= \sum_{n\ge0} g(n) λe^s e^{-λ e^s} (λe^s)^{n-1}/(n-1)! - λe^s  e^{-λ e^s} (λe^s)^n/n! \\
    &= \sum_{n\ge0} g(n+1) λe^s e^{-λ e^s} (λe^s)^n/n! - \sum_{n\ge0} g(n) λe^s  e^{-λ e^s} (λe^s)^n/n! \\
    &= λ e^s E[g(X_{λ e^s} + 1) - g(X_{λ e^s})] \\
\end{aligned}
$$
--></p>
</section>
<section id="exponential" class="level3">
<h3>Exponential</h3>
<p>The density of an exponential with parameter <span
class="math inline">λ</span> is <span class="math inline">f(x) =
λ\exp(-λ x)</span>, <span class="math inline">x\ge 0</span>. The moment
generating function is <span class="math inline">M(s) = E[\exp sX] =
\int_0^\infty \exp(sx) λ\exp(-λ x)\,dx = λ/(λ - s)</span>, <span
class="math inline">s &lt; λ</span>. The Esscher transformed density is
also a an exponential distribution with parameter <span
class="math inline">λ</span> with parameter <span class="math inline">λ
- s</span>.</p>
</section>
<section id="generalized-logistic" class="level3">
<h3>Generalized Logistic</h3>
<p>A generalized logistic random variate has probability density
function <span class="math inline">f(α,b;x) = c e^{-βx}/(1 + e^{-x})^{α
+ β}</span>, <span class="math inline">-\infty &lt; x &lt;
\infty</span>, where <span class="math inline">c = 1/B(α,β)</span> is
the Beta function. If <span class="math inline">α = 1</span> and <span
class="math inline">β = 1</span> this is the standard logistic function.
The Esscher transformed density is also a generalized logistic with
parameters <span class="math inline">α + s</span>, <span
class="math inline">β - s</span>.</p>
<p>Using <span class="math inline">u = 1/(1 + e^{-x})</span>, so <span
class="math inline">e^x = u/(1 - u)</span> and <span
class="math inline">dx = du/u(1-u)</span> the moment generating function
is <span class="math display">
\begin{aligned}
E[e^{sX}] &amp;= c \int_{-\infty}^\infty e^{sx} e^{-βx}/(1 + e^{-x})^{α
+ β}\,dx \\
    &amp;= c \int_0^1 (u/(1 - u))^{s-β} u^{α + β}\,du/u(1 - u) \\
    &amp;= c \int_0^1 u^{α + s - 1} (1 - u)^{β - s - 1}\,du \\
    &amp;= c B(α + s, β - s)\\
\end{aligned}
</span> where <span class="math inline">B(α,β)</span> is the Beta
function. Since <span class="math inline">1 = cB(α, β)</span> <span
class="math display">
    M(s) = B(α + s, β - s)/B(α, β).
</span> A similar calculation shows the incomplete moment generating
function is <span class="math display">
    M(s,x) = B(α + s, β - s; u)/B(α, β) = I_u(α + s, β - s)
</span> where <span class="math inline">u = 1/(1 + e^{-x})</span>, <span
class="math inline">B(α, β; u)</span> is the incomplete Beta function,
and <span class="math inline">I_u(α, β)</span> is the regularized
incomplete Beta function.</p>
<p>The Esscher transformed cumulative distribution function is <span
class="math display">
    F_s(x) = B(α + s, β - s; u)/B(α + s, β - s) = I_u(α + s, β - s).
</span></p>
<p>Next we show <span class="math inline">∂_x^n e^{-β x}(1 + e^{-x})^{-
α - β} = \sum_{k=0}^n A_{n,k} (e^{-x})^{β + k}(1 + e^{-x})^{- α - β -
k}</span> for coefficients <span class="math inline">A_{n,k}</span>,
<span class="math inline">0\le k\le n</span>, not depending on <span
class="math inline">x</span>. Clearly <span class="math inline">A_{0,0}
= 1</span>. <span class="math display">
\begin{aligned}
∂_x^n e^{-β x}(1 + e^{-x})^{- α - β} &amp;= ∂_x\sum_{k=0}^{n-1}
A_{n-1,k} (e^{-x})^{β + k}(1 + e^{-x})^{- α - β - k} \\
    &amp;= \sum_{k=0}^{n-1} A_{n-1,k} \left((β + k)(e^{-x})^{β + k -
1}(-e^{-x})(1 + e^{-x})^{- α - β - k}
       + (e^{-x})^{β + k}(- α - β - k)(1 + e^{-x})^{- α - β - k - 1}
(-e^{-x})\right)\\
    &amp;= \sum_{k=0}^{n-1} A_{n-1,k} \left(-(β + k)(e^{-x})^{β + k}(1 +
e^{-x})^{- α - β - k}
       + (α + β + k) (e^{-x})^{β + k + 1}(1 + e^{-x})^{- α - β - k -
1}\right)\\
    &amp;= \sum_{k=0}^{n-1} A_{n-1,k} (-(β + k))(e^{-x})^{β + k}(1 +
e^{-x})^{- α - β - k}
       + \sum_{k=1}^n A_{n-1,k-1} (α + β + k - 1) (e^{-x})^{β + k}(1 +
e^{-x})^{- α - β - k}\\
    &amp;= A_{n-1,0} (-β)(e^{-x})^β(1 + e^{-x})^{- α - β}
        + \sum_{k=1}^{n-1}
            A_{n-1,k} (-(β + k)) + A_{n-1,k-1}(α + β + k - 1))
(e^{-x})^{β + k}(1 + e^{-x})^{- α - β - k}
       + A_{n-1, n-1} (α + β + n - 1) (e^{-x})^{β + n}(1 + e^{-x})^{- α
- β - n}\\
\end{aligned}
</span> so <span class="math inline">A_{n,0} = (-b)^n A_{n-1,0}</span>,
<span class="math inline">A_{n,k} = -(β + k)A_{n-1,k} + (α + β + k - 1)
A_{n-1,k-1}</span>, <span class="math inline">0 &lt; k &lt; n</span>,
and <span class="math inline">A_{n,n} = (α + β + n - 1) A_{n-1,
n-1}</span>. If we define <span class="math inline">A_{n,-1} = 0 =
A_{n,n+1}</span> <span class="math display">
    A_{n,k} = -(β + k)A_{n-1,k} + (α + β + k - 1) A_{n-1,k-1}
</span> for <span class="math inline">n &gt; 0</span> and <span
class="math inline">A_{0,0} = 1</span>.</p>
<p>The cumulant of the generalized logistic is <span
class="math inline">κ(s) = \log B(α + s, β - s)/B(α, β) = \log Γ(α + s)
- \log Γ(α) + \log Γ(β - s) - \log Γ(β)</span> using the fact <span
class="math inline">B(α,β) = Γ(α)Γ(β)/Γ(α + β)</span></p>
<!--

For the incomplete cumulant we use $y = wt$, $z = w(1 - t)$ so
$w = y + z$, $t = y/(y + z)$, and $dy\,dz = w dt\,dw$. We have
$$
\begin{aligned}
Γ(α;a) Γ(β;b) &= \int_0^a y^{α - 1} e^{-y}\,dy \int_0^b z^{β - 1} e^{-z}\,dz \\
    &= \int_0^b \int_0^a y^{α - 1} z^{β - 1} e^{-y-z}\,dy\,dz \\
    &= (\int_0^a \int_0^1 + \int_a^{2a} \int_{1-a/w}^{a/w} )
    (wt)^{α - 1} (w(1 - t))^{β - 1} e^{-w} w\,dt\,dw \\
    &= (\int_0^a \int_0^1 + \int_a^{2a} \int_{1-a/w}^{a/w} )
    w^{α + β - 1} e^{-w} t^{α - 1} (1 - t)^{β - 1} dt\,dw \\
\end{aligned}
$$

-->
<p>Recall the <em>digamma</em> function <span class="math inline">ψ(s) =
Γ&#39;(s)/Γ(s)</span> is the derivative of the log of the Gamma function
so <span class="math inline">κ^{(n+1)}(s) = ψ^{(n)}(α + s) - (-1)^n
ψ^{(n)}(β - s)</span> for <span class="math inline">n\ge 0</span>. In
particular the mean is <span class="math inline">κ&#39;(0) = ψ(α) -
ψ(β)</span> and variance is <span class="math inline">κ&#39;&#39;(0) =
ψ&#39;(α) + ψ&#39;(β)</span>.</p>
<p>Let the subscripts 1 and 2 indicate the partial derivatives with
respect to the first and second parameter respectively. Recall <span
class="math display">
    B_1(α,β;u) = B(α,β;u)\log u - u^a\;_3F_2(α, α, 1 - β; α + 1, α + 1;
u)
</span> so <span class="math inline">B_1(u)/B(u) - B_1/B = \log u</span>
omitting the parameters <span class="math inline">α</span> and <span
class="math inline">β</span>.</p>
<p>Using <span class="math inline">B(α,β;u) = 1 - B(β,α;1 - u)</span> we
have <span class="math display">
    B_2(α,β;u) = -(\log (1 - u) - ψ(β) + ψ(β + α))B(β, α;1 - u)
</span> so <span class="math inline">B_2(u)/B(u) - B_2/B = -\log (1 -
u)</span>. Since <span class="math inline">F_s(x) = B(α + s, β - s;
u)/B(α + s, β - s) = B(u)/B</span> <span class="math display">
\begin{aligned}
    ∂_s F_s(x) &amp;= \frac{B (B_1(u) - B_2(u)) - B(u)(B_1 - B_2)}{B^2}
\\
        &amp;= \frac{B(u)}{B}\left[\left(\frac{B_1(u)}{B(u)} -
\frac{B_1}{B}\right)
            - \left(\frac{B_2(u)}{B(u)} - \frac{B_2}{B}\right)\right] \\
        &amp;= F_s(x) (\log u + \log (1 - u)).
\end{aligned}
</span></p>
<p>Recall <span class="math display">
    B(α,β;u) = \frac{u^α}{α}\,_2F_1(α, 1 - β, α + 1;u).
</span> where <span class="math inline">\,_2F_1(a,b;c;x) =
\sum_{n=0}^\infty\frac{(a)_n (b)_n}{(c)_n} x^n/n!</span> is the
hypergeometric function.</p>
<p>The derivatives of the hypergeometic function are <span
class="math display">
    ∂_x^n\,_2F_1(a, b; c; x) = \frac{(a)_n (b)_n}{(c)_n}\,_2F_1(a + n, b
+ n; c + n;x).
</span></p>
</section>
</section>
<section id="acknowledgements" class="level2">
<h2>Acknowledgements</h2>
<p>The author thanks Peter Carr and Bill Goff for their helpful comments
and insightful suggestions. Any errors or omissions are due to the
author.</p>
</section>
<hr/>
<footer>
Return to <a href="index.html">index</a>.
</footer>
</body>
</html>
