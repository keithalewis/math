<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Keith A. Lewis" />
  <title>L-moments</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="math.css" />
  
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">L-moments</h1>
<p class="author">Keith A. Lewis</p>
<p class="date">Mar 25, 2025</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#l-moments" id="toc-l-moments">L-moments</a></li>
</ul>
</nav>
<p>Everything there is to know about a random variable <span
class="math inline">X</span> is determined by the probability it is less
than or equal to a given value <span class="math inline">x</span>. Its
<em>cumulative distribution function</em> is <span
class="math inline">F(x) = P(X\le x)</span>. Two random variables have
the same <em>law</em> if they have the same cdf. Every cdf is
non-decreasing, right-continuous, has left limits, tends to 0 as <span
class="math inline">x</span> goes to <span
class="math inline">-\infty</span> and tends to 1 as <span
class="math inline">x</span> goes to <span
class="math inline">+\infty</span>. Every such function defines a cdf of
a random variable. The expected value of <span
class="math inline">X</span> is the Riemann-Stieltjes integral <span
class="math inline">E[X] = \int_{-\infty}^\infty x\,dF(x)</span>. The
expected value of a function of <span class="math inline">X</span> is
<span class="math inline">E[f(X)] = \int_{-\infty}^\infty
f(x)\,dF(x)</span></p>
<p>???link???</p>
<p>There are many possible cumulative distribution functions. To get a
summary picture of a random variable it is useful to reduces the cdf to
a handful of numbers that measure <em>central tendency</em> (mean),
<em>spread</em> (standard deviation), tilt (skewness), and how peaked
(kurtosis) the distribution is. These are defined using the
<em>moments</em> of the distribution. The <span
class="math inline">n</span>-th moment is <span class="math inline">m_n
= \int_{-\infty}^\infty x^n\,dF(x)</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">m_0 =
1</span></em>.</p>
<p><em>Hint</em>: Use <span class="math inline">\int_a^b dF(x) = F(b) -
F(a)</span> and <span class="math inline">\lim_{b\to\infty}F(b) -
\lim_{a\to -\infty}F(a) = 1 - 0</span>.</p>
<p>The <em>moment generating function</em> of <span
class="math inline">X</span> is <span
class="math inline">E[e^{sX}]</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span
class="math inline">E[e^{sX}] = \sum_{n=0}^\infty m_n s^n/n!</span> if
all moments exist</em>.</p>
<p><em>Hint</em>: Use <span class="math inline">e^x = \sum_{n=0}^\infty
x^n/n!</span>.</p>
<p>The <em>central moments</em> of a distribution subtract the
<em>mean</em> <span class="math inline">m_1</span>, <span
class="math inline">\mu_n = \int_{-\infty}^\infty (x -
m_1)^n\,dF(x)</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">\mu_1 =
0</span></em>.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">\mu_n =
\sum_{k=0}^n \binom{n}{k} m_k m_1^{n-k}</span></em>.</p>
<p><em>Hint</em>: Use <span class="math inline">(x - m_1)^n =
\sum_{k=0}^n \binom{n}{k} x^k m_1^{n-k}</span>.</p>
<p>The <em>standard deviation</em> is <span class="math inline">\sigma =
\sqrt{m_2 - m_1^2}</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">\sigma
= \sqrt{\mu_2}</span></em>.</p>
<p>Every random variable with finite mean and standard deviation can be
<em>normalized</em> to have mean 0 and standard deviation 1.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">(X -
\mu)/\sigma</span> has mean 0 and standard deviation 1</em>.</p>
<p><em>Skewness</em> is the third moment of a standardized random
variable <span class="math inline">E[((X - \mu)/\sigma)^3]</span>.</p>
<p><em>Kurtosis</em> is the fourth standardized moment <span
class="math inline">E[((X - \mu)/\sigma)^4]</span>.</p>
<section id="l-moments" class="level2">
<h2>L-moments</h2>
<p>L-moments are similar to moments but are defined for any distribution
with finite mean. They are more difficult to compute but better at
summarizing the shape of a distribution. When given samples of a random
variable they provide a robust way of estimating its distribution.</p>
<p>The first L-moment of a random variable <span
class="math inline">X</span> is the mean <span
class="math inline">\lambda_1 = E[X]</span>.</p>
<p>The second L-moment is <span class="math inline">\lambda_2 =
(1/2)E[|X_2 - X_1|]</span> where <span class="math inline">X_1</span>
and <span class="math inline">X_2</span> are independent and have the
same distribution as <span class="math inline">X</span>. It is a measure
of spread even when standard deviation is not finite. It measures the
distance between two independent random samples of <span
class="math inline">X</span>.</p>
<p><strong>Exercise</strong> <em>Show <span
class="math inline">\lambda_2 = (1/2)E[\max\{X_1,X_2\} -
\min\{X_1,X_2\}]</span></em>.</p>
<p><em>Hint</em>: <span class="math inline">|x| =
\max\{x,-x\}</span>.</p>
<p>Higher order L-moments are defined using <em>order statistics</em>.
They do a better job of estimating the distribution of a random variable
given independent sample values. Given independent random variables
<span class="math inline">X_1,\ldots,X_n</span> having the same law as
<span class="math inline">X</span> define <span
class="math inline">X_{1:n}\le\ldots\le X_{n:n}</span> by sorting
pointwise. Independent samples of a random variable are functions on the
product space <span
class="math inline">X_i\colon\Omega^n\to\boldsymbol{{{R}}}</span> where
<span class="math inline">X_i(\omega_1,\ldots,\omega_n) =
\omega_i</span> equipped with the product measure <span
class="math inline">F_n(x_1,\ldots,x_n) = P(X_1\le x_1,\ldots,X_n\le
x_n) = P(X_1\le x_1)\cdots F(X_n\le x_n)</span>. The numbers <span
class="math inline">X_1(\omega), \dots, X_n(\omega)</span> can be sorted
pointwise to define the order statistics <span
class="math inline">X_{1:n}(\omega),\ldots,X_{n:n}(\omega)</span></p>
<p><strong>Exercise</strong>. <em>Show <span
class="math inline">\lambda_2 = (1/2)E[X_{2:2} -
X_{1:2}]</span></em>.</p>
<p>The third L-moment is <span class="math inline">{\lambda_3 =
(1/3)E[(X_{3:3} - X_{3:2}) - (X_{3:2} - X_{3:1})] = E[X_{3:3} - 2X_{3:2}
+ X_{3:1}]}</span>. If you sample <span class="math inline">X</span>
three times this measures the difference between the high and mid and
the difference between the mid and the low. This is a measure of
skewness. If it is positive then the distribution has more mass toward
the right. If it is negative then the distribution has more mass toward
the left.</p>
<p>The fourth L-moment is <span class="math inline">\lambda_3 =
(1/4)E[(X_{4:4} - X_{4:3}) - (X_{4:3} - X_{4:2}) - ((X_{4:3} - X_{4:2})
- (X_{4:2} - X_{4:1}))] = (1/4)E[X_{4:4} - 3X_{4:3} + 3X_{4:2} -
X_{4:1}]</span>.</p>
<p>The general theme of L-moments is to consider <span
class="math inline">\Delta X_{n:j} = X_{n:j+1} - X_{n:j}</span>. The
second L-moment is <span class="math inline">E[\Delta X_{2:1}]</span>.
The third L-moment is <span class="math inline">E[\Delta\Delta
X_{3:1}]</span> the <span class="math inline">n</span>-th L-moment is
<span class="math inline">E[\Delta^{n-1}X_{n:1}]</span>.</p>
</section>
</body>
</html>
