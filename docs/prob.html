<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Keith A. Lewis" />
  <title>Probability</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="math.css" />
  <!--
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Serif&display=swap" rel="stylesheet"> 
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Literata:wght@300&display=swap" rel="stylesheet"> 
  <link href="https://fonts.googleapis.com/css2?family=Markazi+Text:wght@500&display=swap" rel="stylesheet"> 
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Serif:wght@400;500&display=swap" rel="stylesheet">
  -->
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Probability</h1>
<p class="author">Keith A. Lewis</p>
<div class="abstract">
<div class="abstract-title">Abstract</div>
Probability Theory
</div>
</header>

<section id="probability-theory" class="level1">
<h1>Probability Theory</h1>
<p>A <em>random variable</em> is a function on a <em>probability
space</em>. A probability space is a set of <em>outcomes</em>, <span
class="math inline">\Omega</span>, and a <em>probability measure</em> on
<span class="math inline">\Omega</span>.</p>
<p>A <em>probability measure</em> is a measure <span
class="math inline">P</span> on a set <span
class="math inline">\Omega</span> with <span
class="math inline">P(E)\ge0</span>, <span
class="math inline">E\subseteq\Omega</span>, and <span
class="math inline">P(\Omega) = 1</span>.</p>
<p>If <span class="math inline">\Omega</span> is countable we can define
a probability measure by specifying <span class="math inline">p_\omega =
P(\{\omega\})</span> for <span
class="math inline">\omega\in\Omega</span>. Note <span
class="math inline">p_\omega\ge 0</span> and <span
class="math inline">\sum_{\omega\in\Omega} p_\omega = 1</span>. The
probability of the event <span
class="math inline">E\subseteq\Omega</span> is <span
class="math inline">P(E) = \sum_{\omega\in E} p_\omega</span>.</p>
<section id="random-variable" class="level2">
<h2>Random Variable</h2>
<p>A <em>random variable</em> on a probability space <span
class="math inline">\langle\Omega,P\rangle</span> is a function <span
class="math inline">X\colon\Omega\to\mathbf{R}</span>. Its
<em>cumulative distribution function</em> is <span
class="math inline">F(x) = P(X\le x) = P(\{\omega\in\Omega\mid X(\omega)
\le x\})</span>. More generally, given a subset <span
class="math inline">A\subseteq\mathbf{R}</span> the probability that
<span class="math inline">X</span> takes a value in <span
class="math inline">X</span> is <span class="math inline">P(X\in A) =
P(\{\omega\in\Omega\}\mid X(\omega\in A))\}</span>. Two random variables
have the same <em>law</em> if they have the same cdf.</p>
<p>Random variables are symbols that can be used in place of a number
when manipulating equations and inequalities with with additional
information about the probability of the values it can take on.</p>
<p>The cdf tells you everything there is to know about the probability
of the values the random variable can take on. For example, <span
class="math inline">P(a &lt; X \le b) = F(b) - F(a)</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">P(a\le
X\le b) = \lim_{x\uparrow a} F(b) - F(x)</span></em>.</p>
<p><em>Hint</em>: <span class="math inline">[a,b] = \cap_n (a - 1/n,
b]</span>. Note <span class="math inline">\cup_n (-\infty,x - 1/n] =
(-\infty,x) \not= (-\infty,x]</span>. The sequence <span
class="math inline">F(x - 1/n)</span> is non-decreasing and bounded by
<span class="math inline">F(x)</span> so it has a limit, but not
necessarily <span class="math inline">F(x)</span>.</p>
<p>In general <span class="math inline">P(X\in A) = \int_A dF(x)</span>
for sufficiently nice subsets <span
class="math inline">A\subset\mathbf{R}</span> where we are using <a
href="https://mathworld.wolfram.com/StieltjesIntegral.html">Riemann–Stieltjes</a>
integration.</p>
<p>Every cdf is non-decreasing, continuous from the right, has left
limits, and <span class="math inline">\lim_{x\to-\infty}F(x) = 0</span>,
<span class="math inline">\lim_{x\to+\infty}F(x) = 1</span>. Any
function with these properties is the cdf of a random variable.</p>
<p><strong>Exercise</strong>: <em>Show for any cumulative distribution
function <span class="math inline">F</span> that <span
class="math inline">F(x) \le F(y)</span> if <span class="math inline">x
&lt; y</span>, <span class="math inline">\lim_{x\to -\infty} F(x) =
0</span>, <span class="math inline">\lim_{x\to\infty} F(x) = 1</span>,
and <span class="math inline">F</span> is right continuous with left
limits</em>.</p>
<p><em>Hint</em>: For right continuity use <span
class="math inline">(-\infty, x] = \cap_n (-\infty, x + 1/n]</span>.</p>
<p>The distribution of a <em>uniformly distributed</em> random variable
on <span class="math inline">[0,1]</span>, <span
class="math inline">U</span>, is <span class="math inline">F(x) =
x</span> if <span class="math inline">0\le x\le 1</span>, <span
class="math inline">F(x) = 0</span> if <span class="math inline">x &lt;
0</span>, and <span class="math inline">F(x) = 1</span> if <span
class="math inline">x &gt; 1</span>. In this case <span
class="math inline">P(X\in(a, b]) = b - a</span> for <span
class="math inline">0\le a \le b\le 1</span> and <span
class="math inline">P(U &lt; 0) = 0 = P(U &gt; 1)</span>.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">X</span>
has cdf <span class="math inline">F</span>, then <span
class="math inline">X</span> and <span
class="math inline">F^{-1}(U)</span> have the same law</em>.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">X</span>
has cdf <span class="math inline">F</span>, then <span
class="math inline">F(X)</span> and <span class="math inline">U</span>
have the same law</em>.</p>
<p>If <span class="math inline">F(x)</span> jumps from <span
class="math inline">a</span> to <span class="math inline">b</span> at
<span class="math inline">x = c</span> we define <span
class="math inline">F^{-1}(u) = c</span> for <span class="math inline">a
\le u &lt; b</span>.</p>
<p>This shows a uniformly distributed random variable has sufficient
randomness to generate any random variable. There are no random, random
variables.</p>
<p>Given a cdf <span class="math inline">F</span> we can define a random
variable having that distribution using the identity function <span
class="math inline">X\colon\mathbf{R}\to\mathbf{R}</span>, where <span
class="math inline">X(x) = x</span>. Let <span
class="math inline">P</span> be the probability measure on <span
class="math inline">\mathbf{R}</span> defined by <span
class="math inline">P(A) = \int_A dF(x)</span>.</p>
<p>The mathematical definition is more flexible than defining a random
variable by its cumulative distribution function.</p>
<section id="continuous-random-variable" class="level3">
<h3>Continuous Random Variable</h3>
<p>If the cdf satisfies <span class="math inline">F(x) =
\int_{-\infty}^x F&#39;(u)\,du</span> we say the random variable is
<em>continuously distributed</em>. The <em>density function</em> is
<span class="math inline">f = F&#39;</span>. Any function satisfying
<span class="math inline">f\ge 0</span> and <span
class="math inline">\int_\mathbf{R} f(x)\,dx = 1</span> is a density
function for a random variable.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">X</span>
is continuously distributed show <span class="math inline">P(a &lt; X
\le b) = P(a \le X \le b) = P(a \le X &lt; b) = P(a \le X \le
b)</span></em>.</p>
</section>
<section id="discrete-random-variable" class="level3">
<h3>Discrete Random Variable</h3>
<p>If the set <span class="math inline">\{\omega\in\Omega\mid X(\omega)
\not= 0, P(\{\omega\}) \not=0\}</span> is countable then <span
class="math inline">X</span> is <em>discretely distributed</em>. If the
non-zero values are <span class="math inline">\{x_j\}</span> occuring at
<span class="math inline">\{\omega_j\}</span> then <span
class="math inline">p_j = P(\{\omega_j\})</span> determine <span
class="math inline">X</span>.</p>
</section>
<section id="expected-value" class="level3">
<h3>Expected Value</h3>
<p>The <em>expected value</em> of a random variable is defined by <span
class="math inline">E[X] = \int_\Omega X\,dP</span>. More generally, the
expected value of a function <span
class="math inline">f\colon\bm{R}\to\bm{R}</span> of a random variable
is <span class="math inline">E[f(X)] = \int_\Omega f(X)\,dP</span> where
<span class="math inline">f(X)\colon\Omega\to\bm{R}</span> via <span
class="math inline">f(X)(\omega) = f(X(\omega))</span>, <span
class="math inline">\omega\in\Omega</span>.</p>
<p>The <em>variance</em> of a random variable <span
class="math inline">X</span> is <span
class="math inline">\operatorname{Var}(X) = E[(X - E[X])^2]= E[X^2] -
E[X]^2</span>.</p>
<p><strong>Lemma</strong>. (Chebyshev) <em>If <span
class="math inline">f</span> is non-negative then <span
class="math inline">P(f(X) &gt; \lambda) \le
E[f(X)]/\lambda</span></em>.</p>
<p><em>Proof</em>. We have <span class="math inline">E[f(X)] \ge
E[f(X)1(f(X) &gt; \lambda) \ge \lambda P(f(X) &gt; \lambda)</span>.</p>
<p>An immediate corollaries are <span class="math inline">P(|X| &gt;
\lambda) \le E[|X|]/\lambda</span> and <span class="math inline">P(|X -
E[X]| &gt; \lambda) \le \mathrm{Var}(X)/\lambda^2</span>. Note this only
has import for large <span class="math inline">\lambda</span>.</p>
</section>
<section id="moments" class="level3">
<h3>Moments</h3>
<p>The <em>moments</em> of a random variable are <span
class="math inline">m_n = E[X^n]</span> where <span
class="math inline">n</span> is a non-negative integer. The <em>moment
generating function</em> is <span class="math inline">M(s) = E[e^{sX}] =
\sum_{n=0}^\infty m_n s^n/n!</span>. Note <span
class="math inline">M^{(n)}(0) = m_n</span>.</p>
<p>Moments don’t necessarily exist for all <span
class="math inline">n</span>, except for <span class="math inline">n =
0</span>. They also cannot be an arbitrary sequence of values.</p>
<p>Suppose all moments of <span class="math inline">X</span> exist, then
for any complex numbers, <span class="math inline">(c_i)</span>, <span
class="math inline">0 \le E|\sum_i c_i X^i|^2 = E\sum_{j,k} c_j\bar{c_k}
X^{j+k} = \sum_{j,k} c_j \bar{c_k} m_{j+k}</span>. This says the Hankel
matrix, <span class="math inline">M = [m_{j+k}]_{j,k}</span>, is
positive definite. The converse is also true: if the Hankel matrix is
positive definite there exists a random variable with the corresponding
moments. This is not a trivial result and the random variable might not
be unique.</p>
<p>% Dunford Schwartz Volume 2 pg 1251. % Extending unbounded symmetric
operators. Deficiency index.</p>
</section>
<section id="cumulant" class="level3">
<h3>Cumulant</h3>
<p>The <em>cumulant</em> of <span class="math inline">X</span> is the
natural logarithm of its moment generating function <span
class="math inline">\kappa(s) = \log E[e^sX]</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">κ_X(0)
= 0</span>, <span class="math inline">κ_X&#39;(0) = E[X]</span>, and
<span class="math inline">κ_X&#39;&#39;(0) =
\operatorname{Var}(X)</span></em>.</p>
<p><strong>Exercise</strong>. <em>Show if <span
class="math inline">c</span> is a constant then <span
class="math inline">κ_{c + X}(s) = cs + κ_X(s)</span></em>.</p>
<p><strong>Exercise</strong>. <em>Show if <span
class="math inline">c</span> is a constant then <span
class="math inline">κ_{cX}(s) = κ_X(cs)</span></em>.</p>
<p><strong>Exercise</strong>. <em>Show if <span
class="math inline">X</span> and <span class="math inline">Y</span> are
independent then <span class="math inline">κ_{X + Y}(s) = κ_X(s) +
κ_Y(s)</span></em>.</p>
<p>The <em>cumulants</em> <span class="math inline">(κ_n)</span> are the
coefficients of the power series expansion <span
class="math inline">κ(s) = \sum_{n&gt;0}κ_n s^n/n!</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">κ_1 =
E[X]</span> and <span class="math inline">κ_2 =
\mathrm{Var}(X)</span></em>.</p>
<p>The third and fourth cumulants are related to skew and kurtosis. If
the variance is 1, then <span class="math inline">κ_3</span> is the skew
and <span class="math inline">κ_4</span> is the <a
href="https://en.wikipedia.org/wiki/Kurtosis#Excess_kurtosis">excess
kurtosis</a>.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">κ_1(c +
X) = c + κ_1(X)</span> and <span class="math inline">κ_n(c + X) =
κ_n(X)</span>, <span class="math inline">n \ge 2</span></em>.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">κ_n(cX)
= c^n κ_n(X)</span> for all <span class="math inline">n</span></em>,
<span class="math inline">n\ge 1</span>.</p>
<p><strong>Exercise</strong>. <em>Show if <span
class="math inline">X</span> and <span class="math inline">Y</span> are
independent <span class="math inline">κ_n(X + Y) = κ_n(X) +
κ_n(Y)</span> for all <span class="math inline">n</span></em>.</p>
<p>The moments of <span class="math inline">X</span>, <span
class="math inline">\mu_n = E[X^n]</span> are related to the cumulants
via complete Bell polynomials <span
class="math inline">B_n(κ_1,\ldots,κ_n)</span>. <span
class="math display">
    E[e^{sX}] = \sum_{n\ge0} \mu_n s^n/n! = e^{κ(s)} = e^{\sum_{n&gt;0}
κ_n s^n/n!}
    = \sum_{n\ge0} B_n(κ_1,\ldots,κ_n) s^n/n!
</span> Taking a derivative with respect to <span
class="math inline">s</span> of the last equality gives the recurrence
formula <span class="math display">
    B_0 = 1, B_{n+1}(κ_1,\ldots,κ_{n+1})
        = \sum_{k = 0}^n \binom{n}{k} B_{n - k}(κ_1,\ldots,κ_{n-k})κ_{k
+ 1}, n &gt; 0.
</span></p>
<p>The cumulants are related to the moments via partial Bell polynomials
<span class="math inline">B_{n,k}(\mu_1,\ldots,\mu_{n - k - 1}</span>
<span class="math display">
    κ_n = \sum_{k=0}^{n-1} (-1)^k k! B_{n,k+1}(\mu_1,\ldots,\mu_{n - k})
</span> where <span class="math inline">(B_{n,k})</span> are partial
Bell polynomials satisfying the recurrence <span
class="math inline">B_{0,0} = 1</span>, <span
class="math inline">B_{n,0} = 0</span> for <span class="math inline">n
&gt; 0</span>, <span class="math inline">B_{0,k} = 0</span> for <span
class="math inline">k &gt; 0</span> and <span class="math display">
    B_{n,k}(x_1,\ldots,x_{n - k + 1})
        = \sum_{i=1}^{n-k+1}\binom{n-1}{i - 1}
B_{n-i,k-1}(x_1,\ldots,x_{n - i - k + 2})x_i
</span></p>
</section>
</section>
<section id="jensens-inequality" class="level2">
<h2>Jensen’s Inequality</h2>
<p>A function <span class="math inline">\phi\colon\bm{R}\to\bm{R}</span>
is <em>convex</em> if <span class="math inline">\phi(x) =
\sup_{\lambda\le\phi} \lambda(x)</span> and <em>concave</em> if <span
class="math inline">\phi(x) = \inf_{\lambda\ge\phi} \lambda(x)</span>
where <span class="math inline">\lambda</span> is linear.</p>
<p><strong>Exercise</strong>. <em>If <span
class="math inline">\phi</span> is convex then <span
class="math inline">\phi(tx + (1 - t)x&#39;) \le t\phi(x) + (1 -
t)\phi(x&#39;)</span> if <span class="math inline">0\le t\le
1</span></em>.</p>
<p>Hint: <span class="math inline">\lambda(x) = \phi(x^*) + m(x - x^*)
\le \phi(x)</span> where <span class="math inline">x^* = t\phi(x) + (1 -
t)\phi(x&#39;)</span> and <span class="math inline">m = (\phi(x&#39;) -
\phi(x))/(x&#39; - x)</span>.</p>
<p><strong>Theorem</strong>. <em>If <span
class="math inline">\phi</span> is convex then <span
class="math inline">E[\phi(X)] \ge \phi(E[X])</span></em>.</p>
<p>For <span class="math inline">\lambda\le\phi</span> linear we have
<span class="math inline">E[\phi(X)] \ge E[\lambda(X)] =
\lambda(E[X])</span>.</p>
<section id="convergence" class="level3">
<h3>Convergence</h3>
<p>Random variables <span class="math inline">X_n</span> converge to
<span class="math inline">X</span> in <em>mean</em> if <span
class="math inline">E[|X_n - X|]</span> converges to 0. They converges
in <em>mean square</em> if <span
class="math inline">\operatorname{Var}(X_n - X)</span> converges to 0.
They converges <em>in probability</em> if for all <span
class="math inline">\epsilon &gt; 0</span> it is evenually the case that
<span class="math inline">P(|X_n - X|) &gt; \epsilon) &lt;
\epsilon</span>. They converge <em>almost surely</em> if <span
class="math inline">P(\lim_n X_n = X) = 1</span>.</p>
<p><strong>Lemma</strong>. (Chebyshev) <em>For any non-negative random
variable <span class="math inline">X</span>, <span
class="math inline">P(X &gt; \lambda) \le E[X]/\lambda</span></em>.</p>
<p><em>Proof</em>. <span class="math inline">E[X] \ge E[X 1(X &gt;
\lambda)] \ge \lambda P(X &gt; \lambda)</span>.</p>
<p><strong>Exercise</strong>. <em>For any non-negative random variable
<span class="math inline">X</span> and any increasing function <span
class="math inline">\phi</span>, <span class="math inline">P(X &gt;
\lambda) \le E[\phi(X)]/\phi(\lambda)</span></em>.</p>
<p><strong>Exercise</strong>. <em>If <span
class="math inline">X_n</span> converges in mean square then it
converges in probability</em>.</p>
<p>Hint: <span class="math inline">\phi(x) = x^2</span> is increasing
for <span class="math inline">x &gt; 0</span>.</p>
<!--

## Joint Distribution

Two random variables, $X$ and $Y$, are defined by their _joint
distribution_, $H(x,y) = P(X\le x, Y\le y)$.  For example, the point $(X,Y)$ is
in the square $(a,b]\times (c,d]$ with probability
$P(a < X \le b, c < Y \le d) = P(X \le b, Y \le d) - P(X \le a) - P(Y \le c) + P(X \le a, Y \le c)$.

The _marginal distbutions_ are $F(x) = H(x,\infty)$ and $G(y) =  H(\infty,y)$,
where $F$ and $G$ are the cumulative distributions of $X$ and $Y$ respectively.

In general, the joint distribution of $X_1$, \ldots, $X_n$ is
$F(x_1,\ldots,x_n) = P(X_1\le x_1,\ldots, X_n\le x_n$).


### Partition

A _partition_ splits a sample space into disjoint subsets with union
equal to the sample space. Partitions are how _partial information_
is represented.  The elements of the partition are called _atoms_. The
way paritions represent partial information is you only know what atom an
outcome belongs to, not the exact outcome.
Complete information corresponds to the _finest_ partiion of singletons
$\{\{\omega\}\mid \omega\in\Omega\}$.
No information corresponds to the _coarsest_ partition $\{\Omega\}$.

Partitions define an _equivalence relation_ on outcomes. We say $\omega\sim\omega'$
if and only if they belong to the same atom.

__Exercise__. _Show $\omega\sim\omega$, $\omega\sim\omega'$ implies $\omega'\sim\omega$,
and $\omega\sim\omega'$, $\omega'\sim\omega''$ implies $\omega\sim\omega''$_.

This is the definition of an equivalence relation. It is the mathematical way
of saying two things are the "same" even if they are not equal.

An _algebra_ of sets on $\Omega$ is a collection of subsets of $\Omega$
that contains the empty set and is closed under complement and finite
union. Closure under complements imples $\Omega$ belongs to the algebra
and De Morgan's Laws imply it is also close under finite intersection.

An element $A\subseteq S$ is an _atom_ of the algebra if $B\subseteq A$ and
$B$ in the algebra imply $B$ is either the empty set or equals $A$.

__Exercise__. _If an algebra is finite its atoms form a partition and the
smallest algebra containing the partition is equal to the algebra_.

A function is _measurable_ with respect to an algebra if the preimage
of every interval belongs to the algebra.

__Exercise__. _If an algebra is finite then a function is measurable if and
only if it is constant on atoms_.

In this case, it is a function on the atoms of the algebra.



## Examples

See ...

### Uniform

A random variable is uniformly distributed on the interval $[a, b]$ has density function
$f(x) = (x - a)/(b - a)$ if $a \le x \le b$ and $f(x) = 0$ if $x < a$ or $x > b$.

__Exercise__. _Show if $U$ is uniformly distributed on $[0,1]$ then $a(1 - U) + bU$
is uniformly distributed on $[a, b]$_.

### Cantor

Let's define a function on the interval $[0,1]$ as follows.
Every $x\in[0,1]$ can be written $x = \sum_{j>0} x_j/3^j$ where $x_j\in\{0,1,2\}$
The digits $(x_j)$ are the base 3 representaton of $x$. Define $F(x) = ???$

We have $F(0) = 0$, $F(1) = 1$, and $F$ is continuous...

The measure of the set where $F' = 0$ is 1.

### Measurable

A function $X\colon\Omega\to\mathbf{R}$ is _measurable_ with respect to the algebra $\mathcal{A}$
if $\{\omega\in\Omega : X(\omega) \le a\}$ belongs to $\mathcal{A}$ for all $a\in\mathbf{R}$.

__Exercise__. _Show $X$ is measurable if and only if it is constant on atoms of $\mathcal{A}$
when the algebra has a finite number of elements._

In this case we can write $X\colon\mathcal{A}\to\mathbf{R}$ as a function on the
atoms of $\mathcal{A}$.

!!! use more recent recurrence

%Exercise. (Inclusion-Exclusion principal) Let $S$ be a finite set and
%let $f$ be any function defined on subsets of $S$.
%Define $\phi f(T) = \sum_{U\supseteq T} f(U)$ and
%$\psi g(T) = \sum_{U\supseteq T} (-1)^{|U| - |T|} g(T)$.
%These are both operators from $2^S\to\mathbf{R}$.
%Show $\phi\psi g = g$ and $\psi\phi f = f$.

%Hint: Group the sum by $|Y| - |T|$.

## Conditional Expectation

The _conditional expectation_ of an event $B$ given an event $A$ is
$P(B|A) = P(B\cap A)/P(A)$. In some sense, this reduces the sample space to $A$
since $P(A|A) = 1$ and $P(B\cup C|A) = P(B|A) + P(C|A) - P(B\cap C|A)$.
We also have $P(A|B) = P(A\cap B)/P(B)$ so $P(A|B) = P(B|A)P(A)/P(B)$. 
This is the simplest form of Bayes Theorem. It shows how to update your degree
of belief based on new information. Every probability is conditional on information.

Define the conditional expectation of the random variable $X$ with respect
to the event $A$ by $E[X|A] = E[X 1_A]/P(A)$.  If $X = 1_B$ then
this coincides with the definition of conditional expectation above.

Define the conditional expectation of $X$ with respect to the algebra
$\mathcal{A}$, $E[X|\mathcal{A}]:\mathcal{A}\to\mathbf{R}$, by
$E[X|\mathcal{A}](A) = E[X|A]$ for $A$ an atom of $\mathcal{A}$.

## Joint Distribution

Two random variables, $X$ and $Y$, are defined by their _joint
distribution_, $H(x,y) = P(X\le x, Y\le y)$.  For example, the point $(X,Y)$ is
in the square $(a,b]\times (c,d]$ with probability
$P(a < X \le b, c < Y \le d) = P(X \le b, Y \le d) - P(X \le a) - P(Y \le c) + P(X \le a, Y \le c)$.

The _marginal distbutions_ are $F(x) = H(x,\infty)$ and $G(y) =  H(\infty,y)$,
where $F$ and $G$ are the cumulative distributions of $X$ and $Y$ respectively.

In general, the joint distribution of $X_1$, \ldots, $X_n$ is
$F(x_1,\ldots,x_n) = P(X_1\le x_1,\ldots, X_n\le x_n$).

## Independent

The random variables $X$ and $Y$ are _independent_ if $H(x,y) = F(x)G(y)$ for all $x$ and $y$.
This is equivalent to $P(X\in A,Y\in B) = P(X\in A)P(Y\in B)$ for any sets $A$ and $B$.

We also have that $Ef(X)g(Y) = Ef(X) Eg(Y)$ for any functions $f$ and $g$ whenever all expected
values exist.

__Exercise__: Prove this for the case $f = \sum_i a_i 1_{A_i}$ and $g = \sum_j b_j 1_{B_j}$.

In general, $X_1$, \ldots, $X_n$ are independent if
$F(x_1,\ldots,x_n) = F_1(x_1)\cdots F_n(x_n)$, where $F_j$ is the law of $X_j$.

## Copula

A _copula_ is the joint distribution of uniformly distributed random variables on the unit interval.
The copula of $X$ and $Y$ is the joint distribution of $F^{-1}(X)$ and $G^{-1}(Y)$ where
$F$ and $G$ are the cumulative distributions of $X$ and $Y$ respectively:
$C(u,v) = C^{X,Y}(u,v) = P(F^{-1}(X) \le u, G^{-1}(Y) \le v)$.

__Exercise__: Show $C(u,v) = H(F(u),G(v))$ where
and $H$ is the joint distribution of $X$ and $Y$ and $F$ and $G$ are the cumulative
distribution of $X$, and $Y$.

__Exercise__: Show $H(x,y) = C(F^{-1}(x), G^{-1}(y))$

This shows how to use the copula and marginal distributions to recover the joint distribution.

An equivalent definition is a copula is a probability measure on $[0,1]^2$ with uniform
marginals. 

__Exercise__: Prove this.

If $U$ and $V$ are independent, uniformly distributed random variables on the unit interval
then $C(u,v) = uv$.

If $V=U$ then their joint distribution is
$C(u,v) = P(U\le u, V\le v) = P(U\le u, U\le v) = P(U\le \min\{u, v\}) = \min\{u,v\} = M(u,v)$.

If $V=1-U$ then their joint distribution is $C(u,v) = P(U\le u, V\le v) = P(U\le u, 1-U\le v)
= P(1-v\le U\le u) = \max\{u - (1 -v), 0\} = \max\{u + v - 1, 0\} = W(u,v)$

__Exercise__: (Fr&#233;chet-Hoeffding) For every copula, $C$, $W \le C \le M$.

Hint: For the upper bound use $H(x,y) \le F(x)$ and $H(x,y) \le G(y)$.
For the lower bound note $0\le C(u_1,v_1) - C(u_1, v_2) - C(u_2, v_1) + C(u_2, v_2)$
for $u_1 \ge u_2$ and $v_1 \ge v_2$.

## Examples

### Discrete

A _discrete_ random variable, $X$, is defined by
$x_i\in\mathbf{R}$ and $p_i > 0$ with $\sum p_i = 1$.
The probability the random variable takes on value $x_i$ is P(X = x_i) = $p_i$.

If a discrete random variable takes on a finite number of values, $n$, then
if $p_i = 1/n$ for all $i$ the variable is called _discrete uniform_.

### Bernoulli

A _Bernoulli_ random variable is a discrete random variable with $P(X = 0) = p$, $P(X = 1) = 1 - p$.

### Binomial

A _Binomial_ random variable is a discrete random variable with
$P(X = k) = \binom{n}{k}p^k(1-p)^{n-k}$, $k = 0$, \ldots, $n$.

### Uniform

A _continuous uniform_ random variable on the interval $[a,b]$ has density
$f(x) = 1_{[a,b]}/(b - a)$.

A _discrete uniform_ random variable on $\Omega = \{x_1,\ldots,x_n\}$
has $P(X = x_j) = 1/n$ for $j = 1,\ldots,n$.

### Normal

The _standard normal_ random variable, $Z$, has density function $\phi(x) = \exp(-x^2/2)/\sqrt{2\pi}$.

If $X$ is normal then $E\exp(N) = \exp(E[N] + \mathrm{Var}(N)/2)$ so the cumulants satisfy
$κ_n = 0$ for $n > 2$.

This follows from 
$$
\begin{aligned}
E[e^N] &= E[e^{\mu + \sigma Z}] \\
       &= \int_{-\infty}^\infty e^{\mu + \sigma z} e^{-z^2/2}\,dz/\sqrt{2\pi}\\
       &= e^{\mu + \sigma^2/2} \int_{-\infty}^\infty e^{-(z - \sigma)^2/2}\,dz/\sqrt{2\pi}\\
       &= e^{\mu + \sigma^2/2} \int_{-\infty}^\infty e^{-z^2/2}\,dz/\sqrt{2\pi}\\
       &= e^{\mu + \sigma^2/2}
\end{aligned}
$$

For any normal random variable, $N$, $E[e^N f(N)] = E[e^N] E[f(N + \mathrm{Var}(N)]$.

__Exercise__. Prove this by first showing $E[e^{\sigma Z} f(Z)] = e^{\sigma^2/2} E[f(Z + \sigma)]$.

If $N$, $N_1$, \ldots, are jointly normal then
$E[e^N f(N_1,\ldots)] = E[e^N] E[f(N_1 + \Cov(N,N_1),\ldots)]$.

### Poisson

A _Poisson_ random variable with parameter $\lambda$ is defined by
$P(X = k) = e^{-\lambda}\lambda^k/k!$ for $k = 0, 1, \ldots$.

If $X$ is Poisson with parameter $\lambda$ then 
$$
\begin{aligned}
Ee^{sX} &= \sum_{k=0}^\infty e^{sk} e^{-\lambda}\lambda^k/k!\\
        &= \sum_{k=0}^\infty  (e^s\lambda)^ke^{-\lambda}/k!\\
        &= \exp(\lambda(e^s - 1))
\end{aligned}
$$
so $κ(s) = \lambda(e^s - 1)$ and $κ_n = \lambda$ for all $n$.

### Infinitely Divisible

A random variable, $X$, is _infinitely divisible_ if for any positive integer, $n$,
there exist independent, identically distributed random variables $X_1$,\ldots,$X_n$
such that $X_1 + \cdots + X_n$ has the same law as $X$.

A theorem of Kolmogorov states for every infinitely divisible random variable the exists
a number $\gamma$ and a non-decreasing function $G$ with

$$
κ(s) = \log E e^{sX} = \gamma s + \int_{-\infty}^\infty K_s(x)\,dG(x),
$$

where $K_s(x) = (e^{sx} - 1 - sx)/x^2 = \sum_{n=2}^\infty x^{n-2}s^n/n!$.
Note if $G(x) = 1_{(-\infty,0]}$ then $κ(s) = \gamma s + K_s(0) = \gamma s + s^2/2$
so the random variable is normal.

Note the cumulants of the random variable are $κ_1 = \gamma$ and
$κ_n = \int_{-\infty}^\infty x^{n - 2}\,dG(x)$ for $n \ge 2$.

If $G(x) = a^2 1_{(-\infty,a]}$ for $a\not=0$ then
$$
\begin{aligned}
κ(s) &= \gamma s + a^2 K_s(a)\\ 
          &= \gamma s + a^2 \sum_{n=2}^\infty a^{n-2}s^n/n!\\ 
          &= \gamma s + \sum_{n=2}^\infty a^n s^n/n!\\ 
          &= \gamma s - as + \sum_{n=1}^\infty a^n s^n/n!\\ 
          &= (\gamma - a)s + \sum_{n=1}^\infty a^n s^n/n!\\ 
\end{aligned}
$$

so the random variable is Poisson with parameter $a$ plus the constant $\gamma - a$.

This theorem states every infinitely divisible random variable can be
approximated by a normal plus a linear combination of independent Poisson
random variables.

If $X = \mu + \sigma Z + \sum_j \alpha_j a_j^2 P_j$ where $P_j$ is Poisson
with parameter $a_j$, then
$$
κ(s) = \mu s + \sigma s^2/2 + \sum_j \alpha_j (e^{a_j s} - 1) - \alpha_j s
$$


## Unsorted

### Characteristic Function

The _characteristic function_ of a random variable, $X$, is $\xi(t) = κ(it)$.

### Fourier Transform

The _Fourier transform_ is $\psi(t) = \xi(-t) = κ(-it)$.
Clearly $\psi(t) = \xi(-t)$.


## Remarks

Cheval de Mere

Pascal

Bernoulli(s)

Kolmogorov

Willy Feller

These can be used to prove the _central limit theorem_:
if $X_j$ are independent, identically distributed random variables with mean zero
and variance one, then $(X_1 + \cdots X_n)/sqrt{n}$ converges to a standard
normal random variable.

Probability and Logic (Ramsey)

Littlewood's law P(miracle) is large. 1 event per second, 1/10^6 are miracles, 8 hour days.

Birthday problem.

UUIDs

## Partition

A _partition_ of a set $\Omega$ is a collection of subsets (events)
that are _pairwise disjoint_ with union $\Omega$.
A partition $\mathcal{A} = \{A_i\}_{i\in I}$ satisfies $A_i\subseteq\Omega$ for all $i\in I$,
$A_i\cap A_j = \emptyset$ if $i \not= j$, and $\cup_{i\in I} A_i = \Omega$.
The elements $A_i$ of the partition $\mathcal{A}$ are called _atoms_.

__Exercise__. _If $\{A_i\}$ are pairwise disjoint show $A_i\cap A_j\cap
A_k = \emptyset$ if $i$, $j$, and $k$ are distinct._

__Exercise__. _If $\{A_i\}$ are pairwise disjoint show $A_i\cap A_j\cap
A_k = \emptyset$ if $i$, $j$, and $k$ are not all the same._

__Exercise__. _If $\{A_i\}$ are pairwise disjoint show $\cap_{j\in J}A_j =
\emptyset$ if $J\subseteq I$ has at least two elements._

Partitions represent partial information. 
Complete information means knowing what outcome $\omega\in\Omega$ occured.
This corresponds to the _finest_ partition consisting of singletons
$\{\{\omega\}:\omega\in\Omega\}$.  Complete lack of information
corresponds to the _coarsest_ partion consisting of one set $\{\Omega\}$.
Partial information correponds to knowing what atom of a partition $\omega$ belongs to.

Partitions have a _partial ordering_ where $\mathcal{A}\preceq\mathcal{B}$ indicates
every atom of $\mathcal{A}$ is a union of atoms in $\mathcal{B}$. In this case we say
$\mathcal{B}$ is a _refinement_ of $\mathcal{A}$ and $\mathcal{B}$ is _finer_ than $\mathcal{A}$
or $\mathcal{A}$ is _coarser_ than $\mathcal{B}$.

Recall a partial ordering is _reflexive_: $\mathcal{A}\preceq\mathcal{A}$ and
_transitive_:$\mathcal{A}\preceq\mathcal{B}$ and $\mathcal{B}\preceq\mathcal{C}$
imply $\mathcal{A}\preceq\mathcal{C}$ for $A,B,C\subseteq\Omega$.

__Exercise__. _Show refinement is a partial ordering._

### Algebra of Sets

More advanced texts use an _algebra_ of sets instead of a partition.
An algebra of sets is a collection of subsets closed under complement and union
that also contains the empty set.

Since algebras are closed under complement $\Omega = \emptyset' =
\Omega\setminus\emptyset$
is also in the algebra.

By [De Morgan's Laws](https://en.wikipedia.org/wiki/De_Morgan's_laws)
an algebra is also closed under intersection since
$A\cap B = (A'\cup B')'$.

If $E$ and $F$ are elements of an algebra we can use plain English to
talk about 'not $E$' ($E'= \Omega\setminus E$), '$E$ or $F$' ($E\cup F$), and
'$E$ and $F$' ($E\cap F$). The phrases '$E$ implies $F$' and 'if $E$ then $F$'
correspond to $E\subseteq F$.

An _atom_ of an algebra $\mathcal{A}$ is an element $A\in\mathcal{A}$ with
the property $B\subseteq A$ and $B\in\mathcal{A}$ imply $B = \emptyset$
or $B = A$.

__Exercise__. _If an algebra is finite its atoms form a partition._

_Hint_: Show $A_\omega = \cap\{B\in\mathcal{A}:\omega\in B\}$ is an atom for all $\omega\in\Omega$. 

If $\mathcal{A}$ is infinite then there is no guarantee the intersection above is still
in $\mathcal{A}$. A _countably addititve measure_ guarantees the algebra is closed under
countable unions (and hence countable intersections) and $P(\cup_i E_i) = \sum_i P(E_i)$ if
$(E_i)_{i\in\mathbf{N}}$ are pairwise disjoint. These conditions are required to prove
_limit theorems_ about measures.

For example, The Borel-Cantelli lemma states that if $\sum_i P(E_i) <
\infty$ for any countable collection of events $(E_i)_{i\in\mathbf{N}}$
then none of the events can occur _infinitely often_. The outcome $\omega$ occurs
after $k$ if $\omega\in\cup_{k > n} E_k$. If an outcome occurs a finite number
of times then $\omega\not\in\cup_{k > n} E_k$ for sufficiently large $k$.
If the outcome occurs in an infinite number of events then
$\omega\in\cap_n \cup_{k > n} E_k$ and we say $\omega$ occurs _infinitely often_.
For any $\epsilon > 0$ there exists $n$ such that $\sum_{k > n} P(E_k) < \epsilon$
since the infinite sum converges to a finite value. 

-->
</section>
</section>
</section>
<footer>
Return to <a href="index.html">index</a>.
</footer>
</body>
</html>
