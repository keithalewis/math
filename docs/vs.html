<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Keith A. Lewis" />
  <meta name="dcterms.date" content="2025-02-25" />
  <title>Vector Space</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="math.css" />
  
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Vector Space</h1>
<p class="author">Keith A. Lewis</p>
<p class="date">February 25, 2025</p>
<div class="abstract">
<div class="abstract-title">Abstract</div>
A mathematical sweet spot
</div>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#set" id="toc-set">Set</a></li>
<li><a href="#boldsymbolrn" id="toc-boldsymbolrn"><span
class="math inline">\boldsymbol{{{R}}}^n</span></a>
<ul>
<li><a href="#pair" id="toc-pair">Pair</a></li>
<li><a href="#structure" id="toc-structure">Structure</a></li>
</ul></li>
<li><a href="#tensor" id="toc-tensor">Tensor</a>
<ul>
<li><a href="#rank-2" id="toc-rank-2">Rank 2</a></li>
<li><a href="#arbitrary-rank" id="toc-arbitrary-rank">Arbitrary
Rank</a></li>
<li><a href="#axioms" id="toc-axioms">Axioms</a></li>
<li><a href="#standard-and-dual-basis"
id="toc-standard-and-dual-basis">Standard and Dual Basis</a></li>
</ul></li>
<li><a href="#linear-operator" id="toc-linear-operator">Linear
Operator</a>
<ul>
<li><a href="#tensor-1" id="toc-tensor-1">Tensor</a></li>
<li><a href="#inner-product" id="toc-inner-product">Inner
Product</a></li>
</ul></li>
</ul>
</nav>
<p>Blah, blah, blah…</p>
<section id="set" class="level2">
<h2>Set</h2>
<p>Everything in math is a set. The set of function from a set <span
class="math inline">A</span> to a set <span class="math inline">B</span>
is the <em>set exponential</em> <span class="math inline">B^A =
\{f\colon A\to B\}</span>. A function is a set defined by its
<em>graph</em> <span class="math inline">\{(a,f(a))\mid a\in
A\}\subseteq A\times B</span>. An <em>ordered pair</em> <span
class="math inline">(a,b)\in A\times B</span> can be defined as the set
<span class="math inline">\{a,\{a,b\}\}</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span
class="math inline">\cap\{a,\{a,b\}\} = \{a\}</span> and <span
class="math inline">\cup\{a,\{a,b\}\} = \{a,b\}</span></em>.</p>
<p><em>Hint</em> For any set <span class="math inline">A</span>, <span
class="math inline">\cap A = \cap\{a\mid a\in A\}</span> and <span
class="math inline">\cup A = \cup\{a\mid a\in A\}</span>.</p>
<p>The intersection identifies the first item of the pair. If the union
is a singleton then the second item is equal to the first item,
otherwise we remove the first item to identify the second item.</p>
<p>Ordered pairs have <em>projections</em> <span
class="math inline">\operatorname{first}\colon A\times B\to A</span>
where <span class="math inline">\operatorname{first}(a,b) = a</span> and
<span class="math inline">\operatorname{second}\colon A\times B\to
B</span> where <span class="math inline">\operatorname{second}(a,b) =
b</span>.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">f\colon
C\to A</span> and <span class="math inline">g\colon C\to B</span> show
there exists <span class="math inline">h\colon C\to A\times B</span>
with <span class="math inline">\operatorname{first}(h(c)) = f(c)</span>
and <span class="math inline">\operatorname{second}(h(c)) = g(c)</span>,
<span class="math inline">c\in C</span></em>.</p>
<p><em>Hint</em>: Of course <span class="math inline">h(c) = (f(c),
g(c))</span>.</p>
<p>Two sets <span class="math inline">A</span> and <span
class="math inline">B</span> are <em>equivalent</em> (as sets), <span
class="math inline">A\cong B</span>, if there exists a one-to-one
correspondence between them.</p>
<p><strong>Exercise</strong>. <em>Show <span
class="math inline">\cong</span> is an</em> equivalence
<em>relation</em>.</p>
<p><em>Hint</em>. Establish <span class="math inline">A\cong A</span>,
<span class="math inline">A\cong B</span> implies <span
class="math inline">B\cong A</span>, and <span
class="math inline">A\cong B</span>, <span class="math inline">B\cong
C</span> imply <span class="math inline">A\cong C</span> for sets <span
class="math inline">A,B,C</span>.</p>
<details>
<summary>
Solution
</summary>
Show the identity function from <span class="math inline">A</span> to
<span class="math inline">A</span> is bijective. Show if <span
class="math inline">f\colon A\to B</span> is bijective, so is <span
class="math inline">f^{-1}\colon B\to A</span>. Show if <span
class="math inline">f\colon A\to B</span> and <span
class="math inline">g\colon B\to C</span> are bijective then so is <span
class="math inline">gf\colon A\to C</span>.
</details>
<p>An equivalence relation on a set <span class="math inline">S</span>
is a subset <span class="math inline">R\subseteq S\times S</span> with
<span class="math inline">aRa</span> (reflexive), <span
class="math inline">aRb</span> implies <span
class="math inline">bRa</span> (symmetric), and <span
class="math inline">aRb</span>, <span class="math inline">bRc</span>
imply <span class="math inline">aRc</span> (transitive), <span
class="math inline">a,b,c\in S</span>, where we write <span
class="math inline">aRb</span> for <span class="math inline">(a,b)\in
R</span>. The <em>equivalence class</em> of <span
class="math inline">a\in S</span> is <span class="math inline">[a] =
\{b\in S\mid aRb\}</span>.</p>
<p><strong>Exercise</strong>. <em>Show either <span
class="math inline">[a] = [b]</span> or <span
class="math inline">[a]\cap [b] = \emptyset</span> for <span
class="math inline">a,b\in S</span></em>.</p>
<p>A <em>partition</em> of a set is a collection of disjoint subsets
whose union is the entire set. This exercise shows <span
class="math inline">\{[a]\mid a\in A\}</span> is a partition of <span
class="math inline">S</span>.</p>
<p><strong>Exercise</strong>. <em>If <span
class="math inline">\Sigma</span> is a partions of <span
class="math inline">S</span> then <span
class="math inline">\cup_{s\in\Sigma} s\times s</span> is an equivalence
relation on <span class="math inline">S</span></em>.</p>
<p>Equivalence allows us to consider when two things are “the same” in a
particualar sense even if they are not equal.</p>
<p>Functions between sets with structure that preserve the structure are
<em>homomorphisms</em>. A homomorphism that is bijective (one-to-one and
onto) is an <em>isomorphism</em>. Two sets with structure are
<em>equivalent</em> if there is an isomorphism between them.</p>
</section>
<section id="boldsymbolrn" class="level2">
<h2><span class="math inline">\boldsymbol{{{R}}}^n</span></h2>
<p>There are two ways to think about <span
class="math inline">\boldsymbol{{{R}}}^n</span>, one is as a set of
tuples, the other is as a set of functions. These two perspectives are a
source of confusion and insight.</p>
<p>An impoverished notion of a vector is that it is a tuple of real
numbers <span class="math inline">{x = (x_1,\ldots,x_n)}</span>. Given a
natural number <span class="math inline">n\in\boldsymbol{{N}}</span>,
let <span class="math inline">{{\boldsymbol{{{R}}}^n =
\{(x_1,\ldots,x_n)\mid x_i\in\boldsymbol{{{R}}}, 1\le i\le n\} =
\prod_{1\le i\le n}\boldsymbol{{{R}}}}}</span> be the cartesian product
of <span class="math inline">n\in\boldsymbol{{N}}</span> copies of the
real numbers. If bold <span class="math inline">{\boldsymbol{{n}} =
\{1,\ldots,n\}}</span> then <span class="math inline">{i\in
\boldsymbol{{n}}}</span> is a shorter notation for <span
class="math inline">{1\le i\le n}</span>. Recall if <span
class="math inline">A</span> and <span class="math inline">B</span> are
sets then the <em>set exponential</em> <span class="math inline">{B^A =
\{f\colon A\to B\}}</span> is the set of all functions from <span
class="math inline">A</span> to <span class="math inline">B</span>. We
can identify <span class="math inline">\boldsymbol{{{R}}}^n</span> with
<span class="math inline">\boldsymbol{{{R}}}^{\boldsymbol{{n}}}</span>
where the tuple <span
class="math inline">{(x_i)_{i\in\boldsymbol{{n}}}}</span> corresponds to
the function <span
class="math inline">\boldsymbol{{x}}\colon\boldsymbol{{n}}\to\boldsymbol{{{R}}}</span>
defined by <span class="math inline">\boldsymbol{{x}}(i) = x_i</span>,
<span class="math inline">i\in\boldsymbol{{n}}</span>.</p>
<p>!!! <span class="math inline">n\in\boldsymbol{{N}}</span> to <span
class="math inline">\boldsymbol{{n}}\subseteq\boldsymbol{{N}}</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span
class="math inline">\prod_{i\in\boldsymbol{{n}}}\boldsymbol{{{R}}}</span>
is in one-to-one correspondence with <span
class="math inline">\boldsymbol{{{R}}}^{\boldsymbol{{n}}}</span></em>.</p>
<p>A more powerful notion is to consider a vector as an element of the
<em>vector space</em> of all functions from an <em>index set</em> <span
class="math inline">I</span> to the real numbers, <span
class="math inline">\boldsymbol{{{R}}}^I</span>. The tuple <span
class="math inline">x = (x_i)_{i\in I}</span> in <span
class="math inline">\prod_{i\in I}\boldsymbol{{{R}}}</span> corresponds
to a function <span class="math inline">{\boldsymbol{{x}}\colon
I\to\boldsymbol{{{R}}}}</span> in <span
class="math inline">\boldsymbol{{{R}}}^I</span> defined by <span
class="math inline">\boldsymbol{{x}}(i) = x_i</span>, <span
class="math inline">i\in I</span>. The tuple <span class="math inline">x
= (x_i)_{i\in I}</span> where <span class="math inline">x_i =
\boldsymbol{{x}}(i)</span>, <span class="math inline">i\in I</span>.
corresponds to the function <span
class="math inline">{\boldsymbol{{x}}\colon
I\to\boldsymbol{{{R}}}}</span>. In what follows we just write <span
class="math inline">x</span> for <span
class="math inline">\boldsymbol{{x}}</span> and leave it to you to
figure out from context if a vector is a tuple or a function.</p>
<p><strong>Exercise</strong>. <em>Show <span
class="math inline">\prod_{i\in I}\boldsymbol{{{R}}}</span> is in
one-to-one correspondence with <span
class="math inline">\boldsymbol{{{R}}}^{I}</span></em>.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">I</span>
and <span class="math inline">J</span> are in one-to-one correspondence
then so are <span class="math inline">\boldsymbol{{{R}}}^I</span> and
<span class="math inline">\boldsymbol{{{R}}}^J</span></em>.</p>
<section id="pair" class="level3">
<h3>Pair</h3>
<p>Defining the <em>ordered pair</em> <span
class="math inline">(a,b)</span> using only set theory is a little
tricky. Norbert Wiener gave a first crack at it in 1914 by defining
<span class="math inline">(a,b)</span> to be the set <span
class="math inline">\{\{\{a,\emptyset\}, \{\{b\}\}\}</span>. Around the
same time Felix Hausdorff proposed the more recognizable definition
<span class="math inline">\{\{a,1\},\{b,2\}</span> “where <span
class="math inline">1</span> and <span class="math inline">2</span> are
two distinct objects different from <span class="math inline">a</span>
and <span class="math inline">b</span>.” In 1921, Kazimierz Kuratowski
eliminated Housdorff’s circumloqution and proposed simplifying Wiener’s
definition to <span class="math inline">\{a,\{a,b\}\}</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span
class="math inline">\cap\{a,\{a,b\}\} = \{a\}</span> and <span
class="math inline">\cup\{a,\{a,b\}\} = \{a,b\}</span></em>.</p>
<p><em>Hint</em> For any set <span class="math inline">A</span>, <span
class="math inline">\cap A = \cap\{a\mid a\in A\}</span> and <span
class="math inline">\cup A = \cup\{a\mid a\in A\}</span>.</p>
<p>The intersection identifies the first item of the pair. If the union
is a singleton then the second item is equal to the first item,
otherwise we remove the first item to identify the second item.</p>
<p>Ordered pairs have <em>projections</em> <span
class="math inline">\operatorname{first}\colon A\times B\to A</span>
where <span class="math inline">\operatorname{first}(a,b) = a</span> and
<span class="math inline">\operatorname{second}\colon A\times B\to
B</span> where <span class="math inline">\operatorname{second}(a,b) =
b</span>.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">f\colon
C\to A</span> and <span class="math inline">g\colon C\to B</span> show
there exists <span class="math inline">h\colon C\to A\times B</span>
with <span class="math inline">\operatorname{first}(h(c)) = f(c)</span>
and <span class="math inline">\operatorname{second}(h(c)) = g(c)</span>,
<span class="math inline">c\in C</span></em>.</p>
<aside>
As of 2024, the <code>Java</code> language standard does not provide a
pair object. One must <code>import javafx.util.Pair;</code> to make that
available.
</aside>
<p>A function <span class="math inline">f\colon A\to B</span> is
<em>one-to-one</em>, or <em>injective</em> if <span
class="math inline">f(a) = f(a&#39;)</span> implies <span
class="math inline">a = a&#39;</span>, <span
class="math inline">a,a&#39;\in A</span>. A function is <em>onto</em>,
or <em>surjective</em>, if for every <span class="math inline">b\in
B</span> there exists an <span class="math inline">a\in A</span> with
<span class="math inline">f(a) = b</span>. A function is a
<em>one-to-one correspondence</em>, or <em>bijective</em>, if it is
one-to-one and onto.</p>
<p>Two sets <span class="math inline">A</span> and <span
class="math inline">B</span> are <em>equivalent</em> (as sets), <span
class="math inline">A\cong B</span>, if there exists a one-to-one
correspondence between them.</p>
<p><strong>Exercise</strong>. <em>Show <span
class="math inline">\cong</span> is an</em> equivalence
<em>relation</em>.</p>
<p><em>Hint</em>. Establish <span class="math inline">A\cong A</span>,
<span class="math inline">A\cong B</span> implies <span
class="math inline">B\cong A</span>, and <span
class="math inline">A\cong B</span>, <span class="math inline">B\cong
C</span> imply <span class="math inline">A\cong C</span> for sets <span
class="math inline">A,B,C</span>.</p>
<details>
<summary>
Solution
</summary>
Show the identity function from <span class="math inline">A</span> to
<span class="math inline">A</span> is bijective. Show if <span
class="math inline">f\colon A\to B</span> is bijective, so is <span
class="math inline">f^{-1}\colon B\to A</span>. Show if <span
class="math inline">f\colon A\to B</span> and <span
class="math inline">g\colon B\to C</span> are bijective then so is <span
class="math inline">gf\colon A\to C</span>.
</details>
<p>An equivalence relation on a set <span class="math inline">S</span>
is a subset <span class="math inline">R\subseteq S\times S</span> with
<span class="math inline">aRa</span> (reflexive), <span
class="math inline">aRb</span> implies <span
class="math inline">bRa</span> (symmetric), and <span
class="math inline">aRb</span>, <span class="math inline">bRc</span>
imply <span class="math inline">aRc</span> (transitive), <span
class="math inline">a,b,c\in S</span>, where we write <span
class="math inline">aRb</span> for <span class="math inline">(a,b)\in
R</span>. The <em>equivalence class</em> of <span
class="math inline">a\in S</span> is <span class="math inline">[a] =
\{b\in S\mid aRb\}</span>.</p>
<p><strong>Exercise</strong>. <em>Show either <span
class="math inline">[a] = [b]</span> or <span
class="math inline">[a]\cap [b] = \emptyset</span> for <span
class="math inline">a,b\in S</span></em>.</p>
<p>A <em>partition</em> of a set is a collection of disjoint subsets
whose union is the entire set. This exercise shows <span
class="math inline">\{[a]\mid a\in A\}</span> is a partition of <span
class="math inline">S</span>.</p>
<p><strong>Exercise</strong>. <em>If <span
class="math inline">\Sigma</span> is a partions of <span
class="math inline">S</span> then <span
class="math inline">\cup_{s\in\Sigma} s\times s</span> is an equivalence
relation on <span class="math inline">S</span></em>.</p>
<p>Equivalence allows us to consider when two things are “the same” in a
particualar sense even if they are not equal.</p>
<p>Currying is quite common in mathematics and is often done implicity.
Although currying is common, there is no generally accepted notation for
it, so let’s define one for our purposes. If <span
class="math inline">f\colon A\to(B\to C)</span> define <span
class="math inline">f,\colon A\times B\to C</span> by <span
class="math inline">f,(a, b) = (f(a))(b) = fa(b)</span> for <span
class="math inline">a\in A</span>, <span class="math inline">b\in
B</span>. If <span class="math inline">g\colon A\times B\to C</span>
define <span class="math inline">,g\colon A\to(B\to C)</span> by <span
class="math inline">,ga(b) = g(a,b)</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">,(f,) =
f</span> and <span class="math inline">(,g), = g</span></em>.</p>
<p>Currying shows <span class="math inline">(B\times C)^A</span> is
equavalent to <span class="math inline">(B^C)^A</span>.</p>
<p>If <span class="math inline">h\in B^A</span> and <span
class="math inline">a\in A</span> then evaluating <span
class="math inline">h</span> at <span class="math inline">a</span>
results in <span class="math inline">h(a)\in B</span>. We reify this by
defining the <span class="math inline">\operatorname{eval}</span>
function <span class="math inline">{\operatorname{eval}\colon B^A\times
A\to B}</span> by <span class="math inline">\operatorname{eval}(h,a) =
h(a)</span>. We decorate this as <span
class="math inline">{\operatorname{eval}^{A,B}}</span> when we want to
make the sets involved explicit.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">f\in
(C^B)^A</span> show <span class="math inline">f, =
\operatorname{eval}(\operatorname{eval}(f, a), b)</span></em>.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">g\in
(B\times C)^A</span> show <span class="math inline">,g =
\operatorname{eval}(\operatorname{eval}(g, a), b)</span></em>.</p>
</section>
<section id="structure" class="level3">
<h3>Structure</h3>
<p>Mathematical objects are sets with structure. For example, a
<em>semigroup</em> is a set <span class="math inline">S</span> with a
binary operation <span class="math inline">m\colon S\times S\to S</span>
that is <em>associative</em>: <span class="math inline">m(a,m(b,c)) =
m(m(a,b),c)</span>, <span class="math inline">a,b,c\in S</span>. If we
write <span class="math inline">ab</span> for <span
class="math inline">m(a,b)</span> this becomes <span
class="math inline">a(bc) = (ab)c</span>. This allows us to write <span
class="math inline">abc</span> unambigously for either term.</p>
<p>You might think this is too simple to be very useful, but you would
be wrong. Semigroups are the basis of the Map-Reduce algorithm. A
computation <span class="math inline">a_1a_2\cdots a_n</span> can be
partitioned into <span class="math inline">(a_1\cdots
a_{n_1})(a_{n_1+1}\cdots a_{n_2})\cdots(a_{n_k+1}\cdots a_n)</span>
where <span class="math inline">1 &lt; n_1 &lt; \cdots &lt; n_k &lt;
n</span>. The partitioned calculations can be performed in parallel and
the <span class="math inline">k + 1</span> calculations can be combined
to get the final result.</p>
<p>Functions between sets with structure that preserve the structure are
<em>homomorphisms</em>. A homomorphism that is bijective (one-to-one and
onto) is an <em>isomorphism</em>. Two sets with structure are
<em>equivalent</em> if there is an isomorphism between them.</p>
<p>In general, it is difficult to determine when two sets with structure
are equivalent, but vector spaces are a mathematical sweet spot. Two
vector spaces are equivalent if and only if they have the same
dimension.</p>
<!--
A _linear operator_ is homomorphism from one vector space to another.
Given two vector spaces, the set of all linear operators is also a vector space.
If the two vector spaces are equal then
composition defines a product making the set of _endomorphisms_ into an _algebra_.
Two endomorphisms are _similar_ in the algebra if and
only if they have the same _Jordan canonical form_.

If a vector space is provided with an _inner product_ we can define the distance
between two vectors.
-->
</section>
</section>
<section id="tensor" class="level2">
<h2>Tensor</h2>
<p>A <em>tensor</em><a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a> of <em>rank</em> <span
class="math inline">m</span> is a vector space where the index set is a
cartesian product of <span class="math inline">m</span> index sets <span
class="math inline">{I = I_1\times\cdots\times I_m =
\prod_{j\in\boldsymbol{{m}}} I_j}</span>. A rank 1 tensor is a vector.
The <em>shape</em> of a tensor is its index set. First we consider rank
2 tensors with natual number index sets. More precisely, if <span
class="math inline">n\in\boldsymbol{{N}}</span> is a natural number we
use the index set <span class="math inline">\boldsymbol{{n}} =
\{1,\ldots,n\}</span>.</p>
<section id="rank-2" class="level3">
<h3>Rank 2</h3>
<p>A rank 2 tensor is a <em>matrix</em>. If <span
class="math inline">x\in\boldsymbol{{{R}}}^{n\times m}</span>, <span
class="math inline">n,m\in\boldsymbol{{N}}</span>, then we can define
<span class="math inline">x_{i,j} = x(i, j)</span>, <span
class="math inline">i\in\boldsymbol{{n}}</span>, <span
class="math inline">j\in\boldsymbol{{m}}</span>. Thinking of <span
class="math inline">x</span> as the two-dimensional matrix <span
class="math inline">[x_{i,j}]</span>, the <span
class="math inline">i</span>-th row of <span
class="math inline">x</span> is <span
class="math inline">{(x_{i,j})_{j\in\boldsymbol{{m}}}}</span> and the
<span class="math inline">j</span>-th column is <span
class="math inline">{(x_{i,j})_{i\in\boldsymbol{{n}}}}</span>. In the
Python <a
href="https://numpy.org/doc/stable/reference/arrays.ndarray.html">numpy</a>
package the <span class="math inline">i</span>-th row is expressed as
<span class="math inline">x[i,:]</span> and the <span
class="math inline">j</span>-th column as <span
class="math inline">x[:,j]</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span
class="math inline">\boldsymbol{{{R}}}^{\boldsymbol{{n}}\timex\boldsymbol{{m}}}</span>
is in one-to-one correspondence with <span
class="math inline">\boldsymbol{{{R}}}^{nm}</span>, <span
class="math inline">n,m\in\boldsymbol{{N}}</span></em>.</p>
<p>Note selecting a row is a function <span
class="math inline">\boldsymbol{{{R}}}^{n\times
m}\times\boldsymbol{{n}}\to\boldsymbol{{{R}}}^{m}</span> where <span
class="math inline">(x, i)</span> is associated with the function <span
class="math inline">j\mapsto x(i,j)</span> in <span
class="math inline">\boldsymbol{{{R}}}^m</span>.</p>
<p>This is an example of <em>currying</em>.</p>
<p>Selecting a row is currying. If <span
class="math inline">g\colon\boldsymbol{{{R}}}^{n\times
m}\times\boldsymbol{{n}}\to\boldsymbol{{{R}}}^m</span> then <span
class="math inline">,g\colon\boldsymbol{{{R}}}^{n\times
m}\to(\boldsymbol{{n}}\to\boldsymbol{{{R}}}^m)</span>. We write this
using square brackets without mentioning <span
class="math inline">g</span> or its curried value <span
class="math inline">g,</span> as <span
class="math inline">x[i,.]\colon\boldsymbol{{m}}\to\boldsymbol{{{R}}}</span>
defined by <span class="math inline">x[i,.](j) = x(i,j)</span>.
Similarly, selecting a column <span
class="math inline">x[.,j]\colon\boldsymbol{{m}}\to\boldsymbol{{{R}}}</span>
is defined by <span class="math inline">x[.,j](i) = x(i,j)</span>.</p>
<p>Define the <em>transpose</em> operator <span
class="math inline">{}^T\colon A\times B\to B\times A</span> by <span
class="math inline">{}^T(a,b) = (b,a)</span>. We decorate this as <span
class="math inline">{{}^{T^{A,B}}}</span> when we want to make the sets
involved explicit.</p>
<p><strong>Exercise</strong>. <em>For <span
class="math inline">x\in\boldsymbol{{{R}}}^{n\times m}</span> show <span
class="math inline">{x^{T^{\boldsymbol{{m}},\boldsymbol{{n}}}}(j,i) =
x(i,j)}</span> for <span
class="math inline">i\in\boldsymbol{{n}}</span>, <span
class="math inline">j\in\boldsymbol{{m}}</span></em>.</p>
<p><em>Hint</em>. <span
class="math inline">x^{T^{\boldsymbol{{m}},\boldsymbol{{n}}}}</span> is
the composition of <span
class="math inline">{}^{T^{\boldsymbol{{m}},\boldsymbol{{n}}}}</span>
with <span class="math inline">x</span>.</p>
<p>When the indices are understood we write <span
class="math inline">x^T(j,i) = x(i,j)</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span
class="math inline">x^T[.,k] = x[k,.]</span> for <span
class="math inline">x\in\boldsymbol{{{R}}}^{n\times m}</span>, <span
class="math inline">k\in\boldsymbol{{n}}</span></em>.</p>
</section>
<section id="arbitrary-rank" class="level3">
<h3>Arbitrary Rank</h3>
<p>Let’s extend the notation to handle tensors of any rank, not just
rank 2. Let <span class="math inline">\mathcal{T}_m</span> be the
collection of rank <span class="math inline">m</span> tensors. Define
<span
class="math inline">[]\colon\mathcal{T}_m\times\bf{m}\to\mathcal{T}_{m-1}</span></p>
<p>More rigourously, define $[]^{_{i}}<br />
</p>
<p>If <span class="math inline">\iota\colon J\to I</span> and <span
class="math inline">x\in\boldsymbol{{{R}}}^I</span> we have <span
class="math inline">x\iota\in\boldsymbol{{{R}}}^J</span> for any index
sets <span class="math inline">I</span>, <span
class="math inline">J</span>. The name for composition on the right by
<span class="math inline">\iota</span> is <span
class="math inline">\circ\iota\colon\boldsymbol{{{R}}}^I\to\boldsymbol{{{R}}}^J</span>
so <span class="math inline">\circ\iota(x) = x\iota</span>, <span
class="math inline">x\in\boldsymbol{{{R}}}^I</span>.</p>
<p>Even with this minimal material we can consider interesting
operations to perform on <span
class="math inline">\boldsymbol{{{R}}}^I</span>. For any function <span
class="math inline">\iota\colon J\to I</span> we have a mapping <span
class="math inline">\boldsymbol{{{R}}}^I\to\boldsymbol{{{R}}}^J</span>
<span class="math inline">x\mapsto x\iota</span>. We call <span
class="math inline">\iota</span> a <em>view</em> of <span
class="math inline">I</span>. Element of <span
class="math inline">x\in\boldsymbol{{{R}}}^I</span> are extracted from a
view by function application <span class="math inline">x(J(j))</span>,
<span class="math inline">j\in J</span>.</p>
<p><strong>Exercise</strong> <em>If <span class="math inline">J</span>
is the</em> singleton <span class="math inline">\{i\}</span> <em>for
some <span class="math inline">i\in I</span> then the inclusion <span
class="math inline">\iota\colon J\to I</span> selects the <span
class="math inline">i</span>-the element of <span
class="math inline">x</span></em>.</p>
<p><em>Hint</em>. Show <span class="math inline">x(\iota(i)) =
x(i)</span>.</p>
<p>More generally, if <span class="math inline">J\subseteq I</span> then
the inclusion selects <span class="math inline">(x_j)_{j\in
J}</span>.</p>
<!--
If $J$ is a _multiset_ then elements can be duplicated.
For example if $J = \{i,i,i\}$ then $x\iota = (x_i,x_i,x_i)$.
-->
<p>Many computer languages use 0-based array indexing. In this case we
define <span class="math inline">\boldsymbol{{n}} =
\{0,\ldots,n-1\}</span>. We often want to select a <em>slice</em> of
<span class="math inline">k</span> elements starting at index <span
class="math inline">i</span> in increments of <span
class="math inline">j</span>, <span class="math inline">{(x_i, x_{i +
j},\ldots, x_{i + j(k-1)})}</span></p>
<p><strong>Exercise</strong> <em>What is the largest value of <span
class="math inline">k</span> for which <span class="math inline">0 \le i
+ j(k-1) &lt; n</span></em>?</p>
<p><em>Hint</em>: The increment <span class="math inline">j</span> may
be negative.</p>
<p>Some low level languages do not have any guards aginst indexing an
array out of bounds. The onus is on the programmer to use their tools
correctly if performance is an issue. ### Scalar Multiplication and
Vector Addition</p>
<p>Scalar multiplication and vector addition on <span
class="math inline">\boldsymbol{{{R}}}^I</span> are defined
<em>pointwise</em> by <span class="math inline">{(ax)(i) =
a(x(i))}</span> and <span class="math inline">{(x + y)(i) = x(i) +
y(i)}</span>, <span class="math inline">a\in\boldsymbol{{{R}}}</span>,
<span class="math inline">x,y\in\boldsymbol{{{R}}}^I</span>.</p>
</section>
<section id="axioms" class="level3">
<h3>Axioms</h3>
<p>For any set <span class="math inline">I</span>, <span
class="math inline">a,b\in\boldsymbol{{{R}}}</span>, and <span
class="math inline">x, y, z\in\boldsymbol{{{R}}}^I</span> show the
following:</p>
<p><strong>Exercise</strong>. <em><span class="math inline">x + (y + z)
= (x + y) + z</span></em>.</p>
<p><strong>Exercise</strong>. <em><span class="math inline">x + y = y +
x</span></em>.</p>
<p><strong>Exercise</strong>. <em><span
class="math inline">\boldsymbol{{0}}+ x = x</span> where <span
class="math inline">\boldsymbol{{0}}(i) = 0</span>, <span
class="math inline">i\in I</span></em>.</p>
<p><strong>Exercise</strong>. <em><span class="math inline">x + (-x) =
\boldsymbol{{0}}</span> where <span class="math inline">(-x)(i) =
-(x(i))</span>, for <span class="math inline">i\in I</span></em>.</p>
<p><strong>Exercise</strong>. <em><span class="math inline">a(bx) =
(ab)x</span></em>.</p>
<p><strong>Exercise</strong>. <em><span class="math inline">1x =
x</span></em>.</p>
<p><strong>Exercise</strong>. <em><span class="math inline">a(x + y) =
ax + ay</span></em>.</p>
<p><strong>Exercise</strong>. <em><span class="math inline">(a + b)x =
ax + bx</span></em>.</p>
<p><em>Hint</em>: Use the properties of real numbers.</p>
<p>The exercises are the axioms for an <em>abstract vector space</em>
with scalar multiplication <span
class="math inline">{\boldsymbol{{{R}}}\times V\to V}</span> where <span
class="math inline">{(a,x)\mapsto ax = xa}</span> and binary addition
<span class="math inline">{V\times V\to V}</span> where <span
class="math inline">{(x,y)\mapsto x + y}</span>. The first four axioms
show vector addition is an abelian (commutative) group. The last two
axioms are the <em>distributive laws</em> connecting scalar
multiplication and vector addition. Every abstract vector space can be
represented by <span class="math inline">\boldsymbol{{{R}}}^I</span> for
some set <span class="math inline">I</span>. Two vector spaces</p>
<p>Proofs involving only the abstract axioms are considered more
elegant.</p>
<p>If <span class="math inline">x\in\boldsymbol{{{R}}}^I</span> then
<span class="math inline">(0x)(i) = 0(x(i)) = 0</span> and <span
class="math inline">(1x)(i) = 1(x(i)) = x(i)</span> for all <span
class="math inline">i\in I</span> so <span class="math inline">0x =
\boldsymbol{{0}}</span> and <span class="math inline">1x = x</span>.
These hold for any vector space, not just <span
class="math inline">\boldsymbol{{{R}}}^I</span>.</p>
<p><strong>Exercise</strong>. (Zero is unique) <em>Show if <span
class="math inline">\boldsymbol{{0}}&#39; + v = v</span> for all <span
class="math inline">v\in V</span> then <span class="math inline">0&#39;
= 0</span></em>.</p>
<p><em>Hint</em>: <span class="math inline">\boldsymbol{{0}}+ v =
v</span> so <span class="math inline">\boldsymbol{{0}}&#39; + v =
\boldsymbol{{0}}+ v</span>. Add <span class="math inline">-v</span> to
both sides.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">0v =
\boldsymbol{{0}}</span> for all <span class="math inline">v\in
V</span></em>.</p>
<p><em>Hint</em>: Show <span class="math inline">0v + v = v</span> and
use the previous exercise.</p>
<p><strong>Exercise</strong>. <em>For any vector space <span
class="math inline">V</span> show <span class="math inline">{v + v =
v}</span> implies <span class="math inline">{v =
\boldsymbol{{0}}}</span> for all <span class="math inline">{v\in
V}</span></em>.</p>
<details>
<summary>
Solution
</summary>
<span class="math display">
\begin{aligned}
v + v &amp;= v \\
    &amp;\quad\langle x = y\Rightarrow x + z = y + z\mid x\leftarrow v +
v,y\leftarrow v,z\leftarrow -v\rangle\\
(v + v) + (-v) &amp;= v + (-v) \\
    &amp;\quad\langle (x + y) + z = x + (y + z)\mid x\leftarrow v,
y\leftarrow v, z\leftarrow -v\rangle\\
v + (v + (-v)) &amp;= v + (-v) \\
    &amp;\quad\langle x + (-x) = \boldsymbol{{0}}\mid x\leftarrow
v\text{ twice }\rangle\\
v + \boldsymbol{{0}}&amp;= \boldsymbol{{0}}\\
    &amp;\quad\langle x + \boldsymbol{{0}}= x\mid x\leftarrow v\rangle\\
v &amp;= \boldsymbol{{0}}
\end{aligned}
</span>
</details>
<!--
If $n = 1$ we can identify $\RR$ with $\RR^1$ by $x\in\RR$ corresponds to $(x)\in\RR^1$.
We can also identify every $y\in\RR$ with the linear function from $\RR$ to $\RR$
of multiplication by $y$ denoted by $y^*x = xy$, for $x\in\RR$.
These trivial definitions are the foundation of a systematic notation for
higher dimensional calculations. There is no need for "row" and "column"
vectors, the dual of a vector space and composition of linear functions
generalizes the Einstein summation convention and various "tensor"
notations invented by less brilliant people. Standard mathematical notation suffices.
-->
</section>
<section id="standard-and-dual-basis" class="level3">
<h3>Standard and Dual Basis</h3>
<p>The <em>standard basis</em> of <span
class="math inline">\boldsymbol{{{R}}}^n</span> is <span
class="math inline">{e_i\in\boldsymbol{{{R}}}^n}</span>, <span
class="math inline">{i\in\boldsymbol{{n}}}</span>, where <span
class="math inline">{e_i = (0,\ldots,1,\ldots,0)}</span> with all
elements <span class="math inline">0</span> except for a <span
class="math inline">1</span> in the <span
class="math inline">i</span>-th position. It is plausible that <span
class="math inline">{x = (x_1,\ldots,x_n) = x_1 e_1 + \cdots + x_n
e_n}</span> for <span
class="math inline">{x\in\boldsymbol{{{R}}}^n}</span>, but you should
always be wary of definitions involving dots.</p>
<p>More generally, the standard basis of <span
class="math inline">\boldsymbol{{{R}}}^I</span> is <span
class="math inline">{e_i\in\boldsymbol{{{R}}}^I}</span>, <span
class="math inline">{i\in I}</span>, where <span
class="math inline">{e_i(j) = \delta_{ij}}</span> for <span
class="math inline">j\in I</span>, where $_{ij} is the <em>Kronecker
delta</em> defined by <span class="math inline">{δ_{ij} = 1}</span> if
<span class="math inline">{i=j}</span> and <span
class="math inline">{δ_{ij} = 0}</span> if <span
class="math inline">{i\not=j}</span>.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">I</span>
is finite, show <span class="math inline">x = \sum_{i\in I}
x(i)e_i</span> for <span
class="math inline">x\in\boldsymbol{{{R}}}^I</span></em>.</p>
<p><em>Hint</em>: Consider <span class="math inline">x(j)</span>, <span
class="math inline">j\in I</span>.</p>
<p>Define the <em>dual basis</em> by <span
class="math inline">e^*_j\colon\boldsymbol{{{R}}}^I\to\boldsymbol{{{R}}}</span>
by <span class="math inline">{e^*_j(x) = x(j)}</span>, <span
class="math inline">{j\in I}</span>. The standard and dual bases allows
us to identify <span class="math inline">\boldsymbol{{{R}}}^I</span>
with its <em>dual</em> <span
class="math inline">(\boldsymbol{{{R}}}^I)^*</span>, the linear
functionals from <span
class="math inline">\boldsymbol{{{R}}}^I\to\boldsymbol{{{R}}}</span>.
The <em>dual pairing</em> of <span
class="math inline">x\in\boldsymbol{{{R}}}^I</span> and <span
class="math inline">{y\in\boldsymbol{{{R}}}^I}</span> is defined by
<span class="math inline">y^*(x) = \langle x, y^*\rangle = \sum_{i\in I}
x(i)y(i)</span>.</p>
<p>If <span class="math inline">{x,y\in\boldsymbol{{{R}}}^I}</span> the
<em>inner product</em>, or <em>dot product</em> is <span
class="math inline">{(x,y) = x\cdot y = \sum_{i\in I}x_i y_i}</span>.
This is similar, but different from, the dual pairing is a function
<span
class="math inline">\langle\cdot,\cdot\rangle\colon\boldsymbol{{{R}}}^I\times(\boldsymbol{{{R}}}^I)^*\to\boldsymbol{{{R}}}</span>
while the dot product is a function <span
class="math inline">(\cdot,\cdot)\colon\boldsymbol{{{R}}}^I\times\boldsymbol{{{R}}}^I\to\boldsymbol{{{R}}}</span>.</p>
</section>
</section>
<section id="linear-operator" class="level2">
<h2>Linear Operator</h2>
<p>A function <span
class="math inline">T\colon\boldsymbol{{{R}}}^n\to\boldsymbol{{{R}}}^m</span>
is a <em>linear operator</em> if <span class="math inline">{T(ax + y) =
aTx + y}</span>, <span
class="math inline">{a\in\boldsymbol{{{R}}}}</span>, <span
class="math inline">{x,y\in\boldsymbol{{{R}}}^n}</span>.</p>
<p><strong>Exercise</strong>. <em>Show if <span
class="math inline">T</span> is a linear operator then <span
class="math inline">T(ax) = aTx</span> and <span class="math inline">T(x
+ y) = Tx + Ty</span>, <span
class="math inline">{a\in\boldsymbol{{{R}}}}</span>, <span
class="math inline">{x,y\in\boldsymbol{{{R}}}^n}</span></em>.</p>
<p><em>Hint</em>: Take <span class="math inline">y = (0,\ldots,0)</span>
and <span class="math inline">a = 1</span>.</p>
<p>If <span
class="math inline">T\colon\boldsymbol{{{R}}}^n\to\boldsymbol{{{R}}}^m</span>
the <em>matrix</em> of <span class="math inline">T</span> is <span
class="math inline">t\colon n\times
m\to\boldsymbol{{{R}}}\in\boldsymbol{{{R}}}^{n\times m}</span> where
<span class="math inline">{t(i,j) = t_{ij} = \langle
Te_i,e^*_j\rangle}</span>, <span class="math inline">{i\in n}</span>,
<span class="math inline">{j\in m}</span>. We denote the map $</p>
<p>If <span
class="math inline">S\colon\boldsymbol{{{R}}}^m\to\boldsymbol{{{R}}}^l</span>
then <span class="math inline">U =
ST\colon\boldsymbol{{{R}}}^n\to\boldsymbol{{{R}}}^l</span>.</p>
<p><strong>Exercise</strong>. <em>Show the matrix of the composition
<span class="math inline">U = ST</span> is <span
class="math inline">{u_{ik} = \sum_{j\in m} s_{ij}t_{jk}}</span>, <span
class="math inline">i\in n</span>, <span class="math inline">k\in
l</span></em>.</p>
<p>Matrix multiplication is just composition of linear operators.</p>
<section id="tensor-1" class="level3">
<h3>Tensor</h3>
<p>Write <span class="math inline">\boldsymbol{{{R}}}^{-n}</span> for
<span class="math inline">(\boldsymbol{{{R}}}^n)^*</span> for <span
class="math inline">n\in\boldsymbol{{N}}</span>. A <em>tensor</em> is a
cartesian product <span
class="math inline">\boldsymbol{{{R}}}^{n_1}\times\cdots\times\boldsymbol{{{R}}}^{n_m}</span>
where the <span class="math inline">n_i</span> are integers <span
class="math inline">n_i\in\boldsymbol{{Z}}</span> for <span
class="math inline">i\in m</span>.</p>
</section>
<section id="inner-product" class="level3">
<h3>Inner Product</h3>
<p>The <em>inner product</em>, or <em>dot product</em>, of <span
class="math inline">x,y\in\boldsymbol{{{R}}}^n</span> is <span
class="math inline">(x,y) = x\cdot y = \sum_{1\le i\le n}x_i y_i</span>.
This is used to define a <em>norm</em> on <span
class="math inline">\boldsymbol{{{R}}}^n</span> by <span
class="math inline">\|x\| = \sqrt{x\cdot x}</span>.</p>
<p>The <em>Cauchy-Schwartz inequality</em> is <span
class="math inline">{|(x, y)|\le\|x\|\|y\|}</span>, <span
class="math inline">{x,y\in\boldsymbol{{{R}}}^n}</span>. Since <span
class="math inline">{-1\le (x, y)/\|x\|\|y\|\le 1}</span> there exists
<span class="math inline">{\theta\in[0, 2\pi)}</span> with <span
class="math inline">{\cos\theta = (x, y)/\|x\|\|y\|}</span>. This
defines the <em>angle</em> between two vectors.</p>
<p><strong>Exercise</strong> <em>Show <span class="math inline">\|x\|
\ge 0</span>, <span class="math inline">\|ax\| = |a|\|x\|</span>, and
<span class="math inline">\|x + y\| \le \|x\| + \|y\|</span>, for <span
class="math inline">a\in\boldsymbol{{{R}}}</span>, <span
class="math inline">x,y\in\boldsymbol{{{R}}}^n</span></em>.</p>
<p><em>Hint</em>: Use <span class="math inline">\|x + y\|^2 = \|x\|^2 +
2x\cdot y + \|y\|^2</span> and the Cauchy-Schwartz inequality.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">\|x\| =
0</span> implies <span class="math inline">x = 0</span>, <span
class="math inline">x\in\boldsymbol{{{R}}}^n</span></em>.</p>
<p><strong>Exercise</strong>. <em>Show equality holds in the
Cauchy-Schwartz inequality if and only if <span
class="math inline">x</span> is a scalar multiple of <span
class="math inline">y</span></em>.</p>
<p><em>Hint</em>: TODO Sid. Use <span class="math inline">0\le\|x -
ty\|^2</span> for <span
class="math inline">t\in\boldsymbol{{{R}}}</span>, <span
class="math inline">x,y\in\boldsymbol{{{R}}}^n</span>.</p>
<p><strong>Exercise</strong>. <em>Show if <span
class="math inline">\lim_{n\to\infty} \|x_n - x\| = 0</span> show <span
class="math inline">x\in\boldsymbol{{{R}}}^n</span></em>.</p>
<p><em>Hint</em>: The real numbers with absolute value norm is
complete.</p>
<!--
A norm defines a _metric_ ${d(x,y) = \|x - y\|}$

__Exercise__. _Show ${d(x, y) \le d(x,z) + d(z,y)}$, ${x,y,z\in\RR^n}$ and
${d(x,y) = 0}$ implies ${x = y}$_.
-->
<!--
A homomorphism $T\colon V\to W$ from a vector space $V$ to a vector space $W$ is a _linear operator_.

__Exercise__. _If $T$ is a linear operator then $T\zero = \zero$_.

_Hint_: Show $T\zero + T\zero = T\zero$ and use the previous exercise. Note
the $\zero$s on the left hand side are the additive identity of $V$
and the $\zero$ on the right hand side is the additive identity of $W$.

<details>
<summary>Solution</summary>
Since ${Tv + Tw = T(v + w)}$ and ${\zero + v = v}$ we have
${T\zero + T\zero = T(\zero + \zero) = T\zero}$.
</details>

The collection of all linear operators from a vector space $V$ to a
vector space $W$ is denoted $\LL(V,W)$. It is a vector space with scalar
multiplication and vector addition defined by ${(aT)v = a(Tv)}$ and
${(T + U)v = Tv + Tv}$ for $T\in\LL(V,W)$, $a\in\RR$, $v\in V$.

__Exercise__. _Show linear operators form a vector space_.

An important special case is when the range is the one-dimensional vector space $\RR$.
Define the _dual_ of the vector space $V$ by $V^* = \LL(V,\RR)$.
We write the _dual pairing_ using angle brackets
${\langle v,v^*\rangle = v^*v}$ for ${v^*\in V^*}$, ${v\in V}$.

If $T\colon V\to W$ is a linear transformation define its _adjoint_
${T^*\colon W^*\to V^*}$ by ${\langle v, T^*w^*\rangle = \langle Tv, w^*\rangle}$,
${v\in V}$, ${w^*\in W^*}$.

The set of all functions from a (possibly infinite) set $I$ to the real numbers,
${\RR^I = \{x\colon I\to\RR\}}$, is a vector space over $\RR$.
Scalar multiplication and vector addition are defined pointwise:
${(ax)(i) = ax(i)}$ and ${(x + y)(i) = x(i) + y(i)}$ for ${i\in I}$.
This generalizes $\RR^n$ when ${I = \{1,\ldots,n\}}$.
We can identify $x\colon\{1,\ldots,n\}\to\RR$
with $(x_1,\ldots,x_n)$ by $x(i) = x_i$, $i\in I$.

__Exercise__. _Show $\RR^I$ satisfies the abstract vector space axioms._

The _standard basis_ $e_i\in\RR^I$, $i\in I$, is defined by $e_i(j) = δ_{ij}$, $j\in I$,
where $δ_{ij} = 1$ if $i=j$ and $δ_{ij} = 0$ if $i\not= j$ is the _Kronecker delta_.

__Exercise__. _Show if $I$ is finite then $v = \sum_i v(i) e_i$_.

_Hint_. Show $v(j) = (\sum_i v(i) e_i)(j)$ for $j\in I$.

We define the _dual basis_ $e_j^*\in(\RR^I)^*$ by $e_j^*e_i = \delta_{ij}$.

__Exercise__. _Show if $I$ is finite then $v^* = \sum_i v^*e_i e^*_i$_.

_Hint_. Consider $v^*e_j$.

We omit the non-trivial result these axioms imply there exists
a set $I$ where $V$ can be identified with $\RR^I$. The cardinality of $I$
is the _dimension_ of $\RR^I$.

### Subspace

A subset $U\subseteq V$ of a vector space $V$ is a _subspace_ if $U$ is also a vector space.

__Exercise__. _Let $U$ be a subset of $V$. If $\RR U\subseteq U$ and $U + U\subseteq U$ then
$U$ is a subspace of $V$_.

_Hint_. $\RR U = \{au\mid a\in\RR, u\in U\}$ and $U + U = \{v + w\mid v\in U, w\in U\}$.

<details><summary>Solution</summary>
If $u\in U$ and $a\in\RR$ then $au\in\RR U\subseteq U$.
If $v\in U$ and $w\in U$ then $v + w\in U + U\subseteq U$.
</details>

__Exercise__. _Show the intersection of two subspaces is a subspace_.

_Hint_. Show if $v$ is in the intersection then $av$ is also in the intersection for $a\in\RR$
and if $u$ and $w$ are in the intersection then $u + w$ is also in the intersection.

<details><summary>Solution</summary>
If $v\in U\cap V$ then $au\in U$ and $av\in V$ so $au\in U\cap V$.
If $u,w\in U\cap V$ then $u + w\in U$ and $u + w\in V$ so $u + w\in U\cap V$.
</details>

__Exercise__. _Show the sum of two subspaces is a subspace_.

_Hint_. The sum of subspaces $U, W\subseteq V$ is $U + W = \{u + w\mid u\in U, w\in W\}$.

### Quotient

If $U$ is a subspace of $V$ define ${V/U = \{v + U\mid v\in V\}}$.
Define scalar multiplication by ${a(v + U) = av + U}$ and vector addition
by ${(v + U) + (w + U) = (v + w) + U}$, ${a\in\RR}$, ${v,w\in V}$.

__Exercise__. _Show $v + U = w + U$ if and only if $v - w\in U$, $v,w\in V$_.

__Exercise__. _Show scalar multiplication and vector multiplication are well-defined_.

_Hint_: Show $v + U = v' + U$ implies $a(v + U) = a(v' + U)$, and
$v + U = v' + U$, $w + U = w' + U$, imply $(v + w) + U = (v' + w') + U$.

__Exercise__. _Show $V/U$ is a vector space with identity $\zero + U = U$_.

### Span

The _span_ of a subset of a vector space is the smallest subspace
containing the subset.

A _linear combination_ of vectors $v_i\in V$, $i\in I$, is a sum $\sum_i a_i v_i$ where
a finite number of $a_i\in\RR$ are non-zero. The _span_ of $\{v_i\}_{i\in I}$
is the set of all linear combinations,
$$
\span\{v_i\} = \{\sum_i a_i v_i\mid a_i\in\RR\}.
$$

__Exercise__. _Show the span is a vector space_.

_Hint_. Show if $u$ is in the span then $au$ is also in the span for $a\in\RR$
and if $v$ and $w$ are in the span then $v + w$ is also in the span.

<details><summary>Solution</summary>
If $u = \sum_j a_j v_j$ then ${au = \sum_j a(a_j v_j) = \sum_j (aa_j)v_j}$ is in the span.
If ${v = \sum_j b_j v_j}$ and ${w = \sum_j c_j v_j}$ then
${v + w = \sum_j (b_j + c_j) v_j}$ is in the span.
</details>

### Independent

A set of vectors $\{v_i\}_{i\in I}$ are _independent_ if
for any finite sum $\sum a_i v_i = 0$ implies $a_i = 0$ for all $i$.

__Exercise__. _If $\{v_i\}_{i\in I}$ are independent and ${\sum_i a_i v_i = \sum_i b_i v_i}$
then ${a_i = b_i}$ for all ${i\in I}$_.

_Hint_: $\sum_i (a_i - b_i) v_i = \zero$.

<details><summary>Solution</summary>
If ${\sum_i a_i v_i = \sum_i b_i v_i}$ then
$\zero = \sum_i a_i v_i - \sum_i b_i v_i = \sum_i (a_i - b_i)v_i$ so $a_i - b_i = 0$ for $i\in I$.
</details>

__Exercise__. _If $\{v_i\}$ are not independent then
$v_j = \sum_{i\not= j} b_i v_i$ for some $j\in I$ and $b_i\in\RR$_.

_Hint_: If $\sum a_i v_i = \zero$ and $a_j\not= 0$ for some $j\in I$ then
$a_j v_j = -\sum_{i\not=j} a_i v_i$.

<details><summary>Solution</summary>
$v_j = -\sum_{i\not=j} a_i/a_j v_i$.
</details>

### Basis

A collection of vectors $\{v_i\}_{i\in I}$, $v_i\in V$, is a _basis_ of $V$ if
they are independent and span $V$. Since they span $V$ every
vector $v\in V$ can be written as a linear combination $v = \sum_{i\in I} a_i v_i$.
This shows how to identify any vector space $V$ with $\RR^I$ given a basis $(v_i)_{i\in I}$.

The _dimension_ of a vector space is the number of elements of a basis.
A vector space has many collections of vectors that are a basis but
every basis has the same number of vectors. This is not trivial to prove.

## Linear Transformation

A _linear transformation_, or _linear operator_, is a function $T\colon V\to W$, where $V$ and
$W$ are vector spaces that satisfies preserves the vector space structure: ${T(au + v) = aTu + Tv}$, $a\in\RR$,
$u,v\in V$. Note that the addition ${au + v}$ occurs in $V$ and
${aTu + Tv}$ occurs in $W$.  The space of all such linear transformations is
denoted $\LL(V,W)$.

__Exercise__. _Show if $T$ is a linear transformation then $T\zero = \zero$_.

__Hint__: Consider $T(\zero + \zero)$ and $v + v = v$ implies $v = \zero$.

<details><summary>Solution</summary>
$T(\zero + \zero) = T(\zero) + T(\zero)$ and $T(\zero + \zero) = T(\zero)$ so $T(\zero) = \zero$.
</details>

__Exercise__. _Show $T(av) = aTv$, $a\in\RR$, $v\in V$_.

<details>
<summary>Solution</summary>
Using $T(av + w) = aTv + Tw$,
$T(av) = T(av + 0) = aTv + T0 = aTv + 0 = aTv$.
</details>

__Exercise__. _Show $T(av + bw) = aTv + bTw$, $a,b\in\RR$, $v,w\in V$_.

<details>
<summary>Solution</summary>
$T(av + bw) = aTv + T(bw) = aTv = bTw$.
</details>

A linear transformation $T\colon V\to W$ is _one-to-one_, or _injective_,
if $Tu = Tv$ implies $u = v$.

__Exercise__. _Show if $Tv = \zero$ implies $v = \zero$ then $T$ is one-to-one_.

_Hint_. Use linearity.

<details><summary>Solution</summary>
If $Tu = Tv$ then $T(u - v) = \zero$ so $u - v = \zero$ and $u = v$.
</details>

Define the _kernel_ of a linear transformation $\ker T = \{v\in V\mid Tv = \zero\}$.

__Exercise__. _Show $\ker T$ is a subspace_.

__Exercise__. _Show $T$ is one-to-one if and only if $\ker T = \{\zero\}$_.

Define $\pi_T\colon V\to V/\ker T$ by $\pi_T v = v + \ker T$.

__Exercise__. _Show $\pi_T$ is a well-defined linear transformation_.

A linear transformation $T\colon V\to W$ is _onto_, or _surjective_,
if for every $w\in W$ there exists $v\in V$ with $Tv = w$.

Define the _range_ of a linear transformation
${\ran T = TV = \{w\mid w = Tv\text{ for some }v\in V\}}$
and let $\nu_T\colon V/\ker T\to\ran T$ by $\nu_T(v + \ker T) = Tv$.

__Exercise__ _Show $\nu_T$ well-defined, one-to-one, and onto_.

A linear transformation that is one-to-one and onto, or _bijective_, is an _isomorphism_.
If $T\colon V\to W$ is an isomorphism then $V$ and $W$ are _isomorphic_, $V\cong W$.

__Exercise__. _Show if $T$ is an isomorphism then $T^{-1}$ is linear_.

__Exercise__. _Show $V\cong W$ is an equivalence relation_.

_Hint_: This means $V\cong V$, $V\cong W$ implies $W\cong V$, and $V\cong W$, $W\cong U$ implies $V\cong U$.

<details><summary>Solution</summary>
The identity transformation $I\colon V\to V$ defined by $I(v) = v$ shows $V\cong V$.
If $T\colon V\to W$ is an isomorphism then its inverse $T^{-1}\colon W\to V$ shows
$W\cong V$. If $T\colon V\to W$ and $S\colon W\to U$ are isomorphisms then
so is $ST$ and $V\cong U$.
</details>

The space of linear transformations $\LL(V,W)$ is also a vector
space under pointwise scalar multiplication ${(aT)v = a(Tv)}$
pointwise addition ${(T + S)v = Tv + Sv}$, $a\in\RR$, $v,w\in V$.
The space $\LL(\RR^n,\RR^m)$ can be identified with $\RR^{n\times m}$.
If ${T\colon\RR^n\to\RR^m}$ then ${Te_i = \sum_j t_{ij} e_j}$
for some $t_{ij}\in\RR$.

__Exercise__. _If $T\colon\RR^k\to\RR^n$ and $S\colon\RR^n\to\RR^m$ then the composition
$U = ST\colon\RR^k\to\RR^m$. Show $u_{ij} = \sum_k t_{ik} s_{kj}$_.

<details><summary>Solution</summary>
$R(e_i) = ST(e_i)
    = S(\sum_k t_{ik} e_k)
    = \sum_k t_{ik} Se_k
    = \sum_k t_{ik} \sum_j s_{kj} e_j
    = \sum_j \sum_k t_{ik} s_{kj} e_j
    = \sum_j u_{ij} e_j$
</details>

Matrix multiplication is composition of linear transformations.

Another way to see this is to use $A\times B\to C$ is isomorphic to
$A\to(B\to C)$ for any sets $A, B, C$. This is called _currying_ after Haskell Curry.
If $f\colon A\times B\to C$ define $f,\colon A\to(B\to C)$
by $(f,a)b = f(a,b)$.
If $g\colon A\to(B\to C)$ define $g`\colon A\times B\to C$
by $g`(a,b) = (ga)b$.

__Exercise__. _If $f\colon A\times B\to C$ show $(f,)` = f$
and if $g\colon A\to(B\to C)$ show $(g`), = g$_.

This shows a bijection???

We can identify $(I\to\RR)\to(J\to\RR)$ with $I\times J\to\RR$.

Let $\LL(V) = \LL(V,V)$ be the space of linear transformations from
a vector space to itself. It is also an _algebra_ with multiplication
defined by composition with identity ${I_V = I\colon V\to V}$ defined by ${Iv = v}$, ${v\in V}$.

__Exercise__. _Show $IT = TI$ and $T(U + V) = TU + TV$, ${T,U,V\in\LL(V)}$_.

### Indexing

For every _change of index function_ $s\colon I\to J$ define $\circ s\colon \RR^J\to\RR^I$
define $\circ s v = vs\in\RR^I$, for $v\in\RR^J$.

__Exercise__. _Show $\circ s$ is a linear operator_.

_Hint_: $\circ s v(i) = v(s(i))$ for $v\in\RR^J$.

### Sum

If $U$ and $W$ are vector spaces define the _external direct sum_
$U\oplus W = \{u\oplus w)\mid u\in U, w\in W\}$, where
$u\oplus w$ is the pair $(u,w)$.
Define scalar multiplication
$a(u\oplus w) = (au)\oplus (aw)$ and
vector addition $(u\oplus w) + (u'\oplus w') = (u + u')\oplus (w + w')$.

__Exercise__. _Show $U\oplus W$ is a vector space_.

If $U$ and $W$ are subspaces of the vector space $V$ with
$U\cap W = \{\zero\}$  then the _interal direct sum_
$U + V\cong U\oplus V$.

__Exercise__. _Show this!!!_.

Claim: $V\cong U\oplus V/U$.

Define $T\colon U\oplus V/U\to V$ by $T(u\oplus v+W) = u + v$.

If $U$ is invariant for $T$ then $U$ is invariant for $T - \lambda I$.

_Hint_: If $Tu = v$, $u,v\in U$ then $(T - \lambda I)u\in U$.

If $T\in\LL(U,V)$ define $\graph T = \{u\oplus Tu\mid u\in U\}$.

__Exercise__. _Show $T$ is linear if and only if $\graph T$ is a subspace_.

### Invariant Subspace

An _invariant subspace_ of a linear operator $T\colon V\to V$ is a subspace $U\subseteq V$ with $TU\subseteq U$.
Similar to prime factorizaton of numbers, invariant subspaces break down linear operators
into smaller pieces.

__Exercise__. _If $T\colon V\to V$ is a linear operator show $\ker T$ and $\ran T$ are invariant subspaces_.

If $U$ is a 1-dimensional invariant subspace spanned by $e\in V$ then $e$ is an _eigenvector_
and $Te = \lambda e$ for some $\lambda\in\RR$, the _eigenvalue_ corresponding to $u$.

If the eigenvectors of $T$ are independent they and their corresponding eigenvalues determine $T$.
Let $(e_i)$, $(\lambda_i)$ be the eigenvectors and corresponding eigenvalues. Every vector $v\in V$
can be written $v = \sum_i a_i e_i$ so $Tv = \sum_i a_i Te_i = \sum_i \lambda_i a_i v_i$.
In this case we say $T$ is _diagonalizable_. Using the eigenvectors as a basis,
$t_{ij} = \lambda_i δ_{ij}$.

If $e$ is an eigenvector with eigenvalue $\lambda$ then $Te = \lambda e$ so $(T - \lambda I)e = 0$
and $e\in\ker (T - \lambda I)$. There may be vectors that are not eigenvectors
that belong to $\ker (T - \lambda I)$.

__Exercise__. _If the matrix of $T$ is $[0, 1; 0, 0]$ then 




__Exercise__. _If the eigenvectors of $T$ form a basis then $(T-\lambda_1I)\cdots(T-\lambda_nI) = 0$_.

The dimension of $\LL(\RR^n,\RR^n)$ is $n^2$ so we know
$I$, $T$, $T^2$, \dots, $T^{n^2}$ must be linearly dependent so there is a polynomial
of order at most $n^2$ with $p(T) = 0$. If $T$ is diagonalizable the above exercise shows there is
a polynomial of order $n$ satisfying this. The Cayley-Hamilton states this is true for any $T$
where $p(\lambda) = \det(T - \lambda I)$.

## Dual

The _dual_ of a vector space is $V^* = \LL(V,\RR)$, the space of _linear functionals_ on $V$.
Define the _dual pairing_ by $\langle v,v^*\rangle = v^*(v)$ for $v\in V$ and $v^*\in V^*$.

If $V = \RR^n$ we can identify $V^*$ with $\RR^n$ using the standard basis.
Define the _dual basis_ $e_j^*\colon\RR^n\to\RR$ by $e_j^*(e_k) = δ_{jk}$.

__Exercise__. _Show $v = \sum_j e_j^*(v) e_j$, $v\in\RR^n$_.

<details><summary>Solution</summary>
If $v = \sum_j v_j e_j$ then $e_i^*(v) = v_i$.
</details>

__Exercise__. _Show $v^* = \sum_j v^*(e_j) e_j^*$, $v^*\in(\RR^n)^*$_.

<details><summary>Solution</summary>
If $v^* = \sum_j v_j e_j^*$ then $e_i^*(v) = v_i$.
</details>

If $V$ has _any_ basis $e_j$ then every $v\in V$ can be written $v = \sum v_j e_j$ for some $v_j\in\RR$. 
Define the _dual basis_ $e_j^*\colon V\to\RR\in V^*$ by $e_j^*(v) = v_j$. 
The map $V\to V^*$ by $v = \sum_j v_j e_j\mapsto \sum v_j e_j^* = v^*$
is one-to-one and onto (an _isomorphism_).

Functions are vectors. They can be added and scalar multiplication satisfies the distributed law.
Integration is a linear functional on a space of functions.
Given a set $\Omega$ let $B(\Omega) = \{f\colon\Omega\to\RR : \|f\| = \sup_{\omega\in\Omega}|f(\omega)| < \infty\}$.

If $L\colon B(\Omega)\to\RR$ is a linear functional define $\lambda(E) = L(1_E)$ for $E\subseteq\Omega$.

__Exercise__. _If $E,F\subseteq\Omega$ are disjoint the $1_{E\cup F} = 1_E + 1_F$_.

This shows $\lambda(E\cup F) = \lambda(E) + \lambda(F)$ if $E\cap F=\emptyset$.
Since $1_\emptyset = 0$ we have $\lambda(\emptyset) = 0$ so
$\lambda$ is a (finitely additive) measure.

Given a finitely additive measure $\lambda$ on subsets of $\Omega$ define a linear functional
$L\colon B(\Omega)\to\RR$ by $L(\sum_i a_i 1_{E_i}) = \sum_i a_i \lambda(E_i)$.

__Exercise__. _Show this is well-defined_.

_Hint_: $\sum_i a_i 1_{A_i} = \sum_j b_j 1_{B_j}$ where $(B_j)$ are pairwise disjoint.
Note $1_A + 1_B = 1_{A\setminus B} + 1_{A\cap B} + 1_{B\setminus A}$ is a sum of
pairwise disjoint sets.

__Exercise__. _Given $f\in B(Ω)$ and $ε > 0$ show there exist a finite number of $a_i\in\RR$
and $A_i\subseteq Ω$ with $\|f - \sum_i a_i 1_{A_i}\| < ε$_.

This shows the linear functional can be extended to $B(\Omega)$
and $B(\Omega)^*$ is isomorphic to the space of finitely additive measures on $\Omega$, $ba(\Omega)$.

If $\Omega$ has a sufficiently rich topology (e.g., compact and Hausdorff) then
$C(\Omega)^*$ can be identfied with the space of countably additive Borel measures on $\Omega$, $M(\Omega)$.
If $\mu\in M(\Omega)$ define $L^p(\mu) = \{f\colon\Omega\to\RR : \int_\Omega |f|^p\,d\mu < \infty\}$.
It is true that $L^p(\mu)^*\cong L^q(\mu)$ where $1/p + 1/q = 1$ and $p > 1$.
It is not true that $L^\infty(\mu)^* \cong L^1(\mu)$ in general.
Proving these claims is non-trivial.

## Grassmann

For $I \subset V$ let $I\colon I\to V$ be injective.

Define $\vee I$ to be the span of $I$.

$t\colon I\to J$. $T\colon\RR^J\to\RR^I$.
$v\colon J\to\RR$ mapsto $Tv\colon I\to\RR$ by $Tv(i) = v(t(i)$.

$f\colon A\to B$

$\circ f\colon C^B\to C^A$ by $(\circ f)g = gf$.

$f\circ\colon A^C\to B^C$ by $(f\circ)h = fh$.

A function $f\colon I\to J$ determines a linear transformation
$F\colon\RR^J\to\RR^I$.

$I^n\to\RR$.

$d_i\colon I^n\to I^n\setminus\{i\}$

$\partial\colon I^n\to\RR^{I^{n-1}}$.

## Heisenberg

Werner Heisenberg rediscovered matrix multiplication by considering
orbital levels of the hydrogen atom. If $e_{ij}$ represents a jump
from level $i$ to level $j$, he posited
$e_{ij}e_{kl} = e_{il}$ if $j = k$ and equals $e_{ij}e_{kl} = 0$ if $j\not= k$. [@cite Hei]
An electron can jump from $i$ to $j$, then $j$ to $l$, but not
from $i$ to $j$, then $k$ to $l$ if $k\not= j$.

__Exercise__. _If $S = \sum_{i,j}s_{ij}e_{ij}$ and $T = \sum_{k,l} t_{kl}e_{kl}$ show
$TS = \sum_{i,j} (\sum_k t_{ik} s_{kj}) e_{ij}$_.

<details><summary>Solution</summary>
$$
\begin{aligned}
TS &= (\sum_{kl} t_{kl}e_{kl})(\sum_{ij}s_{ij}e_{ij}) \\
&= \sum_{ij} \sum_{kl} s_{ij} t_{kl} e_{ij}e_{kl} \\
&= \sum_{ij} \sum_{kl} s_{ij} t_{kl} e_{il}\delta_{jk} \\
&= \sum_{ij} \sum_k s_{ik} t_{kl} e_{il} \\
\end{aligned}
$$
</details>

The _kernel_ of a linear transformation $T\colon V\to W$ is
$\ker T = \{v\in V\mid Tv = 0\}\subseteq V$.

__Exercise__. _The kernel of a linear transformation is a subspace_.

_Hint_: $T(av + w) = aTv + Tw = 0$ for $a\in\RR$, $v,w\in \ker T$.

__Exercise__. _$T$ is one-to-one if and only if $\ker T = \{0\}$_.

_Hint_: Consider $T(v - v')$.

The _range_ of a linear transformation $T\colon V\to W$ is
$\ran T = \{Tv\mid v\in V\}\subseteq W$.

__Exercise__. _The range of a linear transformation is a subspace_.

_Hint_: $aTv + Tw = T(av + w)\in\ran T$.

If $\ran T = W$ then $T$ is _onto_, or _surjective_.

Every linear transformation $T\colon V\to W$ factors through the quotient space $V/\ker T$.
Define $π\colon V\to V/\ker T$ by $πv = v + \ker T$.

__Exercise__. _Show $π$ is a surjective linear transformation_.

Define $ν\colon V/\ker T\to\ran T$ by $ν(v + \ker T) = Tv$.

__Exercise__. _Show $ν$ is a well-defined injective linear transformation_.

_Hint_: Start by showing it is well-defined; if $v + \ker T = v' + \ker T$ then $Tv = Tv'$, $v,v'\in V$.

<details>
<summary>Solution</summary>
Since $v + \ker T = v' + \ker T$ if and only if $v - v'\in\ker T$ we have
$T(v - v') = 0$ so $Tv = Tv'$ and $ν$ is well-defined.
If $Tv = Tv'$ then $v - v'\in\ker T$ so $v + \ker T = v' + \ker T$
showing $ν$ is injective.
</details>


### Quotient

If $U$ is a subspace of $V$ and $v\in V$ define the _coset_ of $U$ containing $v$
by $v + U = \{v + u\mid u\in U\}$. Subspaces factor vector spaces into smaller vector spaces.

__Exercise__. _Show $v\in v+U$ for $v\in V$_.

_Hint_: $U$ is a vector space so $0\in U$.

__Exercise__. _Show $u + U = U$ if and only if $u\in U$_.

<details>
<summary>Solution</summary>
If $u + U = U$ then $u + u' = u''$ for some $u',u''\in U$
so $u = u'' - u'\in U$.

If $u\in U$ then $u + u'\in U$ for all $u'\in U$ so $u + U \subseteq U$
and if $u'\in U$ then $u' = u + (u' - u)\in u + U$ so $U\subseteq u + U$.
</details>

__Exercise__. _Show $v + U = w + U$ if and only if $v - w\in U$_.

<details>
<summary>Solution</summary>
If $v + U = w + U$ then $v + u = w + u'$ for some $u,u'\in U$
so $v - w = u' - u\in U$.

If $v - w\in U$ then $v - w = u$ for some $u\in U$
so $v + U = w + u + U = w + U$.
</details>

__Exercise__. _Show $v\cong_U w$ if and only if $v + U = w + U$ is
an equivalence relation_.

_Hint_: Show $v\cong v$ (reflexive), $v\cong w$ implies $w\cong v$ (symmetric), and
$v\cong w$ and $w\cong x$ implies $v\cong x$ (transitive).

The _quotient space_ $V/U = \{v + U\mid v\in V\}$ is a vector
space with scalar multiplication $a(v + U) = av + U$ and
addition $(v + U) + (w + U) = (v + w) + U$.

__Exercise__. _Show $v + U = w + U$ implies $av + U = aw + U$, $a\in\RR$, $v,w\in V$_.

_Hint_: $av - aw\in U$.

<details>
<summary>Solution</summary>
If $v + U = w + U$ then $v - w\in U$, so $a(v - w)\in U$ and $av + U = aw + U$.
</details>

__Exercise__. _Show $v + U = v' + U$ and $w + U = w' + U$ implies
$v + u + U = v' + w' + U$, $v,v',w,w'\in V$_.

_Hint_: $v - v', w - w'\in U$.

The last two exercises show scalar multiplication and addition are well-defined in $V/U$.

__Exercise__. _Show $(u + U) + (v + U) = (v + U) + (u + U)$ and
$a((u + U) + (v + U)) = a(u + U) + a(v + U)$_.

This shows addition is commutative and scalar multiplication distributes over addition,
hence the quotient space $V/U$ is a vector space where
the cosets are the vectors. A subspace $U$ and the quotient space $V/U$
determine $V$ up to isomorphism, but that requires more machinery.

$T\colon U\to V$

$0\to\ker T\to U\to \ran T\to V\to 0$

$0\to U\to V\to V/U\to 0$.

## Norm

A _norm_ on a vector space is a function $\|\cdot\|\colon V\to[0,\infty)$ with
$\|av\| = |a|\|v\|$, $\|v + w\| \le \|v\| + \|w\|$, $a\in\RR$, $v,w\in V$,
and $\|v\| = 0$ implies $v = 0$.

If $V=\CC^n$ then $\|v\|_\infty = \max_i |v_i|$ and $\|v\|_p = (\sum_i |v_i|^p)^{1/p}$
are the _sup norm_ and $p$-_norm_, $p\ge 1$.

__Exercise__. _Show $\lim_{p\to\infty}\|v\|_p = \|v\|_\infty$_.

If $T\colon V\to W$ is a linear transformation between normed vector spaces then
the _operator norm_ is $\|T\| = \sup_{\|v\|\le 1}\|Tv\|$.

__Exercise__. _Show $\|aT\| = |a|\|T\|$, $\|T + S\|\le \|T\| + \|S\|$ and $\|T\| = 0$ implies $T = 0$,
$a\in\RR$, $T,S\in\LL(V,W)$_.

## Inner Product

An _inner product_ on a vector space is a bilinear function $V\times V\to\RR$.
The pair $(u,v)$ is sent to $v\cdot w$, $v, w\in V$. The inner product satisfies
$v\cdot v \ge 0$ and $v\cdot v = 0$ implies $v = 0$.

__Exercise__. _Show $\|v\| = v\cdot v$ is a norm_.

__Theorem__ (Cauchy-Schwartz) _$|u\cdot v| \le \|u\| \|v\|$ and equality
holds if and only if $u$ and $v$ are colinear_.

_Proof_. Since $0\le\|au - v\|^2 = a^2\|u\|^2 - 2au\cdot v + \|v\|^2$
the discriminat $|u\cdot v|^2 - \|u\|^2 \|v\|^2\ge 0$. The discriminant
is 0 if and only if $au - v = 0$.

## Spectrum

If $V$ is a finite dimensional normed space over $\CC$ then every operator $T\colon V\to V$ has
and eigenvector.

The _spectrum_, $σ(T)$, of a linear operator $T\colon V\to V$ is the set of all $\lambda\in\CC$
such that $\ker(T - \lambdaI)$ is not invertable. The _spectral radius_ is
$ρ(T) = \max\{|\lambda|\mid \lambda\in σ(T)\}$.

__Exercise__. _Show if $V$ is finite dimensional then the spectrum is the set of eigenvalues_.

_Hint_: $\ker(T - \lambdaI)\neq 0$ if and only if $Te = \lambdae$ for some $e\in V$.

Define $E_\lambda = \ker(T - \lambdaI)$.

__Exercise__. _Show $E_\lambda\cap E_μ = 0$ if $\lambda\ne μ$_.

__Exercise__. _Show $\sum_{\lambda\in σ(T)} E_\lambda = V$_.

Define the _multiplicity_ of $\lambda\in\CC$ by $m(\lambda) = \dim\ker(T - \lambdaI)$.

__Exercise__. _Show there exists $e\in V$ with $(T - \lambdaI)^ke\neq 0$ for $0\le k < m(\lambda)$
and $(T - \lambdaI)^{m(\lambda)}e = 0$_.

### Adjoint

The _adjoint_ of a linear operator $T\colon V\to W$ is $T^*\colon W^*\to V^*$ defined
by $\langle v, T^* w\rangle = \langle Tv, w^*\rangle$, $v\in V$, $w^*\in W^*$.

## Fréchet Derivative

If $F\colon X\to Y$ is a function between normed vector spaces the _Fréchet_ derivative
$DF\colon X\to\LL(X,Y)$ is defined by
$$
    F(x + h) - F(x) = DF(x)h + o(\|h\|).
$$

Recall $F(x) = G(x) + o(\|h\|)$ means $\lim_{\|h\|\to 0} \|F(x) - G(x)\|/\|h\| = 0$.

__Exercise__. _If $F(x) = x^2$ where $x$ is a square matrix show $DF(x) = L_x + R_x$ where
$L_xy = xy$ and $R_xy = yx$_.

A suggestive way to write this is $D(x^2) = x(Dx) + (Dx)x$.

_Hint_: $(x + h)^2 = x^2 + xh + hx + h^2$ and $h^2 = o(\|h\|)$.

<details><summary>Solution</summary>
Since $(x + h)^2 = xx + xh + hx + hh$ and $\|h^2\| = o(\|h\})$ we have
$D(x^2)h = L_x h + R_x h$.
</details>

__Exercise__. _If $F(x) = x^n$ where $x$ is a square matrix and $n\in\NN$ show
$DF(x) = \sum_{i=0}^{n-1} L_x^{n-i-1}R_x^{i}$_.

_Hint_: What are the terms in $(x + h)^n$ containing exactly one $h$?

__Exercise__. _If $F\colon\RR^n\to\RR$ is
$F(x) = \|x\|^p$ show $DF(x) = p\|x\|^{p-2}x^*$._

_Hint_. Show $D\|x\|^2 = 2x^*$ and note $\|x\|^p = (\|x\|^2)^{p/2}$.
By the chain rule $D\|x\|^p = (p/2)\|x\|^{2(p/2 - 1)}2x^* = p\|x\|^{p - 2}x^*$.

For example, a _semigroup_ is a set $S$ and binary operation ${m\colon S\times S\to S}$
that is associative: ${m(a,m(b,c)) = m(m(a,b),c)}$ for ${a,b,c\in S}$,
or ${a(bc) = (ab)c}$ if we write $ab$ for $m(a,b)$.
While this may seem trivial, is allows us to write $abc$ without parentheses.
This is the foundation of [MapReduce](https://en.wikipedia.org/wiki/MapReduce).

We can add an _identity_ $e$, not in the semigroup $S$, that satisfies
${es = s = se}$ for all $s\in S$ to turn a semigroup into a monoid
${M = S\cup\{e\}}$.

__Exercise__. _Show ${a(bc) = (ab)c}$ for ${a,b,c\in M}$_.

_Hint_: There are seven boring cases when $a$, $b$, and $c$ are $e$.

A _monoid_ is a semigroup with an identity for the binary operation.
The identity is unique.

__Exercise__. _If $M$ is a monoid and $e'\in M$ satisfies $e'm = m = me'$, $m\in M$, then $e' = e$_.

<details>
<summary>Solution</summary>
$e' = ee' = e$
</details>

__Exercise__. _Show $\max\{a,b\}$, $a,b\in\RR$ is a monoid with
identity $-\infty$_.

_Hint_: $-\infty < a$ for all $a\in\RR$.

__Exercise__. _Show $\min\{a,b\}$, $a,b\in\RR$ is a monoid with
identity $+\infty$_.

_Hint_: $\infty > a$ for all $a\in\RR$.

__Exercise__. _Show string concatenation is a monoid with
identity the empty string_.

The _Kleen star_ of a set is the union of the cartesian product of ${n\in\NN}$ copies of of the set.
If $M$ is a monoid 
where ${M^0 = \{e\}}$ and ${M^n = \{(m_1,\ldots,m_n)\mid m_j\in M, 1\le j\le n\}}$.
Define _fold_ ${f\colon M^*\to M}$ by ${f(e) = e}$ and ${f((m_1,\ldots,m_n)) = m_1\cdots m_n}$.
-->
</section>
</section>
<aside id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>This is the definition of a tensor common in machine
learning. The mathematical definition of a tensor is quite different.<a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</body>
</html>
