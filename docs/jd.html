<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Keith A. Lewis" />
  <meta name="dcterms.date" content="2025-09-18" />
  <title>Johnson Distribution</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="math.css" />
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Johnson Distribution</h1>
<p class="author">Keith A. Lewis</p>
<p class="date">September 18, 2025</p>
</header>
<p>N. L. Johnson considered how to apply classical statistical
techniques involving normal distributions to almost normal
distributions. Given an almost normal distribution <span
class="math inline">X</span> he looked for transformations of the form
<span class="math inline">\gamma + \delta f((X - \xi)/\lambda)</span>
that would result in a normal distribution, where <span
class="math inline">\gamma</span>, <span
class="math inline">\delta</span>, <span class="math inline">\xi</span>,
<span class="math inline">\lambda</span> are real numbers and <span
class="math inline">f</span> is function. We assume <span
class="math inline">\delta</span> and <span
class="math inline">\lambda</span> are positive</p>
<p>If <span class="math inline">N = \gamma + \delta f((X -
\xi)/\lambda)</span> is normal then we can adjust <span
class="math inline">\gamma</span> and <span
class="math inline">\delta</span> to make it standard normal.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">\mu =
E[N]</span> and <span class="math inline">\sigma^2 =
\operatorname{Var}(N)</span> then <span class="math inline">\gamma&#39;
+ \delta&#39; f((X -\xi)/\lambda)</span> is standard normal if <span
class="math inline">\gamma&#39; = (\gamma - \mu)/\sigma</span> and <span
class="math inline">\delta&#39; = \delta/\sigma</span></em>.</p>
<p>We will use <span class="math inline">Z = \gamma + \delta f((X -
\xi)/\lambda)</span> where <span class="math inline">Z</span> is
standard normal in what follows. If <span class="math inline">f</span>
is invertable then <span class="math inline">X = \xi + \lambda f^{-1}((Z
- \gamma)/\delta)</span>.</p>
<p>Johnsonâ€™s <span class="math inline">S_U</span> (unbounded) system is
<span class="math inline">{f(y) = \sinh^{-1}(y) = \log(y + \sqrt{1 +
y^2})}</span> so <span class="math inline">{X = \xi + \lambda \sinh((Z -
\gamma)/\delta) = \xi + \lambda \sinh(U)]}</span> where <span
class="math inline">{U = (Z - \gamma)/\delta}</span>. Since <span
class="math inline">E[e^N] = e^{E[N] + \operatorname{Var}(N)/2}</span>
for any normally distributed <span class="math inline">N</span> we have
<span class="math display">
E[\sinh U] = E[(e^U - e^{-U})/2] = (e^{E[U] + \operatorname{Var}(U)/2} -
e^{-E[U] + \operatorname{Var}(U)/2})/2 =
e^{\operatorname{Var}(U)/2}\sinh E[U]
</span></p>
<p>In terms of the Johnson parameters <span class="math inline">E[\sinh
U] = e^{1/2\delta^2}\sinh(-\gamma/\delta)</span>. Note this only depends
on <span class="math inline">\gamma</span> and <span
class="math inline">\delta</span>.</p>
<p>The expected value of <span class="math inline">X</span> is <span
class="math inline">E[X] = E[\xi + \lambda \sinh(U)]</span>. For small
<span class="math inline">\gamma</span> and <span
class="math inline">\delta</span> near 1 we have <span
class="math inline">E[X] \approx \xi - \sqrt{e}\lambda\gamma</span>.</p>
<p>Clearly <span class="math inline">X\le x</span> is equivalent to
<span class="math inline">Z \le z</span> where <span
class="math inline">z = \gamma + \delta \sinh^{-1}((x -
\xi)/\lambda)</span>. We have <span class="math inline">{P(X\le x) =
\Phi(z)}</span> where <span class="math inline">\Phi</span> is the
standard normal cumulative distribution.</p>
<p>Recall for <span class="math inline">N</span> and <span
class="math inline">M</span> jointly normal we have <span
class="math inline">E[e^N f(M)] = E[e^N] E[f(M +
\operatorname{Cov}(M,N))]</span>. For share measure we calculate</p>
<p><span class="math display">
\begin{aligned}
E[X 1(X\le x)] &amp;= E[(\xi + \lambda \sinh(U) 1(Z\le z)] \\
    &amp;= \xi P(Z\le z) + \lambda E[\sinh(U) 1(Z\le z)] \\
    &amp;= \xi P(X\le x) + \lambda E[\frac{e^U - e^{-U}}{2} 1(Z\le z)]
\\
    &amp;= \xi P(X\le x) + \frac{\lambda}{2} (E[e^U] P(Z +
\operatorname{Cov}(U,Z)\le z) - E[e^{-U}] P(Z -
\operatorname{Cov}(U,Z)\le z])) \\
\end{aligned}
</span></p>
<p>Since <span class="math inline">{\operatorname{Cov}(U, Z) =
1/\delta}</span></p>
<p><span class="math display">
E[X 1(X\le x)] = \xi P(X\le x) + \frac{\lambda
e^{\operatorname{Var}(U)/2}}{2}
\left(e^{E[U]}\Phi(z - 1/\delta) - e^{-E[U]}\Phi(z + 1/\delta)\right)
</span></p>
<p>Note the put forward value is <span class="math inline">E[(k - X)^+]
= kP(X\le k) - E[X 1(X\le k)]</span> so we have a closed form solution
for put values using the Johnson distribution.</p>
<p>To compute the second moment of <span class="math inline">X</span> we
will need <span class="math display">
\begin{aligned}
E[\sinh^2(U)] &amp;= E[((e^U - e^{-U})/2)^2] \\
    &amp;= E[(e^{2U} - 2 + e^{-2U})]/4 \\
    &amp;= (e^{-2\gamma/\delta + 2/\delta^2} - 2 + e^{2\gamma/\delta +
2/\delta^2})/4 \\
    &amp;= e^{2/\delta^2}\cosh(2\gamma/\delta)/2 - 1/2 \\
\end{aligned}
</span> Note <span class="math inline">E[\sinh^2(U)]</span> does not
depend on <span class="math inline">\xi</span> and <span
class="math inline">\lambda</span>.</p>
<p>The second moment of <span class="math inline">X</span> is <span
class="math display">
\begin{aligned}
E[X^2] &amp;= E[(\xi + \lambda \sinh(U))^2] \\
    &amp;= \xi^2 + 2\xi\lambda E[\sinh(U)] + \lambda^2 E[\sinh^2(U)] \\
\end{aligned}
</span></p>
<p>To compare with the lognormal model we want to match the first two
moments of <span class="math inline">X</span> and <span
class="math inline">F = f\exp(\sigma\sqrt{t} Z - \sigma^2t/2)</span>. We
have <span class="math inline">E[F] = f</span> and <span
class="math inline">E[F^2] = f^2\exp(\sigma^2 t)</span>.</p>
<p>Consider <span class="math inline">\gamma</span> and <span
class="math inline">\delta</span> as given input parameters
corresponding to skew and kertosis respectively. Given <span
class="math inline">f (= se^{rt})</span> and <span
class="math inline">\sigma</span> we want <span class="math display">
\begin{aligned}
f &amp;= \xi + \lambda E[\sinh(U)] \\
f^2\exp(\sigma^2 t) &amp;= \xi^2 + 2\xi\lambda E[\sinh(U)] + \lambda^2
E[\sinh^2(U)] \\
\end{aligned}
</span> Substituting <span class="math inline">{\xi = f - \lambda
E[\sinh(U)]}</span> and using <span class="math inline">u_n =
E[\sinh^n(U)]</span> gives a quadratic expression in <span
class="math inline">\lambda</span> <span class="math display">
\begin{aligned}
f^2\exp(\sigma^2 t) &amp;= (f - \lambda u_1)^2 + 2(f - \lambda
u_1)\lambda u_1 + \lambda^2 u_2 \\
f^2\exp(\sigma^2 t) &amp;=
    f^2 - 2f\lambda u_1 + \lambda^2 u_1^2
    + 2f\lambda u_1 - 2\lambda^2 u_1^2
    + \lambda^2 u_2 \\
f^2\exp(\sigma^2 t) &amp;= (u_2 - u_1^2)\lambda^2 + f^2 \\
\end{aligned}
</span> This implies <span class="math inline">\lambda = \sqrt{(u_2 -
u_1^2)/\operatorname{Var}(F)}</span>.</p>
</body>
</html>
