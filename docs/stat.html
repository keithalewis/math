<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Keith A. Lewis" />
  <meta name="dcterms.date" content="2024-04-23" />
  <title>Statistics</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="math.css" />
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Statistics</h1>
<p class="author">Keith A. Lewis</p>
<p class="date">April 23, 2024</p>
<div class="abstract">
<div class="abstract-title">Abstract</div>
Observations of outcomes
</div>
</header>
<blockquote>
<em>Probability all the things</em>!
</blockquote>
<section id="statistics" class="level2">
<h2>Statistics</h2>
<p>Statistics is the study of finding <em>estimators</em> for a
<em>statistic</em>.</p>
<p>A statistic is a function <span class="math inline">s</span> from
random variables to the real numbers <span
class="math inline">\bm{R}</span>. Estimators are collection of
functions <span class="math inline">s_n\colon\bm{R}^n\to\bm{R}</span>,
where <span class="math inline">n\in\bm{N}</span> is a natural number.
Given independent <span class="math inline">(X_1,\ldots, X_n)</span>
having the same law as <span class="math inline">X</span> how do we find
estimators <span class="math inline">s_n</span> such that the random
variable <span class="math inline">s_n(X_1,\ldots,X_n)</span>
approximates the number <span class="math inline">s(X)</span> as <span
class="math inline">n</span> gets large?</p>
<p>For example, if <span class="math inline">s(X) = E[X]</span> is the
expected value of <span class="math inline">X</span>, then the
<em>arithmetic means</em> <span class="math inline">m_n(x_1,\ldots,x_n)
= (x_1 + \cdots + x_n)/n</span> are estimators. If <span
class="math inline">M_n = m_n(X_1,\ldots,X_n)</span> then <span
class="math inline">E[M_n] = E[X]</span> and <span
class="math inline">\operatorname{Var}(M_n) = \operatorname{Var}(X)/n\to
0</span> as <span class="math inline">n\to\infty</span>. We say the
random variables <span class="math inline">S_n</span> converge to the
number <span class="math inline">s</span> in <em>mean squared</em> when
<span class="math inline">\lim_{n\to\infty}E[(S_n - s)^2] =
0</span>.</p>
<p>If <span class="math inline">s_n</span> are estimators let <span
class="math inline">S_n = s_n(X_1,\ldots,X_n)</span>. Estimators with
the property <span class="math inline">E[S_n] = s(X)</span>, <span
class="math inline">n\in\bm{N}</span>, are <em>unbiased</em>.</p>
<p>We could also use the <em>geometric means</em> <span
class="math inline">g_n(x_1,\ldots,x_n) =
\sqrt[n]{x_1\cdots x_n}</span> as estimators for <span
class="math inline">E[X]</span> if <span class="math inline">X</span> is
positive. By <a href="prob.html#jensens-inequality%22">Jensen’s
Inequality</a> <span class="math inline">E[\sqrt[n]{X_1\cdots X_n}] \le
E[X]</span> so the geometric means are not unbiased.</p>
<p>Clearly <span class="math inline">E[M_n\mid X_1 = x_1, \ldots, X_n =
x_n] = E[M_n\mid M_n = m_n(x_1, \ldots, x_n)]</span> since both are
equal to <span class="math inline">m_n(x_1, \ldots, x_n)</span>. If
<span class="math inline">s_n</span> and <span
class="math inline">t_n</span> are estimators with the property <span
class="math inline">E[S_n\mid X_1 = x_1, \ldots, X_n = x_n]
= E[S_n\mid S_n = t_n(x_1,\ldots,x_n)]</span> then <span
class="math inline">t_n</span> are <em>sufficient</em> for <span
class="math inline">s_n</span>.</p>
<p>Some statistics are better than other statistics.</p>
<section id="bias" class="level3">
<h3>Bias</h3>
<p>If <span class="math inline">E[s_n(X_1,\ldots,X_n)] = \sigma</span>
we say <span class="math inline">s_n</span> is an <em>unbiased</em>
<em>estimator</em> of <span class="math inline">\sigma</span>. The
arithmetic mean is an unbiased estimator of the mean. Since <span
class="math inline">E[(X_1\cdots X_n)^{1/n}] \le E[X_1\cdots X_n]^{1/n}
= E[X]</span> the geometric mean is biased.</p>
</section>
<section id="efficient" class="level3">
<h3>Efficient</h3>
<p>An unbiased statistic <span class="math inline">s_n</span> is
<em>efficient</em> if it has the smallest variance among all unbiased
statistics. This leaves open the possibility of biased statistics that
have lower variance than efficient statistics.</p>
</section>
<section id="complete" class="level3">
<h3>Complete</h3>
<p>A statistic <span class="math inline">s\colon\bm{R}^n\to\bm{R}</span>
is <em>complete</em> if <span
class="math inline">E_\theta[g(s(X_1,\ldots,X_n))] = 0</span> for all
<span class="math inline">\theta</span> implies <span
class="math inline">g(s(X_1,\ldots,X_n)) = 0</span> a.s.</p>
</section>
<section id="sufficient" class="level3">
<h3>Sufficient</h3>
<p>A statistic <span class="math inline">t\colon\bm{R}^n\to\bm{R}</span>
is <em>sufficient</em> if the <span class="math inline">n</span>
conditions <span class="math inline">X_j = x_j</span>, <span
class="math inline">1\le j\le n</span> can be replaced by one condition
<span class="math inline">t(X_1,\ldots,X_n) = t(x_1,\ldots,x_n)</span>,
i.e., <span class="math inline">E_\theta[g(X)\mid X_j = x_j] =
E_\theta[g(X)\mid s(X_1,\ldots,X_n) = s(x_1,\ldots,x_n)]</span>.</p>
</section>
</section>
<section id="statistic" class="level2">
<h2>Statistic</h2>
<p>Given two statistics <span class="math inline">s</span> and <span
class="math inline">t</span> for <span class="math inline">\sigma</span>
where <span class="math inline">t</span> is sufficient let <span
class="math inline">\delta(X) = E[s(X)\mid t(X)]</span> be the
<em>improved estimator</em>. The following theorem justifies this
name.</p>
<p><strong>Theorem</strong>. (Rao–Blackwell–Kolmogorov) <em><span
class="math inline">E[(\delta(X) - \sigma)^2] \le E[(s(X) -
\sigma)^2]</span></em>.</p>
<p><em>Proof</em>. We have $</p>
<p><strong>Theorem</strong>. (Lehmann–Scheffé) <em>If …</em>.</p>
<p>See <a
href="https://en.wikipedia.org/wiki/Lehmann%E2%80%93Scheff%C3%A9_theorem">Lehmann–Scheffé
Theorem</a></p>
</section>
<section id="population" class="level2">
<h2>Population</h2>
<p>Sampling from a <em>population</em> is not special. One always does
this.</p>
</section>
<section id="hypothesis-testing" class="level2">
<h2>Hypothesis Testing</h2>
<p>Given random variables <span class="math inline">X_\theta</span>,
<span class="math inline">\theta\in\Theta</span>, and a partion <span
class="math inline">\{\Theta_0,\Theta_1\}</span> of <span
class="math inline">\Theta</span> how can we decide if <span
class="math inline">\theta\in\Theta_0</span> (<em>null hypothesis</em>)
or <span class="math inline">\theta\in\Theta_1</span> (<em>alternate
hypothesis</em>)?</p>
<p>This is done by designing <em>tests</em> and collecting <em>data
samples</em>. A test is a subset <span
class="math inline">\delta_0\subseteq \bm{R}^n</span> and is called the
<em>critical region</em>. A data sample is a collection of numbers <span
class="math inline">x = (x_1,\ldots,x_n)\in\bm{R}^n</span>. We
<em>accept</em> the null hypothesis if the sample belongs to this
set.</p>
<p>Let <span class="math inline">X = (X_1,\ldots,X_n)</span> be iid
random variables with the same law as <span
class="math inline">X_\theta</span>. The <em>power function</em> of a
test <span class="math inline">\delta_0</span> is <span
class="math inline">\pi(\theta) = P(X\in\delta\mid\theta)</span>. If
<span class="math inline">\pi = 1_{\Theta_1}</span> then the test
determines whether or not <span
class="math inline">\theta\in\Theta_0</span> with probability 1.</p>
<p><strong>Exercise</strong>. <em>Show if <span class="math inline">\pi
= 1_{\Theta_1}</span> then <span class="math inline">x\in\delta_0</span>
implies <span class="math inline">P(\theta\in\Theta_0) =
1</span></em>.</p>
<p>Hint: If <span class="math inline">x\in\delta_0</span> then $</p>
<p>This is usually not possible so we look for tests that approximate
the indicator function of <span class="math inline">\Theta_1</span>. The
<em>size</em> of a test <span class="math inline">\delta</span> is <span
class="math inline">\alpha = \sup_{\theta\in\Theta_0}
\pi(\theta)</span>.</p>
</section>
</body>
</html>
